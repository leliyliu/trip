06-30-23 14:42:start training resnet20:
06-30-23 14:42:Training messages:
*********************************************************
* Training Setting:                                     *
*     1. weight_update() executes one time every        *
*        epoch;                                         *
*     2. zero_grad2() executes every batch;             *
*     3. lr = 0.1;                                      *
*     4. grad_update() adds momentum, momentum = 0.9;   *
*     5. num_bits = num_grad_bits = 10;                 *
* Methods: Trip with 8+8;                               *
*********************************************************
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [0/391]	Time 0.428 (0.428)	Data 0.270 (0.270)	Loss 3.342 (3.342)	Prec@1 7.812 (7.812)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [50/391]	Time 0.157 (0.163)	Data 0.002 (0.007)	Loss 1.836 (2.286)	Prec@1 30.469 (20.818)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [100/391]	Time 0.157 (0.160)	Data 0.002 (0.005)	Loss 1.789 (2.053)	Prec@1 34.375 (26.044)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [150/391]	Time 0.071 (0.145)	Data 0.002 (0.004)	Loss 1.585 (1.944)	Prec@1 38.281 (29.341)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [200/391]	Time 0.060 (0.125)	Data 0.002 (0.003)	Loss 1.507 (1.859)	Prec@1 46.094 (32.245)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [250/391]	Time 0.055 (0.111)	Data 0.002 (0.003)	Loss 1.423 (1.792)	Prec@1 50.781 (34.425)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [300/391]	Time 0.068 (0.102)	Data 0.002 (0.003)	Loss 1.349 (1.738)	Prec@1 50.781 (36.425)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError(0, 'Error'))': /api/5288891/envelope/
06-30-23 14:43:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError(0, 'Error'))': /api/5288891/envelope/
06-30-23 14:43:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError(0, 'Error'))': /api/5288891/envelope/
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [350/391]	Time 0.056 (0.098)	Data 0.002 (0.003)	Loss 1.261 (1.693)	Prec@1 56.250 (38.054)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Test: [0/79]	Time: 0.2896(0.2896)	Loss: 1.318(1.318)	Prec@1: 45.312(45.312)	
06-30-23 14:43:Test: [50/79]	Time: 0.0253(0.0314)	Loss: 1.475(1.330)	Prec@1: 46.875(52.267)	
06-30-23 14:43:Test: [78/79]	Time: 0.0247(0.0293)	Loss: 1.180(1.326)	Prec@1: 56.250(52.300)	
06-30-23 14:43:Step 0 * Prec@1 52.300
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [0/391]	Time 0.419 (0.419)	Data 0.262 (0.262)	Loss 1.170 (1.170)	Prec@1 52.344 (52.344)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [50/391]	Time 0.057 (0.094)	Data 0.002 (0.007)	Loss 1.201 (1.266)	Prec@1 54.688 (54.090)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [100/391]	Time 0.070 (0.078)	Data 0.002 (0.005)	Loss 1.254 (1.251)	Prec@1 53.125 (54.571)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [150/391]	Time 0.058 (0.073)	Data 0.002 (0.004)	Loss 1.239 (1.219)	Prec@1 51.562 (55.800)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [200/391]	Time 0.052 (0.071)	Data 0.002 (0.003)	Loss 1.242 (1.204)	Prec@1 57.031 (56.336)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:43:Num bit 10	Num grad bit 10	
06-30-23 14:43:Iter: [250/391]	Time 0.076 (0.071)	Data 0.003 (0.003)	Loss 0.921 (1.183)	Prec@1 68.750 (57.174)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:44:Num bit 10	Num grad bit 10	
06-30-23 14:44:Iter: [300/391]	Time 0.157 (0.080)	Data 0.002 (0.003)	Loss 1.144 (1.165)	Prec@1 62.500 (57.916)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:44:Num bit 10	Num grad bit 10	
06-30-23 14:44:Iter: [350/391]	Time 0.158 (0.091)	Data 0.002 (0.003)	Loss 1.238 (1.150)	Prec@1 57.031 (58.500)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:44:Test: [0/79]	Time: 0.3421(0.3421)	Loss: 1.166(1.166)	Prec@1: 67.188(67.188)	
06-30-23 14:44:Test: [50/79]	Time: 0.0255(0.0893)	Loss: 1.078(1.141)	Prec@1: 65.625(60.447)	
06-30-23 14:44:Test: [78/79]	Time: 0.0232(0.0668)	Loss: 1.484(1.139)	Prec@1: 56.250(60.290)	
06-30-23 14:44:Step 1 * Prec@1 60.290
06-30-23 14:44:Num bit 10	Num grad bit 10	
06-30-23 14:44:Iter: [0/391]	Time 0.297 (0.297)	Data 0.250 (0.250)	Loss 0.956 (0.956)	Prec@1 65.625 (65.625)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:44:Num bit 10	Num grad bit 10	
06-30-23 14:44:Iter: [50/391]	Time 0.080 (0.067)	Data 0.002 (0.007)	Loss 0.984 (0.991)	Prec@1 60.938 (63.971)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:44:Num bit 10	Num grad bit 10	
06-30-23 14:44:Iter: [100/391]	Time 0.076 (0.066)	Data 0.003 (0.005)	Loss 0.853 (0.978)	Prec@1 70.312 (64.240)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:44:Num bit 10	Num grad bit 10	
06-30-23 14:44:Iter: [150/391]	Time 0.066 (0.067)	Data 0.003 (0.004)	Loss 0.987 (0.970)	Prec@1 64.844 (64.725)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:44:Num bit 10	Num grad bit 10	
06-30-23 14:44:Iter: [200/391]	Time 0.067 (0.068)	Data 0.002 (0.004)	Loss 0.918 (0.967)	Prec@1 66.406 (65.116)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:44:Num bit 10	Num grad bit 10	
06-30-23 14:44:Iter: [250/391]	Time 0.074 (0.069)	Data 0.002 (0.003)	Loss 0.949 (0.963)	Prec@1 63.281 (65.401)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:44:Num bit 10	Num grad bit 10	
06-30-23 14:44:Iter: [300/391]	Time 0.079 (0.072)	Data 0.002 (0.003)	Loss 0.764 (0.952)	Prec@1 71.875 (65.887)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:44:Num bit 10	Num grad bit 10	
06-30-23 14:44:Iter: [350/391]	Time 0.062 (0.073)	Data 0.002 (0.003)	Loss 0.849 (0.946)	Prec@1 68.750 (66.170)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:44:Test: [0/79]	Time: 0.3016(0.3016)	Loss: 1.023(1.023)	Prec@1: 66.406(66.406)	
06-30-23 14:44:Test: [50/79]	Time: 0.0250(0.0306)	Loss: 1.066(1.104)	Prec@1: 64.062(62.714)	
06-30-23 14:44:Test: [78/79]	Time: 0.0226(0.0287)	Loss: 0.977(1.108)	Prec@1: 75.000(62.560)	
06-30-23 14:44:Step 2 * Prec@1 62.560
06-30-23 14:44:Num bit 10	Num grad bit 10	
06-30-23 14:44:Iter: [0/391]	Time 0.355 (0.355)	Data 0.295 (0.295)	Loss 0.806 (0.806)	Prec@1 69.531 (69.531)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:44:Num bit 10	Num grad bit 10	
06-30-23 14:44:Iter: [50/391]	Time 0.067 (0.084)	Data 0.003 (0.008)	Loss 0.861 (0.876)	Prec@1 70.312 (68.857)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:45:Num bit 10	Num grad bit 10	
06-30-23 14:45:Iter: [100/391]	Time 0.157 (0.120)	Data 0.002 (0.005)	Loss 0.712 (0.870)	Prec@1 75.000 (69.237)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:45:Num bit 10	Num grad bit 10	
06-30-23 14:45:Iter: [150/391]	Time 0.158 (0.132)	Data 0.002 (0.004)	Loss 0.945 (0.852)	Prec@1 67.969 (69.868)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:45:Num bit 10	Num grad bit 10	
06-30-23 14:45:Iter: [200/391]	Time 0.057 (0.135)	Data 0.002 (0.003)	Loss 0.910 (0.847)	Prec@1 69.531 (70.149)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:45:Num bit 10	Num grad bit 10	
06-30-23 14:45:Iter: [250/391]	Time 0.073 (0.121)	Data 0.002 (0.003)	Loss 0.815 (0.850)	Prec@1 72.656 (70.129)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:45:Num bit 10	Num grad bit 10	
06-30-23 14:45:Iter: [300/391]	Time 0.068 (0.113)	Data 0.003 (0.003)	Loss 0.817 (0.842)	Prec@1 72.656 (70.440)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:45:Num bit 10	Num grad bit 10	
06-30-23 14:45:Iter: [350/391]	Time 0.069 (0.107)	Data 0.003 (0.003)	Loss 0.724 (0.837)	Prec@1 71.094 (70.577)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:45:Test: [0/79]	Time: 0.2672(0.2672)	Loss: 1.066(1.066)	Prec@1: 66.406(66.406)	
06-30-23 14:45:Test: [50/79]	Time: 0.0253(0.0307)	Loss: 1.152(0.940)	Prec@1: 69.531(69.179)	
06-30-23 14:45:Test: [78/79]	Time: 0.0228(0.0288)	Loss: 1.038(0.945)	Prec@1: 68.750(68.890)	
06-30-23 14:45:Step 3 * Prec@1 68.890
06-30-23 14:45:Num bit 10	Num grad bit 10	
06-30-23 14:45:Iter: [0/391]	Time 0.339 (0.339)	Data 0.265 (0.265)	Loss 0.937 (0.937)	Prec@1 66.406 (66.406)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:45:Num bit 10	Num grad bit 10	
06-30-23 14:45:Iter: [50/391]	Time 0.061 (0.090)	Data 0.002 (0.007)	Loss 0.891 (0.774)	Prec@1 72.656 (73.192)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:45:Num bit 10	Num grad bit 10	
06-30-23 14:45:Iter: [100/391]	Time 0.071 (0.078)	Data 0.002 (0.005)	Loss 0.811 (0.759)	Prec@1 68.750 (73.600)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:45:Num bit 10	Num grad bit 10	
06-30-23 14:45:Iter: [150/391]	Time 0.082 (0.077)	Data 0.003 (0.004)	Loss 0.867 (0.760)	Prec@1 70.312 (73.293)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:45:Num bit 10	Num grad bit 10	
06-30-23 14:45:Iter: [200/391]	Time 0.074 (0.077)	Data 0.002 (0.004)	Loss 0.712 (0.760)	Prec@1 71.094 (73.228)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:45:Num bit 10	Num grad bit 10	
06-30-23 14:45:Iter: [250/391]	Time 0.045 (0.079)	Data 0.002 (0.003)	Loss 0.904 (0.761)	Prec@1 67.188 (73.139)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [300/391]	Time 0.158 (0.088)	Data 0.002 (0.003)	Loss 0.750 (0.759)	Prec@1 69.531 (73.284)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [350/391]	Time 0.157 (0.098)	Data 0.002 (0.003)	Loss 0.672 (0.750)	Prec@1 76.562 (73.620)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Test: [0/79]	Time: 0.3338(0.3338)	Loss: 0.788(0.788)	Prec@1: 71.875(71.875)	
06-30-23 14:46:Test: [50/79]	Time: 0.0396(0.0549)	Loss: 0.791(0.848)	Prec@1: 72.656(72.289)	
06-30-23 14:46:Test: [78/79]	Time: 0.0227(0.0449)	Loss: 1.078(0.849)	Prec@1: 56.250(72.160)	
06-30-23 14:46:Step 4 * Prec@1 72.160
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [0/391]	Time 0.334 (0.334)	Data 0.274 (0.274)	Loss 0.677 (0.677)	Prec@1 74.219 (74.219)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [50/391]	Time 0.050 (0.066)	Data 0.002 (0.008)	Loss 0.684 (0.693)	Prec@1 75.000 (75.628)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [100/391]	Time 0.073 (0.062)	Data 0.003 (0.005)	Loss 0.716 (0.688)	Prec@1 75.781 (75.696)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [150/391]	Time 0.068 (0.064)	Data 0.002 (0.004)	Loss 0.848 (0.694)	Prec@1 70.312 (75.388)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [200/391]	Time 0.047 (0.063)	Data 0.002 (0.004)	Loss 0.755 (0.696)	Prec@1 75.000 (75.455)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [250/391]	Time 0.070 (0.062)	Data 0.003 (0.003)	Loss 0.698 (0.691)	Prec@1 76.562 (75.766)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [300/391]	Time 0.066 (0.067)	Data 0.002 (0.003)	Loss 0.633 (0.689)	Prec@1 78.906 (75.833)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [350/391]	Time 0.066 (0.067)	Data 0.002 (0.003)	Loss 0.587 (0.684)	Prec@1 77.344 (75.964)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Test: [0/79]	Time: 0.2547(0.2547)	Loss: 0.779(0.779)	Prec@1: 80.469(80.469)	
06-30-23 14:46:Test: [50/79]	Time: 0.0251(0.0300)	Loss: 0.884(0.843)	Prec@1: 67.969(73.621)	
06-30-23 14:46:Test: [78/79]	Time: 0.0226(0.0284)	Loss: 0.715(0.846)	Prec@1: 75.000(73.860)	
06-30-23 14:46:Step 5 * Prec@1 73.860
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [0/391]	Time 0.324 (0.324)	Data 0.264 (0.264)	Loss 0.665 (0.665)	Prec@1 74.219 (74.219)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [50/391]	Time 0.063 (0.063)	Data 0.003 (0.007)	Loss 0.666 (0.650)	Prec@1 74.219 (76.961)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:46:Num bit 10	Num grad bit 10	
06-30-23 14:46:Iter: [100/391]	Time 0.052 (0.060)	Data 0.002 (0.005)	Loss 0.650 (0.642)	Prec@1 76.562 (77.661)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [150/391]	Time 0.157 (0.069)	Data 0.002 (0.004)	Loss 0.610 (0.639)	Prec@1 77.344 (77.804)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [200/391]	Time 0.157 (0.091)	Data 0.002 (0.003)	Loss 0.693 (0.637)	Prec@1 75.781 (77.923)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [250/391]	Time 0.157 (0.104)	Data 0.002 (0.003)	Loss 0.660 (0.631)	Prec@1 77.344 (78.144)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [300/391]	Time 0.064 (0.108)	Data 0.002 (0.003)	Loss 0.513 (0.631)	Prec@1 84.375 (78.244)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [350/391]	Time 0.067 (0.103)	Data 0.002 (0.003)	Loss 0.647 (0.624)	Prec@1 75.000 (78.405)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Test: [0/79]	Time: 0.2894(0.2894)	Loss: 0.533(0.533)	Prec@1: 81.250(81.250)	
06-30-23 14:47:Test: [50/79]	Time: 0.0249(0.0308)	Loss: 0.626(0.612)	Prec@1: 77.344(79.182)	
06-30-23 14:47:Test: [78/79]	Time: 0.0226(0.0288)	Loss: 0.825(0.614)	Prec@1: 75.000(78.970)	
06-30-23 14:47:Step 6 * Prec@1 78.970
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [0/391]	Time 0.332 (0.332)	Data 0.275 (0.275)	Loss 0.612 (0.612)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [50/391]	Time 0.076 (0.073)	Data 0.003 (0.008)	Loss 0.431 (0.605)	Prec@1 84.375 (78.891)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [100/391]	Time 0.063 (0.074)	Data 0.002 (0.005)	Loss 0.667 (0.601)	Prec@1 76.562 (79.146)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [150/391]	Time 0.073 (0.078)	Data 0.003 (0.004)	Loss 0.553 (0.599)	Prec@1 81.250 (79.212)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [200/391]	Time 0.064 (0.077)	Data 0.002 (0.004)	Loss 0.633 (0.598)	Prec@1 79.688 (79.217)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [250/391]	Time 0.072 (0.076)	Data 0.002 (0.003)	Loss 0.594 (0.595)	Prec@1 75.781 (79.423)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [300/391]	Time 0.076 (0.076)	Data 0.002 (0.003)	Loss 0.610 (0.591)	Prec@1 78.125 (79.578)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:47:Num bit 10	Num grad bit 10	
06-30-23 14:47:Iter: [350/391]	Time 0.058 (0.077)	Data 0.002 (0.003)	Loss 0.622 (0.592)	Prec@1 78.125 (79.563)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:48:Test: [0/79]	Time: 0.3405(0.3405)	Loss: 0.464(0.464)	Prec@1: 82.031(82.031)	
06-30-23 14:48:Test: [50/79]	Time: 0.1035(0.1078)	Loss: 0.669(0.599)	Prec@1: 76.562(79.994)	
06-30-23 14:48:Test: [78/79]	Time: 0.1003(0.1061)	Loss: 0.531(0.600)	Prec@1: 81.250(79.960)	
06-30-23 14:48:Step 7 * Prec@1 79.960
06-30-23 14:48:Num bit 10	Num grad bit 10	
06-30-23 14:48:Iter: [0/391]	Time 0.407 (0.407)	Data 0.250 (0.250)	Loss 0.718 (0.718)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:48:Num bit 10	Num grad bit 10	
06-30-23 14:48:Iter: [50/391]	Time 0.052 (0.152)	Data 0.002 (0.007)	Loss 0.571 (0.566)	Prec@1 80.469 (80.331)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:48:Num bit 10	Num grad bit 10	
06-30-23 14:48:Iter: [100/391]	Time 0.068 (0.106)	Data 0.002 (0.005)	Loss 0.566 (0.560)	Prec@1 77.344 (80.330)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:48:Num bit 10	Num grad bit 10	
06-30-23 14:48:Iter: [150/391]	Time 0.075 (0.094)	Data 0.002 (0.004)	Loss 0.665 (0.565)	Prec@1 75.781 (80.339)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:48:Num bit 10	Num grad bit 10	
06-30-23 14:48:Iter: [200/391]	Time 0.078 (0.089)	Data 0.002 (0.003)	Loss 0.389 (0.560)	Prec@1 89.062 (80.613)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:48:Num bit 10	Num grad bit 10	
06-30-23 14:48:Iter: [250/391]	Time 0.071 (0.086)	Data 0.002 (0.003)	Loss 0.562 (0.554)	Prec@1 83.594 (80.836)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:48:Num bit 10	Num grad bit 10	
06-30-23 14:48:Iter: [300/391]	Time 0.063 (0.084)	Data 0.002 (0.003)	Loss 0.437 (0.551)	Prec@1 85.156 (80.941)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:48:Num bit 10	Num grad bit 10	
06-30-23 14:48:Iter: [350/391]	Time 0.079 (0.085)	Data 0.003 (0.003)	Loss 0.745 (0.549)	Prec@1 75.781 (80.938)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:48:Test: [0/79]	Time: 0.3174(0.3174)	Loss: 0.464(0.464)	Prec@1: 85.156(85.156)	
06-30-23 14:48:Test: [50/79]	Time: 0.0252(0.0345)	Loss: 0.689(0.648)	Prec@1: 78.906(77.972)	
06-30-23 14:48:Test: [78/79]	Time: 0.0226(0.0313)	Loss: 0.856(0.654)	Prec@1: 75.000(77.890)	
06-30-23 14:48:Step 8 * Prec@1 77.890
06-30-23 14:48:Num bit 10	Num grad bit 10	
06-30-23 14:48:Iter: [0/391]	Time 0.322 (0.322)	Data 0.262 (0.262)	Loss 0.522 (0.522)	Prec@1 82.031 (82.031)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:48:Num bit 10	Num grad bit 10	
06-30-23 14:48:Iter: [50/391]	Time 0.079 (0.077)	Data 0.002 (0.007)	Loss 0.495 (0.526)	Prec@1 78.906 (81.526)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:48:Num bit 10	Num grad bit 10	
06-30-23 14:48:Iter: [100/391]	Time 0.062 (0.081)	Data 0.002 (0.005)	Loss 0.489 (0.527)	Prec@1 79.688 (81.513)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [150/391]	Time 0.157 (0.103)	Data 0.002 (0.004)	Loss 0.555 (0.530)	Prec@1 80.469 (81.353)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [200/391]	Time 0.157 (0.116)	Data 0.002 (0.003)	Loss 0.559 (0.529)	Prec@1 82.812 (81.468)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [250/391]	Time 0.051 (0.124)	Data 0.002 (0.003)	Loss 0.532 (0.525)	Prec@1 81.250 (81.655)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [300/391]	Time 0.060 (0.114)	Data 0.002 (0.003)	Loss 0.474 (0.526)	Prec@1 82.031 (81.637)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [350/391]	Time 0.064 (0.106)	Data 0.002 (0.003)	Loss 0.588 (0.532)	Prec@1 81.250 (81.524)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Test: [0/79]	Time: 0.2993(0.2993)	Loss: 0.641(0.641)	Prec@1: 80.469(80.469)	
06-30-23 14:49:Test: [50/79]	Time: 0.0254(0.0305)	Loss: 1.085(0.829)	Prec@1: 70.312(75.260)	
06-30-23 14:49:Test: [78/79]	Time: 0.0226(0.0291)	Loss: 0.513(0.841)	Prec@1: 68.750(75.290)	
06-30-23 14:49:Step 9 * Prec@1 75.290
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [0/391]	Time 0.323 (0.323)	Data 0.272 (0.272)	Loss 0.440 (0.440)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [50/391]	Time 0.073 (0.072)	Data 0.003 (0.008)	Loss 0.631 (0.533)	Prec@1 82.031 (82.261)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [100/391]	Time 0.158 (0.081)	Data 0.002 (0.005)	Loss 0.589 (0.524)	Prec@1 78.906 (82.225)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [150/391]	Time 0.072 (0.077)	Data 0.002 (0.004)	Loss 0.538 (0.525)	Prec@1 82.031 (82.181)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [200/391]	Time 0.066 (0.076)	Data 0.002 (0.004)	Loss 0.318 (0.522)	Prec@1 89.844 (82.035)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [250/391]	Time 0.074 (0.075)	Data 0.002 (0.003)	Loss 0.573 (0.530)	Prec@1 77.344 (81.723)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [300/391]	Time 0.070 (0.074)	Data 0.003 (0.003)	Loss 0.433 (0.530)	Prec@1 84.375 (81.694)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:49:Num bit 10	Num grad bit 10	
06-30-23 14:49:Iter: [350/391]	Time 0.157 (0.079)	Data 0.002 (0.003)	Loss 0.628 (0.531)	Prec@1 78.125 (81.673)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:50:Test: [0/79]	Time: 0.3598(0.3598)	Loss: 0.546(0.546)	Prec@1: 82.812(82.812)	
06-30-23 14:50:Test: [50/79]	Time: 0.1036(0.1079)	Loss: 0.749(0.687)	Prec@1: 78.125(79.504)	
06-30-23 14:50:Test: [78/79]	Time: 0.1003(0.1062)	Loss: 0.295(0.692)	Prec@1: 87.500(79.240)	
06-30-23 14:50:Step 10 * Prec@1 79.240
06-30-23 14:50:Num bit 10	Num grad bit 10	
06-30-23 14:50:Iter: [0/391]	Time 0.415 (0.415)	Data 0.260 (0.260)	Loss 0.481 (0.481)	Prec@1 82.812 (82.812)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:50:Num bit 10	Num grad bit 10	
06-30-23 14:50:Iter: [50/391]	Time 0.063 (0.121)	Data 0.002 (0.007)	Loss 0.587 (0.509)	Prec@1 78.125 (82.138)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:50:Num bit 10	Num grad bit 10	
06-30-23 14:50:Iter: [100/391]	Time 0.075 (0.094)	Data 0.002 (0.005)	Loss 0.556 (0.506)	Prec@1 82.812 (82.565)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:50:Num bit 10	Num grad bit 10	
06-30-23 14:50:Iter: [150/391]	Time 0.071 (0.087)	Data 0.002 (0.004)	Loss 0.469 (0.506)	Prec@1 83.594 (82.637)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:50:Num bit 10	Num grad bit 10	
06-30-23 14:50:Iter: [200/391]	Time 0.069 (0.082)	Data 0.002 (0.004)	Loss 0.457 (0.512)	Prec@1 83.594 (82.459)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:50:Num bit 10	Num grad bit 10	
06-30-23 14:50:Iter: [250/391]	Time 0.070 (0.080)	Data 0.002 (0.003)	Loss 0.575 (0.518)	Prec@1 79.688 (82.296)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:50:Num bit 10	Num grad bit 10	
06-30-23 14:50:Iter: [300/391]	Time 0.156 (0.081)	Data 0.002 (0.003)	Loss 0.478 (0.518)	Prec@1 82.812 (82.270)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:50:Num bit 10	Num grad bit 10	
06-30-23 14:50:Iter: [350/391]	Time 0.075 (0.081)	Data 0.002 (0.003)	Loss 0.446 (0.518)	Prec@1 81.250 (82.285)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:50:Test: [0/79]	Time: 0.2869(0.2869)	Loss: 0.501(0.501)	Prec@1: 84.375(84.375)	
06-30-23 14:50:Test: [50/79]	Time: 0.0253(0.0305)	Loss: 0.625(0.609)	Prec@1: 82.031(80.070)	
06-30-23 14:50:Test: [78/79]	Time: 0.0239(0.0287)	Loss: 0.746(0.622)	Prec@1: 81.250(79.680)	
06-30-23 14:50:Step 11 * Prec@1 79.680
06-30-23 14:50:Num bit 10	Num grad bit 10	
06-30-23 14:50:Iter: [0/391]	Time 0.397 (0.397)	Data 0.332 (0.332)	Loss 0.535 (0.535)	Prec@1 81.250 (81.250)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:50:Num bit 10	Num grad bit 10	
06-30-23 14:50:Iter: [50/391]	Time 0.054 (0.069)	Data 0.002 (0.008)	Loss 0.530 (0.494)	Prec@1 85.156 (82.889)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:50:Num bit 10	Num grad bit 10	
06-30-23 14:50:Iter: [100/391]	Time 0.059 (0.073)	Data 0.002 (0.005)	Loss 0.378 (0.505)	Prec@1 89.062 (82.604)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:51:Num bit 10	Num grad bit 10	
06-30-23 14:51:Iter: [150/391]	Time 0.163 (0.100)	Data 0.002 (0.004)	Loss 0.463 (0.510)	Prec@1 82.812 (82.414)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:51:Num bit 10	Num grad bit 10	
06-30-23 14:51:Iter: [200/391]	Time 0.199 (0.121)	Data 0.002 (0.004)	Loss 0.468 (0.508)	Prec@1 84.375 (82.529)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:51:Num bit 10	Num grad bit 10	
06-30-23 14:51:Iter: [250/391]	Time 0.050 (0.130)	Data 0.002 (0.003)	Loss 0.354 (0.505)	Prec@1 87.500 (82.626)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:51:Num bit 10	Num grad bit 10	
06-30-23 14:51:Iter: [300/391]	Time 0.092 (0.120)	Data 0.003 (0.003)	Loss 0.503 (0.503)	Prec@1 82.031 (82.607)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:51:Num bit 10	Num grad bit 10	
06-30-23 14:51:Iter: [350/391]	Time 0.075 (0.112)	Data 0.002 (0.003)	Loss 0.404 (0.503)	Prec@1 87.500 (82.668)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:51:Test: [0/79]	Time: 0.2756(0.2756)	Loss: 0.490(0.490)	Prec@1: 83.594(83.594)	
06-30-23 14:51:Test: [50/79]	Time: 0.0258(0.0309)	Loss: 0.858(0.630)	Prec@1: 75.781(79.657)	
06-30-23 14:51:Test: [78/79]	Time: 0.0227(0.0290)	Loss: 0.721(0.625)	Prec@1: 81.250(79.690)	
06-30-23 14:51:Step 12 * Prec@1 79.690
06-30-23 14:51:Num bit 10	Num grad bit 10	
06-30-23 14:51:Iter: [0/391]	Time 0.305 (0.305)	Data 0.252 (0.252)	Loss 0.472 (0.472)	Prec@1 82.031 (82.031)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:51:Num bit 10	Num grad bit 10	
06-30-23 14:51:Iter: [50/391]	Time 0.077 (0.076)	Data 0.003 (0.007)	Loss 0.544 (0.489)	Prec@1 83.594 (83.364)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:51:Num bit 10	Num grad bit 10	
06-30-23 14:51:Iter: [100/391]	Time 0.063 (0.083)	Data 0.002 (0.005)	Loss 0.493 (0.487)	Prec@1 84.375 (83.369)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:51:Num bit 10	Num grad bit 10	
06-30-23 14:51:Iter: [150/391]	Time 0.077 (0.078)	Data 0.003 (0.004)	Loss 0.438 (0.493)	Prec@1 82.812 (83.164)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:51:Num bit 10	Num grad bit 10	
06-30-23 14:51:Iter: [200/391]	Time 0.073 (0.077)	Data 0.002 (0.004)	Loss 0.430 (0.491)	Prec@1 85.156 (83.217)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:51:Num bit 10	Num grad bit 10	
06-30-23 14:51:Iter: [250/391]	Time 0.076 (0.076)	Data 0.003 (0.003)	Loss 0.526 (0.489)	Prec@1 84.375 (83.254)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:51:Num bit 10	Num grad bit 10	
06-30-23 14:51:Iter: [300/391]	Time 0.159 (0.078)	Data 0.002 (0.003)	Loss 0.517 (0.487)	Prec@1 79.688 (83.272)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:52:Num bit 10	Num grad bit 10	
06-30-23 14:52:Iter: [350/391]	Time 0.157 (0.084)	Data 0.002 (0.003)	Loss 0.494 (0.490)	Prec@1 81.250 (83.102)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:52:Test: [0/79]	Time: 0.3343(0.3343)	Loss: 0.490(0.490)	Prec@1: 82.812(82.812)	
06-30-23 14:52:Test: [50/79]	Time: 0.1013(0.1076)	Loss: 0.679(0.544)	Prec@1: 79.688(82.184)	
06-30-23 14:52:Test: [78/79]	Time: 0.1003(0.1060)	Loss: 0.542(0.554)	Prec@1: 87.500(82.150)	
06-30-23 14:52:Step 13 * Prec@1 82.150
06-30-23 14:52:Num bit 10	Num grad bit 10	
06-30-23 14:52:Iter: [0/391]	Time 0.425 (0.425)	Data 0.267 (0.267)	Loss 0.343 (0.343)	Prec@1 89.844 (89.844)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:52:Num bit 10	Num grad bit 10	
06-30-23 14:52:Iter: [50/391]	Time 0.067 (0.093)	Data 0.002 (0.007)	Loss 0.420 (0.465)	Prec@1 85.938 (84.069)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:52:Num bit 10	Num grad bit 10	
06-30-23 14:52:Iter: [100/391]	Time 0.071 (0.080)	Data 0.003 (0.005)	Loss 0.407 (0.471)	Prec@1 83.594 (83.756)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:52:Num bit 10	Num grad bit 10	
06-30-23 14:52:Iter: [150/391]	Time 0.069 (0.078)	Data 0.003 (0.004)	Loss 0.368 (0.469)	Prec@1 84.375 (83.811)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:52:Num bit 10	Num grad bit 10	
06-30-23 14:52:Iter: [200/391]	Time 0.073 (0.076)	Data 0.003 (0.004)	Loss 0.430 (0.470)	Prec@1 85.938 (83.695)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:52:Num bit 10	Num grad bit 10	
06-30-23 14:52:Iter: [250/391]	Time 0.075 (0.076)	Data 0.003 (0.003)	Loss 0.485 (0.476)	Prec@1 83.594 (83.544)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:52:Num bit 10	Num grad bit 10	
06-30-23 14:52:Iter: [300/391]	Time 0.049 (0.078)	Data 0.002 (0.003)	Loss 0.439 (0.478)	Prec@1 86.719 (83.448)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:52:Num bit 10	Num grad bit 10	
06-30-23 14:52:Iter: [350/391]	Time 0.073 (0.076)	Data 0.003 (0.003)	Loss 0.376 (0.476)	Prec@1 87.500 (83.543)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:52:Test: [0/79]	Time: 0.3028(0.3028)	Loss: 0.487(0.487)	Prec@1: 85.156(85.156)	
06-30-23 14:52:Test: [50/79]	Time: 0.0254(0.0325)	Loss: 0.619(0.606)	Prec@1: 82.812(81.127)	
06-30-23 14:52:Test: [78/79]	Time: 0.0243(0.0300)	Loss: 0.883(0.612)	Prec@1: 75.000(80.870)	
06-30-23 14:52:Step 14 * Prec@1 80.870
06-30-23 14:52:Num bit 10	Num grad bit 10	
06-30-23 14:52:Iter: [0/391]	Time 0.305 (0.305)	Data 0.253 (0.253)	Loss 0.432 (0.432)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:52:Num bit 10	Num grad bit 10	
06-30-23 14:52:Iter: [50/391]	Time 0.077 (0.075)	Data 0.002 (0.007)	Loss 0.503 (0.482)	Prec@1 84.375 (83.349)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:52:Num bit 10	Num grad bit 10	
06-30-23 14:52:Iter: [100/391]	Time 0.157 (0.085)	Data 0.002 (0.005)	Loss 0.460 (0.490)	Prec@1 83.594 (83.168)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [150/391]	Time 0.157 (0.114)	Data 0.002 (0.004)	Loss 0.318 (0.479)	Prec@1 89.062 (83.454)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [200/391]	Time 0.157 (0.125)	Data 0.002 (0.003)	Loss 0.325 (0.473)	Prec@1 90.625 (83.710)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [250/391]	Time 0.056 (0.122)	Data 0.002 (0.003)	Loss 0.379 (0.469)	Prec@1 86.719 (83.815)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [300/391]	Time 0.065 (0.112)	Data 0.002 (0.003)	Loss 0.483 (0.469)	Prec@1 84.375 (83.866)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [350/391]	Time 0.057 (0.106)	Data 0.002 (0.003)	Loss 0.455 (0.468)	Prec@1 82.031 (83.854)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Test: [0/79]	Time: 0.2989(0.2989)	Loss: 0.475(0.475)	Prec@1: 83.594(83.594)	
06-30-23 14:53:Test: [50/79]	Time: 0.0307(0.0354)	Loss: 0.627(0.536)	Prec@1: 79.688(82.675)	
06-30-23 14:53:Test: [78/79]	Time: 0.0281(0.0335)	Loss: 0.394(0.533)	Prec@1: 93.750(82.680)	
06-30-23 14:53:Step 15 * Prec@1 82.680
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [0/391]	Time 0.331 (0.331)	Data 0.271 (0.271)	Loss 0.449 (0.449)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [50/391]	Time 0.066 (0.072)	Data 0.002 (0.007)	Loss 0.414 (0.431)	Prec@1 86.719 (85.493)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [100/391]	Time 0.071 (0.070)	Data 0.002 (0.005)	Loss 0.549 (0.442)	Prec@1 81.250 (84.800)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [150/391]	Time 0.158 (0.071)	Data 0.003 (0.004)	Loss 0.457 (0.438)	Prec@1 83.594 (84.758)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [200/391]	Time 0.048 (0.072)	Data 0.002 (0.004)	Loss 0.472 (0.444)	Prec@1 87.500 (84.678)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [250/391]	Time 0.052 (0.069)	Data 0.002 (0.003)	Loss 0.378 (0.446)	Prec@1 88.281 (84.630)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [300/391]	Time 0.048 (0.067)	Data 0.002 (0.003)	Loss 0.427 (0.446)	Prec@1 85.156 (84.658)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [350/391]	Time 0.054 (0.065)	Data 0.002 (0.003)	Loss 0.577 (0.448)	Prec@1 81.250 (84.553)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:53:Test: [0/79]	Time: 0.2803(0.2803)	Loss: 0.556(0.556)	Prec@1: 84.375(84.375)	
06-30-23 14:53:Test: [50/79]	Time: 0.1045(0.0451)	Loss: 0.792(0.588)	Prec@1: 76.562(81.602)	
06-30-23 14:53:Test: [78/79]	Time: 0.0152(0.0405)	Loss: 0.618(0.592)	Prec@1: 81.250(81.200)	
06-30-23 14:53:Step 16 * Prec@1 81.200
06-30-23 14:53:Num bit 10	Num grad bit 10	
06-30-23 14:53:Iter: [0/391]	Time 0.303 (0.303)	Data 0.260 (0.260)	Loss 0.353 (0.353)	Prec@1 88.281 (88.281)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:54:Num bit 10	Num grad bit 10	
06-30-23 14:54:Iter: [50/391]	Time 0.157 (0.160)	Data 0.002 (0.007)	Loss 0.438 (0.459)	Prec@1 88.281 (84.452)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:54:Num bit 10	Num grad bit 10	
06-30-23 14:54:Iter: [100/391]	Time 0.157 (0.158)	Data 0.002 (0.005)	Loss 0.444 (0.452)	Prec@1 86.719 (84.537)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:54:Num bit 10	Num grad bit 10	
06-30-23 14:54:Iter: [150/391]	Time 0.054 (0.153)	Data 0.002 (0.004)	Loss 0.475 (0.448)	Prec@1 83.594 (84.535)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:54:Num bit 10	Num grad bit 10	
06-30-23 14:54:Iter: [200/391]	Time 0.061 (0.129)	Data 0.002 (0.003)	Loss 0.408 (0.448)	Prec@1 88.281 (84.507)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:54:Num bit 10	Num grad bit 10	
06-30-23 14:54:Iter: [250/391]	Time 0.067 (0.117)	Data 0.002 (0.003)	Loss 0.421 (0.450)	Prec@1 83.594 (84.484)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:54:Num bit 10	Num grad bit 10	
06-30-23 14:54:Iter: [300/391]	Time 0.074 (0.109)	Data 0.002 (0.003)	Loss 0.430 (0.445)	Prec@1 88.281 (84.627)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:54:Num bit 10	Num grad bit 10	
06-30-23 14:54:Iter: [350/391]	Time 0.059 (0.104)	Data 0.002 (0.003)	Loss 0.494 (0.446)	Prec@1 82.031 (84.575)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:54:Test: [0/79]	Time: 0.2698(0.2698)	Loss: 0.433(0.433)	Prec@1: 82.812(82.812)	
06-30-23 14:54:Test: [50/79]	Time: 0.0240(0.0292)	Loss: 0.607(0.527)	Prec@1: 79.688(82.506)	
06-30-23 14:54:Test: [78/79]	Time: 0.0257(0.0453)	Loss: 0.564(0.532)	Prec@1: 81.250(82.440)	
06-30-23 14:54:Step 17 * Prec@1 82.440
06-30-23 14:54:Num bit 10	Num grad bit 10	
06-30-23 14:54:Iter: [0/391]	Time 0.298 (0.298)	Data 0.252 (0.252)	Loss 0.465 (0.465)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:54:Num bit 10	Num grad bit 10	
06-30-23 14:54:Iter: [50/391]	Time 0.071 (0.075)	Data 0.002 (0.007)	Loss 0.363 (0.422)	Prec@1 88.281 (85.447)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:54:Num bit 10	Num grad bit 10	
06-30-23 14:54:Iter: [100/391]	Time 0.079 (0.075)	Data 0.002 (0.005)	Loss 0.448 (0.428)	Prec@1 85.156 (85.512)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:54:Num bit 10	Num grad bit 10	
06-30-23 14:54:Iter: [150/391]	Time 0.072 (0.074)	Data 0.003 (0.004)	Loss 0.303 (0.436)	Prec@1 89.844 (85.234)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:54:Num bit 10	Num grad bit 10	
06-30-23 14:54:Iter: [200/391]	Time 0.066 (0.077)	Data 0.003 (0.004)	Loss 0.475 (0.435)	Prec@1 85.938 (85.187)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:55:Num bit 10	Num grad bit 10	
06-30-23 14:55:Iter: [250/391]	Time 0.157 (0.087)	Data 0.002 (0.003)	Loss 0.396 (0.434)	Prec@1 86.719 (85.169)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:55:Num bit 10	Num grad bit 10	
06-30-23 14:55:Iter: [300/391]	Time 0.157 (0.099)	Data 0.002 (0.003)	Loss 0.304 (0.430)	Prec@1 91.406 (85.296)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:55:Num bit 10	Num grad bit 10	
06-30-23 14:55:Iter: [350/391]	Time 0.157 (0.107)	Data 0.002 (0.003)	Loss 0.366 (0.433)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:55:Test: [0/79]	Time: 0.2718(0.2718)	Loss: 0.379(0.379)	Prec@1: 84.375(84.375)	
06-30-23 14:55:Test: [50/79]	Time: 0.0252(0.0313)	Loss: 0.663(0.520)	Prec@1: 85.156(83.349)	
06-30-23 14:55:Test: [78/79]	Time: 0.0255(0.0293)	Loss: 0.591(0.525)	Prec@1: 87.500(83.350)	
06-30-23 14:55:Step 18 * Prec@1 83.350
06-30-23 14:55:Num bit 10	Num grad bit 10	
06-30-23 14:55:Iter: [0/391]	Time 0.322 (0.322)	Data 0.269 (0.269)	Loss 0.504 (0.504)	Prec@1 81.250 (81.250)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:55:Num bit 10	Num grad bit 10	
06-30-23 14:55:Iter: [50/391]	Time 0.063 (0.070)	Data 0.002 (0.008)	Loss 0.332 (0.414)	Prec@1 89.062 (85.723)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:55:Num bit 10	Num grad bit 10	
06-30-23 14:55:Iter: [100/391]	Time 0.067 (0.067)	Data 0.002 (0.005)	Loss 0.230 (0.413)	Prec@1 93.750 (85.605)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:55:Num bit 10	Num grad bit 10	
06-30-23 14:55:Iter: [150/391]	Time 0.066 (0.067)	Data 0.002 (0.004)	Loss 0.360 (0.412)	Prec@1 88.281 (85.689)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:55:Num bit 10	Num grad bit 10	
06-30-23 14:55:Iter: [200/391]	Time 0.158 (0.072)	Data 0.002 (0.004)	Loss 0.444 (0.418)	Prec@1 85.938 (85.401)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:55:Num bit 10	Num grad bit 10	
06-30-23 14:55:Iter: [250/391]	Time 0.079 (0.072)	Data 0.003 (0.003)	Loss 0.436 (0.421)	Prec@1 85.938 (85.334)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:55:Num bit 10	Num grad bit 10	
06-30-23 14:55:Iter: [300/391]	Time 0.076 (0.073)	Data 0.002 (0.003)	Loss 0.451 (0.425)	Prec@1 86.719 (85.265)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:55:Num bit 10	Num grad bit 10	
06-30-23 14:55:Iter: [350/391]	Time 0.063 (0.072)	Data 0.002 (0.003)	Loss 0.440 (0.429)	Prec@1 84.375 (85.143)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:55:Test: [0/79]	Time: 0.2845(0.2845)	Loss: 0.423(0.423)	Prec@1: 88.281(88.281)	
06-30-23 14:55:Test: [50/79]	Time: 0.0170(0.0499)	Loss: 0.649(0.504)	Prec@1: 83.594(83.578)	
06-30-23 14:55:Test: [78/79]	Time: 0.0154(0.0383)	Loss: 0.683(0.509)	Prec@1: 87.500(83.390)	
06-30-23 14:55:Step 19 * Prec@1 83.390
06-30-23 14:55:Num bit 10	Num grad bit 10	
06-30-23 14:55:Iter: [0/391]	Time 0.414 (0.414)	Data 0.258 (0.258)	Loss 0.395 (0.395)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [50/391]	Time 0.157 (0.163)	Data 0.002 (0.007)	Loss 0.382 (0.410)	Prec@1 83.594 (85.386)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [100/391]	Time 0.164 (0.162)	Data 0.002 (0.005)	Loss 0.366 (0.404)	Prec@1 85.938 (85.868)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [150/391]	Time 0.050 (0.157)	Data 0.002 (0.004)	Loss 0.299 (0.412)	Prec@1 87.500 (85.689)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [200/391]	Time 0.063 (0.136)	Data 0.002 (0.003)	Loss 0.495 (0.417)	Prec@1 86.719 (85.529)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [250/391]	Time 0.056 (0.123)	Data 0.002 (0.003)	Loss 0.396 (0.417)	Prec@1 85.938 (85.483)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [300/391]	Time 0.059 (0.114)	Data 0.002 (0.003)	Loss 0.456 (0.418)	Prec@1 86.719 (85.491)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [350/391]	Time 0.058 (0.108)	Data 0.002 (0.003)	Loss 0.450 (0.420)	Prec@1 83.594 (85.510)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Test: [0/79]	Time: 0.3385(0.3385)	Loss: 0.503(0.503)	Prec@1: 84.375(84.375)	
06-30-23 14:56:Test: [50/79]	Time: 0.0247(0.0363)	Loss: 0.766(0.649)	Prec@1: 78.125(79.580)	
06-30-23 14:56:Test: [78/79]	Time: 0.0235(0.0339)	Loss: 0.390(0.651)	Prec@1: 87.500(79.370)	
06-30-23 14:56:Step 20 * Prec@1 79.370
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [0/391]	Time 0.311 (0.311)	Data 0.252 (0.252)	Loss 0.352 (0.352)	Prec@1 89.844 (89.844)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [50/391]	Time 0.114 (0.098)	Data 0.002 (0.007)	Loss 0.389 (0.401)	Prec@1 86.719 (86.152)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [100/391]	Time 0.062 (0.084)	Data 0.002 (0.005)	Loss 0.328 (0.398)	Prec@1 86.719 (86.340)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [150/391]	Time 0.094 (0.079)	Data 0.003 (0.004)	Loss 0.360 (0.395)	Prec@1 85.156 (86.331)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [200/391]	Time 0.067 (0.078)	Data 0.003 (0.003)	Loss 0.429 (0.403)	Prec@1 85.938 (86.093)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:56:Num bit 10	Num grad bit 10	
06-30-23 14:56:Iter: [250/391]	Time 0.056 (0.076)	Data 0.002 (0.003)	Loss 0.418 (0.405)	Prec@1 82.031 (85.959)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:57:Num bit 10	Num grad bit 10	
06-30-23 14:57:Iter: [300/391]	Time 0.159 (0.077)	Data 0.002 (0.003)	Loss 0.427 (0.406)	Prec@1 85.156 (85.834)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:57:Num bit 10	Num grad bit 10	
06-30-23 14:57:Iter: [350/391]	Time 0.157 (0.086)	Data 0.002 (0.003)	Loss 0.438 (0.407)	Prec@1 84.375 (85.820)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:57:Test: [0/79]	Time: 0.3700(0.3700)	Loss: 0.500(0.500)	Prec@1: 84.375(84.375)	
06-30-23 14:57:Test: [50/79]	Time: 0.1333(0.1250)	Loss: 0.776(0.648)	Prec@1: 75.781(79.672)	
06-30-23 14:57:Test: [78/79]	Time: 0.1004(0.1228)	Loss: 0.298(0.636)	Prec@1: 81.250(80.130)	
06-30-23 14:57:Step 21 * Prec@1 80.130
06-30-23 14:57:Num bit 10	Num grad bit 10	
06-30-23 14:57:Iter: [0/391]	Time 0.471 (0.471)	Data 0.280 (0.280)	Loss 0.317 (0.317)	Prec@1 90.625 (90.625)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:57:Num bit 10	Num grad bit 10	
06-30-23 14:57:Iter: [50/391]	Time 0.065 (0.109)	Data 0.002 (0.008)	Loss 0.340 (0.403)	Prec@1 87.500 (86.351)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:57:Num bit 10	Num grad bit 10	
06-30-23 14:57:Iter: [100/391]	Time 0.057 (0.090)	Data 0.003 (0.005)	Loss 0.436 (0.404)	Prec@1 85.156 (86.046)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:57:Num bit 10	Num grad bit 10	
06-30-23 14:57:Iter: [150/391]	Time 0.066 (0.083)	Data 0.002 (0.004)	Loss 0.401 (0.409)	Prec@1 88.281 (85.963)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:57:Num bit 10	Num grad bit 10	
06-30-23 14:57:Iter: [200/391]	Time 0.057 (0.079)	Data 0.002 (0.004)	Loss 0.465 (0.405)	Prec@1 82.812 (86.109)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:57:Num bit 10	Num grad bit 10	
06-30-23 14:57:Iter: [250/391]	Time 0.080 (0.076)	Data 0.002 (0.003)	Loss 0.500 (0.404)	Prec@1 85.156 (86.168)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:57:Num bit 10	Num grad bit 10	
06-30-23 14:57:Iter: [300/391]	Time 0.111 (0.073)	Data 0.002 (0.003)	Loss 0.352 (0.404)	Prec@1 85.938 (86.184)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:57:Num bit 10	Num grad bit 10	
06-30-23 14:57:Iter: [350/391]	Time 0.118 (0.073)	Data 0.002 (0.003)	Loss 0.287 (0.405)	Prec@1 90.625 (86.144)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:57:Test: [0/79]	Time: 0.2556(0.2556)	Loss: 0.560(0.560)	Prec@1: 82.031(82.031)	
06-30-23 14:57:Test: [50/79]	Time: 0.0259(0.0357)	Loss: 0.861(0.690)	Prec@1: 74.219(80.025)	
06-30-23 14:58:Test: [78/79]	Time: 0.0256(0.0341)	Loss: 0.581(0.685)	Prec@1: 93.750(80.240)	
06-30-23 14:58:Step 22 * Prec@1 80.240
06-30-23 14:58:Num bit 10	Num grad bit 10	
06-30-23 14:58:Iter: [0/391]	Time 0.322 (0.322)	Data 0.274 (0.274)	Loss 0.421 (0.421)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:58:Num bit 10	Num grad bit 10	
06-30-23 14:58:Iter: [50/391]	Time 0.069 (0.074)	Data 0.002 (0.008)	Loss 0.349 (0.383)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:58:Num bit 10	Num grad bit 10	
06-30-23 14:58:Iter: [100/391]	Time 0.064 (0.074)	Data 0.002 (0.005)	Loss 0.477 (0.391)	Prec@1 83.594 (86.433)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:58:Num bit 10	Num grad bit 10	
06-30-23 14:58:Iter: [150/391]	Time 0.063 (0.072)	Data 0.003 (0.004)	Loss 0.437 (0.398)	Prec@1 81.250 (86.232)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:58:Num bit 10	Num grad bit 10	
06-30-23 14:58:Iter: [200/391]	Time 0.047 (0.076)	Data 0.002 (0.004)	Loss 0.521 (0.398)	Prec@1 83.594 (86.213)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:58:Num bit 10	Num grad bit 10	
06-30-23 14:58:Iter: [250/391]	Time 0.194 (0.096)	Data 0.002 (0.003)	Loss 0.372 (0.398)	Prec@1 86.719 (86.187)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:58:Num bit 10	Num grad bit 10	
06-30-23 14:58:Iter: [300/391]	Time 0.214 (0.110)	Data 0.002 (0.003)	Loss 0.439 (0.397)	Prec@1 85.938 (86.259)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:58:Num bit 10	Num grad bit 10	
06-30-23 14:58:Iter: [350/391]	Time 0.049 (0.120)	Data 0.002 (0.003)	Loss 0.433 (0.400)	Prec@1 86.719 (86.153)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:58:Test: [0/79]	Time: 0.2924(0.2924)	Loss: 0.485(0.485)	Prec@1: 88.281(88.281)	
06-30-23 14:58:Test: [50/79]	Time: 0.0259(0.0376)	Loss: 0.567(0.531)	Prec@1: 85.938(83.088)	
06-30-23 14:58:Test: [78/79]	Time: 0.0234(0.0353)	Loss: 0.315(0.534)	Prec@1: 93.750(83.000)	
06-30-23 14:58:Step 23 * Prec@1 83.000
06-30-23 14:58:Num bit 10	Num grad bit 10	
06-30-23 14:58:Iter: [0/391]	Time 0.334 (0.334)	Data 0.287 (0.287)	Loss 0.394 (0.394)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:58:Num bit 10	Num grad bit 10	
06-30-23 14:58:Iter: [50/391]	Time 0.068 (0.077)	Data 0.003 (0.008)	Loss 0.338 (0.388)	Prec@1 87.500 (86.397)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:58:Num bit 10	Num grad bit 10	
06-30-23 14:58:Iter: [100/391]	Time 0.069 (0.076)	Data 0.002 (0.005)	Loss 0.413 (0.380)	Prec@1 85.938 (86.750)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:58:Num bit 10	Num grad bit 10	
06-30-23 14:58:Iter: [150/391]	Time 0.086 (0.076)	Data 0.004 (0.004)	Loss 0.534 (0.388)	Prec@1 85.156 (86.651)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:59:Num bit 10	Num grad bit 10	
06-30-23 14:59:Iter: [200/391]	Time 0.064 (0.077)	Data 0.003 (0.004)	Loss 0.385 (0.391)	Prec@1 89.062 (86.497)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:59:Num bit 10	Num grad bit 10	
06-30-23 14:59:Iter: [250/391]	Time 0.060 (0.082)	Data 0.003 (0.004)	Loss 0.538 (0.390)	Prec@1 82.812 (86.526)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:59:Num bit 10	Num grad bit 10	
06-30-23 14:59:Iter: [300/391]	Time 0.054 (0.080)	Data 0.002 (0.003)	Loss 0.298 (0.391)	Prec@1 90.625 (86.451)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:59:Num bit 10	Num grad bit 10	
06-30-23 14:59:Iter: [350/391]	Time 0.067 (0.078)	Data 0.003 (0.003)	Loss 0.311 (0.393)	Prec@1 89.062 (86.434)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:59:Test: [0/79]	Time: 0.3455(0.3455)	Loss: 0.434(0.434)	Prec@1: 85.938(85.938)	
06-30-23 14:59:Test: [50/79]	Time: 0.0265(0.0370)	Loss: 0.709(0.488)	Prec@1: 81.250(84.314)	
06-30-23 14:59:Test: [78/79]	Time: 0.0237(0.0349)	Loss: 0.483(0.498)	Prec@1: 87.500(83.910)	
06-30-23 14:59:Step 24 * Prec@1 83.910
06-30-23 14:59:Num bit 10	Num grad bit 10	
06-30-23 14:59:Iter: [0/391]	Time 0.320 (0.320)	Data 0.262 (0.262)	Loss 0.295 (0.295)	Prec@1 89.844 (89.844)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:59:Num bit 10	Num grad bit 10	
06-30-23 14:59:Iter: [50/391]	Time 0.157 (0.101)	Data 0.002 (0.007)	Loss 0.305 (0.374)	Prec@1 89.844 (87.362)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:59:Num bit 10	Num grad bit 10	
06-30-23 14:59:Iter: [100/391]	Time 0.173 (0.142)	Data 0.002 (0.005)	Loss 0.259 (0.384)	Prec@1 92.188 (87.044)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:59:Num bit 10	Num grad bit 10	
06-30-23 14:59:Iter: [150/391]	Time 0.178 (0.156)	Data 0.002 (0.004)	Loss 0.484 (0.383)	Prec@1 83.594 (86.879)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:59:Num bit 10	Num grad bit 10	
06-30-23 14:59:Iter: [200/391]	Time 0.055 (0.157)	Data 0.002 (0.003)	Loss 0.321 (0.384)	Prec@1 87.500 (86.847)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 14:59:Num bit 10	Num grad bit 10	
06-30-23 14:59:Iter: [250/391]	Time 0.065 (0.139)	Data 0.002 (0.003)	Loss 0.375 (0.387)	Prec@1 84.375 (86.728)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [300/391]	Time 0.111 (0.128)	Data 0.002 (0.003)	Loss 0.361 (0.386)	Prec@1 85.156 (86.807)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [350/391]	Time 0.055 (0.119)	Data 0.002 (0.003)	Loss 0.340 (0.381)	Prec@1 86.719 (86.915)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Test: [0/79]	Time: 0.2634(0.2634)	Loss: 0.396(0.396)	Prec@1: 85.938(85.938)	
06-30-23 15:00:Test: [50/79]	Time: 0.0256(0.0312)	Loss: 0.654(0.492)	Prec@1: 83.594(84.222)	
06-30-23 15:00:Test: [78/79]	Time: 0.0240(0.0293)	Loss: 0.770(0.496)	Prec@1: 81.250(84.220)	
06-30-23 15:00:Step 25 * Prec@1 84.220
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [0/391]	Time 0.347 (0.347)	Data 0.285 (0.285)	Loss 0.315 (0.315)	Prec@1 89.844 (89.844)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [50/391]	Time 0.088 (0.076)	Data 0.002 (0.008)	Loss 0.188 (0.374)	Prec@1 94.531 (87.255)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [100/391]	Time 0.061 (0.076)	Data 0.002 (0.005)	Loss 0.364 (0.378)	Prec@1 88.281 (86.935)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [150/391]	Time 0.064 (0.081)	Data 0.002 (0.004)	Loss 0.301 (0.377)	Prec@1 89.844 (87.143)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [200/391]	Time 0.072 (0.077)	Data 0.002 (0.004)	Loss 0.314 (0.380)	Prec@1 87.500 (86.987)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [250/391]	Time 0.077 (0.076)	Data 0.002 (0.003)	Loss 0.298 (0.381)	Prec@1 89.062 (86.930)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [300/391]	Time 0.068 (0.075)	Data 0.002 (0.003)	Loss 0.313 (0.378)	Prec@1 91.406 (87.020)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [350/391]	Time 0.060 (0.073)	Data 0.002 (0.003)	Loss 0.472 (0.378)	Prec@1 84.375 (87.017)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Test: [0/79]	Time: 0.2738(0.2738)	Loss: 0.366(0.366)	Prec@1: 89.844(89.844)	
06-30-23 15:00:Test: [50/79]	Time: 0.0258(0.0305)	Loss: 0.537(0.466)	Prec@1: 85.156(85.034)	
06-30-23 15:00:Test: [78/79]	Time: 0.1012(0.0336)	Loss: 0.478(0.466)	Prec@1: 87.500(85.010)	
06-30-23 15:00:Step 26 * Prec@1 85.010
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [0/391]	Time 0.410 (0.410)	Data 0.253 (0.253)	Loss 0.355 (0.355)	Prec@1 87.500 (87.500)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [50/391]	Time 0.157 (0.130)	Data 0.002 (0.007)	Loss 0.490 (0.364)	Prec@1 83.594 (88.067)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:00:Num bit 10	Num grad bit 10	
06-30-23 15:00:Iter: [100/391]	Time 0.157 (0.144)	Data 0.002 (0.005)	Loss 0.363 (0.365)	Prec@1 89.844 (87.817)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [150/391]	Time 0.159 (0.148)	Data 0.003 (0.004)	Loss 0.353 (0.372)	Prec@1 84.375 (87.484)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [200/391]	Time 0.068 (0.132)	Data 0.002 (0.003)	Loss 0.437 (0.370)	Prec@1 84.375 (87.492)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [250/391]	Time 0.073 (0.120)	Data 0.002 (0.003)	Loss 0.377 (0.376)	Prec@1 85.938 (87.242)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [300/391]	Time 0.068 (0.112)	Data 0.003 (0.003)	Loss 0.321 (0.375)	Prec@1 91.406 (87.285)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [350/391]	Time 0.077 (0.106)	Data 0.002 (0.003)	Loss 0.391 (0.375)	Prec@1 83.594 (87.275)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Test: [0/79]	Time: 0.2974(0.2974)	Loss: 0.440(0.440)	Prec@1: 86.719(86.719)	
06-30-23 15:01:Test: [50/79]	Time: 0.0271(0.0312)	Loss: 0.689(0.559)	Prec@1: 82.031(82.215)	
06-30-23 15:01:Test: [78/79]	Time: 0.1000(0.0388)	Loss: 0.642(0.556)	Prec@1: 81.250(82.390)	
06-30-23 15:01:Step 27 * Prec@1 82.390
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [0/391]	Time 0.411 (0.411)	Data 0.251 (0.251)	Loss 0.390 (0.390)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [50/391]	Time 0.061 (0.074)	Data 0.002 (0.007)	Loss 0.329 (0.361)	Prec@1 89.844 (87.132)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [100/391]	Time 0.074 (0.071)	Data 0.003 (0.005)	Loss 0.271 (0.358)	Prec@1 91.406 (87.330)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [150/391]	Time 0.078 (0.071)	Data 0.003 (0.004)	Loss 0.418 (0.363)	Prec@1 86.719 (87.309)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [200/391]	Time 0.080 (0.072)	Data 0.002 (0.004)	Loss 0.404 (0.366)	Prec@1 84.375 (87.057)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [250/391]	Time 0.157 (0.079)	Data 0.002 (0.003)	Loss 0.326 (0.364)	Prec@1 89.062 (87.102)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [300/391]	Time 0.157 (0.092)	Data 0.002 (0.003)	Loss 0.440 (0.362)	Prec@1 85.938 (87.240)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:01:Num bit 10	Num grad bit 10	
06-30-23 15:01:Iter: [350/391]	Time 0.157 (0.101)	Data 0.002 (0.003)	Loss 0.373 (0.364)	Prec@1 89.062 (87.224)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:02:Test: [0/79]	Time: 0.2837(0.2837)	Loss: 0.516(0.516)	Prec@1: 85.938(85.938)	
06-30-23 15:02:Test: [50/79]	Time: 0.0250(0.0302)	Loss: 0.860(0.550)	Prec@1: 76.562(82.828)	
06-30-23 15:02:Test: [78/79]	Time: 0.0227(0.0283)	Loss: 0.555(0.545)	Prec@1: 81.250(82.960)	
06-30-23 15:02:Step 28 * Prec@1 82.960
06-30-23 15:02:Num bit 10	Num grad bit 10	
06-30-23 15:02:Iter: [0/391]	Time 0.314 (0.314)	Data 0.250 (0.250)	Loss 0.283 (0.283)	Prec@1 91.406 (91.406)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:02:Num bit 10	Num grad bit 10	
06-30-23 15:02:Iter: [50/391]	Time 0.066 (0.076)	Data 0.002 (0.007)	Loss 0.311 (0.340)	Prec@1 88.281 (87.960)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:02:Num bit 10	Num grad bit 10	
06-30-23 15:02:Iter: [100/391]	Time 0.075 (0.075)	Data 0.002 (0.005)	Loss 0.345 (0.344)	Prec@1 89.062 (87.980)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:02:Num bit 10	Num grad bit 10	
06-30-23 15:02:Iter: [150/391]	Time 0.076 (0.074)	Data 0.002 (0.004)	Loss 0.245 (0.355)	Prec@1 91.406 (87.728)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:02:Num bit 10	Num grad bit 10	
06-30-23 15:02:Iter: [200/391]	Time 0.157 (0.078)	Data 0.002 (0.004)	Loss 0.457 (0.356)	Prec@1 82.812 (87.620)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:02:Num bit 10	Num grad bit 10	
06-30-23 15:02:Iter: [250/391]	Time 0.044 (0.077)	Data 0.002 (0.003)	Loss 0.231 (0.357)	Prec@1 92.188 (87.643)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:02:Num bit 10	Num grad bit 10	
06-30-23 15:02:Iter: [300/391]	Time 0.079 (0.076)	Data 0.003 (0.003)	Loss 0.285 (0.356)	Prec@1 93.750 (87.614)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:02:Num bit 10	Num grad bit 10	
06-30-23 15:02:Iter: [350/391]	Time 0.059 (0.075)	Data 0.002 (0.003)	Loss 0.304 (0.357)	Prec@1 89.062 (87.536)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:02:Test: [0/79]	Time: 0.2612(0.2612)	Loss: 0.411(0.411)	Prec@1: 85.156(85.156)	
06-30-23 15:02:Test: [50/79]	Time: 0.0171(0.0519)	Loss: 0.667(0.523)	Prec@1: 81.250(83.716)	
06-30-23 15:02:Test: [78/79]	Time: 0.0161(0.0396)	Loss: 0.445(0.517)	Prec@1: 87.500(83.780)	
06-30-23 15:02:Step 29 * Prec@1 83.780
06-30-23 15:02:Num bit 10	Num grad bit 10	
06-30-23 15:02:Iter: [0/391]	Time 0.424 (0.424)	Data 0.268 (0.268)	Loss 0.336 (0.336)	Prec@1 89.844 (89.844)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:02:Num bit 10	Num grad bit 10	
06-30-23 15:02:Iter: [50/391]	Time 0.159 (0.162)	Data 0.003 (0.007)	Loss 0.444 (0.323)	Prec@1 85.156 (89.093)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:02:Num bit 10	Num grad bit 10	
06-30-23 15:02:Iter: [100/391]	Time 0.157 (0.160)	Data 0.002 (0.005)	Loss 0.249 (0.333)	Prec@1 90.625 (88.413)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:03:Num bit 10	Num grad bit 10	
06-30-23 15:03:Iter: [150/391]	Time 0.055 (0.154)	Data 0.002 (0.004)	Loss 0.311 (0.343)	Prec@1 90.625 (88.095)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:03:Num bit 10	Num grad bit 10	
06-30-23 15:03:Iter: [200/391]	Time 0.066 (0.131)	Data 0.002 (0.003)	Loss 0.533 (0.347)	Prec@1 85.156 (88.025)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:03:Num bit 10	Num grad bit 10	
06-30-23 15:03:Iter: [250/391]	Time 0.074 (0.119)	Data 0.002 (0.003)	Loss 0.347 (0.351)	Prec@1 85.938 (87.926)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:03:Num bit 10	Num grad bit 10	
06-30-23 15:03:Iter: [300/391]	Time 0.074 (0.110)	Data 0.002 (0.003)	Loss 0.339 (0.352)	Prec@1 85.156 (87.884)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:03:Num bit 10	Num grad bit 10	
06-30-23 15:03:Iter: [350/391]	Time 0.077 (0.105)	Data 0.003 (0.003)	Loss 0.415 (0.355)	Prec@1 83.594 (87.738)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:03:Test: [0/79]	Time: 0.2931(0.2931)	Loss: 0.454(0.454)	Prec@1: 84.375(84.375)	
06-30-23 15:03:Test: [50/79]	Time: 0.0251(0.0576)	Loss: 0.734(0.593)	Prec@1: 81.250(82.384)	
06-30-23 15:03:Test: [78/79]	Time: 0.0226(0.0462)	Loss: 1.032(0.587)	Prec@1: 87.500(82.380)	
06-30-23 15:03:Step 30 * Prec@1 82.380
06-30-23 15:03:Num bit 10	Num grad bit 10	
06-30-23 15:03:Iter: [0/391]	Time 0.311 (0.311)	Data 0.266 (0.266)	Loss 0.398 (0.398)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:03:Num bit 10	Num grad bit 10	
06-30-23 15:03:Iter: [50/391]	Time 0.072 (0.072)	Data 0.002 (0.007)	Loss 0.289 (0.348)	Prec@1 89.062 (87.960)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:03:Num bit 10	Num grad bit 10	
06-30-23 15:03:Iter: [100/391]	Time 0.074 (0.073)	Data 0.003 (0.005)	Loss 0.424 (0.353)	Prec@1 85.938 (87.941)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:03:Num bit 10	Num grad bit 10	
06-30-23 15:03:Iter: [150/391]	Time 0.076 (0.074)	Data 0.002 (0.004)	Loss 0.321 (0.349)	Prec@1 89.062 (88.023)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:03:Num bit 10	Num grad bit 10	
06-30-23 15:03:Iter: [200/391]	Time 0.054 (0.076)	Data 0.002 (0.004)	Loss 0.354 (0.344)	Prec@1 91.406 (88.184)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:03:Num bit 10	Num grad bit 10	
06-30-23 15:03:Iter: [250/391]	Time 0.157 (0.092)	Data 0.002 (0.003)	Loss 0.381 (0.346)	Prec@1 86.719 (88.119)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:03:Num bit 10	Num grad bit 10	
06-30-23 15:03:Iter: [300/391]	Time 0.157 (0.102)	Data 0.003 (0.003)	Loss 0.298 (0.349)	Prec@1 90.625 (88.014)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:04:Num bit 10	Num grad bit 10	
06-30-23 15:04:Iter: [350/391]	Time 0.045 (0.109)	Data 0.001 (0.003)	Loss 0.458 (0.352)	Prec@1 85.938 (87.914)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:04:Test: [0/79]	Time: 0.2545(0.2545)	Loss: 0.584(0.584)	Prec@1: 82.812(82.812)	
06-30-23 15:04:Test: [50/79]	Time: 0.0258(0.0314)	Loss: 0.956(0.686)	Prec@1: 75.781(79.350)	
06-30-23 15:04:Test: [78/79]	Time: 0.0226(0.0291)	Loss: 0.783(0.687)	Prec@1: 68.750(78.980)	
06-30-23 15:04:Step 31 * Prec@1 78.980
06-30-23 15:04:Num bit 10	Num grad bit 10	
06-30-23 15:04:Iter: [0/391]	Time 0.366 (0.366)	Data 0.313 (0.313)	Loss 0.279 (0.279)	Prec@1 89.062 (89.062)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:04:Num bit 10	Num grad bit 10	
06-30-23 15:04:Iter: [50/391]	Time 0.073 (0.077)	Data 0.003 (0.008)	Loss 0.191 (0.352)	Prec@1 95.312 (87.914)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:04:Num bit 10	Num grad bit 10	
06-30-23 15:04:Iter: [100/391]	Time 0.075 (0.073)	Data 0.002 (0.005)	Loss 0.428 (0.350)	Prec@1 83.594 (87.918)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:04:Num bit 10	Num grad bit 10	
06-30-23 15:04:Iter: [150/391]	Time 0.082 (0.074)	Data 0.003 (0.004)	Loss 0.504 (0.348)	Prec@1 82.812 (87.924)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:04:Num bit 10	Num grad bit 10	
06-30-23 15:04:Iter: [200/391]	Time 0.071 (0.078)	Data 0.002 (0.004)	Loss 0.364 (0.349)	Prec@1 89.062 (87.861)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:04:Num bit 10	Num grad bit 10	
06-30-23 15:04:Iter: [250/391]	Time 0.063 (0.076)	Data 0.003 (0.004)	Loss 0.360 (0.353)	Prec@1 89.844 (87.712)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:04:Num bit 10	Num grad bit 10	
06-30-23 15:04:Iter: [300/391]	Time 0.074 (0.075)	Data 0.003 (0.003)	Loss 0.386 (0.352)	Prec@1 89.062 (87.757)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:04:Num bit 10	Num grad bit 10	
06-30-23 15:04:Iter: [350/391]	Time 0.073 (0.075)	Data 0.003 (0.003)	Loss 0.326 (0.350)	Prec@1 89.062 (87.752)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:04:Test: [0/79]	Time: 0.3586(0.3586)	Loss: 0.415(0.415)	Prec@1: 85.156(85.156)	
06-30-23 15:04:Test: [50/79]	Time: 0.0170(0.0266)	Loss: 0.583(0.504)	Prec@1: 83.594(84.115)	
06-30-23 15:04:Test: [78/79]	Time: 0.1003(0.0511)	Loss: 0.467(0.502)	Prec@1: 87.500(84.090)	
06-30-23 15:04:Step 32 * Prec@1 84.090
06-30-23 15:04:Num bit 10	Num grad bit 10	
06-30-23 15:04:Iter: [0/391]	Time 0.431 (0.431)	Data 0.274 (0.274)	Loss 0.508 (0.508)	Prec@1 78.906 (78.906)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:04:Num bit 10	Num grad bit 10	
06-30-23 15:04:Iter: [50/391]	Time 0.157 (0.163)	Data 0.002 (0.007)	Loss 0.472 (0.311)	Prec@1 82.031 (89.185)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:04:Num bit 10	Num grad bit 10	
06-30-23 15:04:Iter: [100/391]	Time 0.157 (0.167)	Data 0.002 (0.005)	Loss 0.386 (0.329)	Prec@1 82.812 (88.451)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [150/391]	Time 0.071 (0.144)	Data 0.003 (0.004)	Loss 0.427 (0.330)	Prec@1 83.594 (88.364)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [200/391]	Time 0.074 (0.126)	Data 0.002 (0.004)	Loss 0.398 (0.337)	Prec@1 89.062 (88.270)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [250/391]	Time 0.068 (0.115)	Data 0.002 (0.003)	Loss 0.279 (0.338)	Prec@1 90.625 (88.179)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [300/391]	Time 0.072 (0.108)	Data 0.002 (0.003)	Loss 0.306 (0.343)	Prec@1 91.406 (88.042)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [350/391]	Time 0.076 (0.103)	Data 0.003 (0.003)	Loss 0.361 (0.347)	Prec@1 91.406 (87.950)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Test: [0/79]	Time: 0.2639(0.2639)	Loss: 0.422(0.422)	Prec@1: 86.719(86.719)	
06-30-23 15:05:Test: [50/79]	Time: 0.0247(0.0303)	Loss: 0.602(0.512)	Prec@1: 82.031(83.670)	
06-30-23 15:05:Test: [78/79]	Time: 0.0225(0.0286)	Loss: 0.534(0.507)	Prec@1: 87.500(83.770)	
06-30-23 15:05:Step 33 * Prec@1 83.770
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [0/391]	Time 0.330 (0.330)	Data 0.281 (0.281)	Loss 0.308 (0.308)	Prec@1 91.406 (91.406)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [50/391]	Time 0.071 (0.063)	Data 0.002 (0.007)	Loss 0.272 (0.317)	Prec@1 87.500 (88.986)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [100/391]	Time 0.073 (0.067)	Data 0.002 (0.005)	Loss 0.301 (0.332)	Prec@1 89.062 (88.513)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [150/391]	Time 0.067 (0.068)	Data 0.002 (0.004)	Loss 0.229 (0.337)	Prec@1 91.406 (88.266)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [200/391]	Time 0.159 (0.074)	Data 0.003 (0.004)	Loss 0.319 (0.340)	Prec@1 87.500 (88.262)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [250/391]	Time 0.157 (0.091)	Data 0.002 (0.003)	Loss 0.435 (0.342)	Prec@1 84.375 (88.182)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [300/391]	Time 0.157 (0.102)	Data 0.002 (0.003)	Loss 0.363 (0.345)	Prec@1 87.500 (88.126)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:05:Num bit 10	Num grad bit 10	
06-30-23 15:05:Iter: [350/391]	Time 0.067 (0.106)	Data 0.002 (0.003)	Loss 0.271 (0.344)	Prec@1 89.844 (88.190)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:06:Test: [0/79]	Time: 0.2839(0.2839)	Loss: 0.414(0.414)	Prec@1: 84.375(84.375)	
06-30-23 15:06:Test: [50/79]	Time: 0.0250(0.0303)	Loss: 0.560(0.510)	Prec@1: 83.594(84.176)	
06-30-23 15:06:Test: [78/79]	Time: 0.0229(0.0285)	Loss: 0.700(0.516)	Prec@1: 75.000(84.100)	
06-30-23 15:06:Step 34 * Prec@1 84.100
06-30-23 15:06:Num bit 10	Num grad bit 10	
06-30-23 15:06:Iter: [0/391]	Time 0.346 (0.346)	Data 0.264 (0.264)	Loss 0.280 (0.280)	Prec@1 89.844 (89.844)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:06:Num bit 10	Num grad bit 10	
06-30-23 15:06:Iter: [50/391]	Time 0.073 (0.070)	Data 0.003 (0.007)	Loss 0.378 (0.310)	Prec@1 89.844 (88.879)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:06:Num bit 10	Num grad bit 10	
06-30-23 15:06:Iter: [100/391]	Time 0.083 (0.070)	Data 0.003 (0.005)	Loss 0.259 (0.318)	Prec@1 89.844 (88.977)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:06:Num bit 10	Num grad bit 10	
06-30-23 15:06:Iter: [150/391]	Time 0.071 (0.073)	Data 0.002 (0.004)	Loss 0.299 (0.316)	Prec@1 88.281 (89.068)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:06:Num bit 10	Num grad bit 10	
06-30-23 15:06:Iter: [200/391]	Time 0.065 (0.077)	Data 0.002 (0.003)	Loss 0.396 (0.328)	Prec@1 85.156 (88.596)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:06:Num bit 10	Num grad bit 10	
06-30-23 15:06:Iter: [250/391]	Time 0.082 (0.076)	Data 0.002 (0.003)	Loss 0.296 (0.333)	Prec@1 90.625 (88.424)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:06:Num bit 10	Num grad bit 10	
06-30-23 15:06:Iter: [300/391]	Time 0.075 (0.075)	Data 0.002 (0.003)	Loss 0.336 (0.335)	Prec@1 89.844 (88.393)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:06:Num bit 10	Num grad bit 10	
06-30-23 15:06:Iter: [350/391]	Time 0.072 (0.075)	Data 0.002 (0.003)	Loss 0.360 (0.337)	Prec@1 85.156 (88.306)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:06:Test: [0/79]	Time: 0.2748(0.2748)	Loss: 0.458(0.458)	Prec@1: 87.500(87.500)	
06-30-23 15:06:Test: [50/79]	Time: 0.1014(0.0738)	Loss: 0.630(0.524)	Prec@1: 81.250(83.640)	
06-30-23 15:06:Test: [78/79]	Time: 0.1000(0.0837)	Loss: 0.720(0.525)	Prec@1: 81.250(83.810)	
06-30-23 15:06:Step 35 * Prec@1 83.810
06-30-23 15:06:Num bit 10	Num grad bit 10	
06-30-23 15:06:Iter: [0/391]	Time 0.410 (0.410)	Data 0.255 (0.255)	Loss 0.227 (0.227)	Prec@1 92.969 (92.969)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:06:Num bit 10	Num grad bit 10	
06-30-23 15:06:Iter: [50/391]	Time 0.157 (0.162)	Data 0.002 (0.007)	Loss 0.371 (0.317)	Prec@1 85.938 (88.695)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:06:Num bit 10	Num grad bit 10	
06-30-23 15:06:Iter: [100/391]	Time 0.096 (0.160)	Data 0.002 (0.005)	Loss 0.252 (0.331)	Prec@1 90.625 (88.250)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:06:Num bit 10	Num grad bit 10	
06-30-23 15:06:Iter: [150/391]	Time 0.057 (0.126)	Data 0.002 (0.004)	Loss 0.300 (0.333)	Prec@1 90.625 (88.374)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Num bit 10	Num grad bit 10	
06-30-23 15:07:Iter: [200/391]	Time 0.079 (0.113)	Data 0.003 (0.003)	Loss 0.303 (0.331)	Prec@1 93.750 (88.522)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Num bit 10	Num grad bit 10	
06-30-23 15:07:Iter: [250/391]	Time 0.057 (0.105)	Data 0.002 (0.003)	Loss 0.452 (0.330)	Prec@1 88.281 (88.530)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Num bit 10	Num grad bit 10	
06-30-23 15:07:Iter: [300/391]	Time 0.067 (0.098)	Data 0.002 (0.003)	Loss 0.450 (0.332)	Prec@1 85.938 (88.502)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Num bit 10	Num grad bit 10	
06-30-23 15:07:Iter: [350/391]	Time 0.053 (0.092)	Data 0.002 (0.003)	Loss 0.310 (0.334)	Prec@1 90.625 (88.435)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Test: [0/79]	Time: 0.3314(0.3314)	Loss: 0.408(0.408)	Prec@1: 89.844(89.844)	
06-30-23 15:07:Test: [50/79]	Time: 0.0394(0.0574)	Loss: 0.511(0.455)	Prec@1: 82.031(85.263)	
06-30-23 15:07:Test: [78/79]	Time: 0.0226(0.0470)	Loss: 0.341(0.455)	Prec@1: 87.500(85.200)	
06-30-23 15:07:Step 36 * Prec@1 85.200
06-30-23 15:07:Num bit 10	Num grad bit 10	
06-30-23 15:07:Iter: [0/391]	Time 0.302 (0.302)	Data 0.246 (0.246)	Loss 0.378 (0.378)	Prec@1 87.500 (87.500)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Num bit 10	Num grad bit 10	
06-30-23 15:07:Iter: [50/391]	Time 0.073 (0.066)	Data 0.003 (0.007)	Loss 0.317 (0.323)	Prec@1 89.062 (88.756)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Num bit 10	Num grad bit 10	
06-30-23 15:07:Iter: [100/391]	Time 0.057 (0.065)	Data 0.002 (0.005)	Loss 0.417 (0.319)	Prec@1 88.281 (89.179)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Num bit 10	Num grad bit 10	
06-30-23 15:07:Iter: [150/391]	Time 0.048 (0.066)	Data 0.001 (0.004)	Loss 0.357 (0.321)	Prec@1 86.719 (89.047)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Num bit 10	Num grad bit 10	
06-30-23 15:07:Iter: [200/391]	Time 0.044 (0.070)	Data 0.002 (0.003)	Loss 0.350 (0.326)	Prec@1 87.500 (88.767)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Num bit 10	Num grad bit 10	
06-30-23 15:07:Iter: [250/391]	Time 0.157 (0.083)	Data 0.002 (0.003)	Loss 0.355 (0.326)	Prec@1 85.156 (88.807)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Num bit 10	Num grad bit 10	
06-30-23 15:07:Iter: [300/391]	Time 0.158 (0.095)	Data 0.003 (0.003)	Loss 0.316 (0.330)	Prec@1 87.500 (88.676)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Num bit 10	Num grad bit 10	
06-30-23 15:07:Iter: [350/391]	Time 0.157 (0.104)	Data 0.002 (0.003)	Loss 0.432 (0.335)	Prec@1 83.594 (88.475)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:07:Test: [0/79]	Time: 0.2896(0.2896)	Loss: 0.498(0.498)	Prec@1: 85.938(85.938)	
06-30-23 15:08:Test: [50/79]	Time: 0.0275(0.0318)	Loss: 0.741(0.577)	Prec@1: 76.562(82.276)	
06-30-23 15:08:Test: [78/79]	Time: 0.0233(0.0297)	Loss: 0.721(0.574)	Prec@1: 81.250(82.160)	
06-30-23 15:08:Step 37 * Prec@1 82.160
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [0/391]	Time 0.329 (0.329)	Data 0.265 (0.265)	Loss 0.326 (0.326)	Prec@1 89.062 (89.062)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [50/391]	Time 0.081 (0.074)	Data 0.003 (0.007)	Loss 0.340 (0.325)	Prec@1 87.500 (88.879)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [100/391]	Time 0.052 (0.070)	Data 0.002 (0.005)	Loss 0.229 (0.321)	Prec@1 91.406 (88.815)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [150/391]	Time 0.068 (0.071)	Data 0.002 (0.004)	Loss 0.414 (0.325)	Prec@1 83.594 (88.695)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [200/391]	Time 0.062 (0.074)	Data 0.002 (0.004)	Loss 0.396 (0.333)	Prec@1 88.281 (88.479)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [250/391]	Time 0.056 (0.071)	Data 0.003 (0.003)	Loss 0.302 (0.333)	Prec@1 90.625 (88.459)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [300/391]	Time 0.048 (0.068)	Data 0.002 (0.003)	Loss 0.319 (0.335)	Prec@1 88.281 (88.447)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [350/391]	Time 0.052 (0.066)	Data 0.002 (0.003)	Loss 0.332 (0.333)	Prec@1 89.844 (88.531)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:08:Test: [0/79]	Time: 0.2920(0.2920)	Loss: 0.488(0.488)	Prec@1: 86.719(86.719)	
06-30-23 15:08:Test: [50/79]	Time: 0.0259(0.0303)	Loss: 0.719(0.559)	Prec@1: 77.344(82.705)	
06-30-23 15:08:Test: [78/79]	Time: 0.0226(0.0284)	Loss: 0.613(0.544)	Prec@1: 81.250(83.010)	
06-30-23 15:08:Step 38 * Prec@1 83.010
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [0/391]	Time 0.410 (0.410)	Data 0.251 (0.251)	Loss 0.482 (0.482)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [50/391]	Time 0.157 (0.128)	Data 0.002 (0.007)	Loss 0.367 (0.325)	Prec@1 87.500 (88.833)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [100/391]	Time 0.158 (0.143)	Data 0.002 (0.005)	Loss 0.274 (0.328)	Prec@1 88.281 (88.691)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [150/391]	Time 0.157 (0.147)	Data 0.002 (0.004)	Loss 0.271 (0.322)	Prec@1 86.719 (88.778)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:08:Num bit 10	Num grad bit 10	
06-30-23 15:08:Iter: [200/391]	Time 0.072 (0.136)	Data 0.002 (0.003)	Loss 0.332 (0.324)	Prec@1 87.500 (88.767)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:09:Num bit 10	Num grad bit 10	
06-30-23 15:09:Iter: [250/391]	Time 0.076 (0.123)	Data 0.002 (0.003)	Loss 0.354 (0.325)	Prec@1 89.062 (88.729)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:09:Num bit 10	Num grad bit 10	
06-30-23 15:09:Iter: [300/391]	Time 0.074 (0.114)	Data 0.002 (0.003)	Loss 0.304 (0.326)	Prec@1 89.062 (88.647)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:09:Num bit 10	Num grad bit 10	
06-30-23 15:09:Iter: [350/391]	Time 0.072 (0.107)	Data 0.002 (0.003)	Loss 0.401 (0.330)	Prec@1 85.156 (88.499)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:09:Test: [0/79]	Time: 0.3038(0.3038)	Loss: 0.509(0.509)	Prec@1: 83.594(83.594)	
06-30-23 15:09:Test: [50/79]	Time: 0.0265(0.0316)	Loss: 0.603(0.539)	Prec@1: 79.688(83.257)	
06-30-23 15:09:Test: [78/79]	Time: 0.0227(0.0295)	Loss: 0.594(0.524)	Prec@1: 75.000(83.540)	
06-30-23 15:09:Step 39 * Prec@1 83.540
06-30-23 15:09:Num bit 10	Num grad bit 10	
06-30-23 15:09:Iter: [0/391]	Time 0.316 (0.316)	Data 0.258 (0.258)	Loss 0.257 (0.257)	Prec@1 92.188 (92.188)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:09:Num bit 10	Num grad bit 10	
06-30-23 15:09:Iter: [50/391]	Time 0.067 (0.093)	Data 0.002 (0.007)	Loss 0.333 (0.302)	Prec@1 88.281 (89.691)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:09:Num bit 10	Num grad bit 10	
06-30-23 15:09:Iter: [100/391]	Time 0.076 (0.080)	Data 0.002 (0.005)	Loss 0.290 (0.307)	Prec@1 89.062 (89.217)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:09:Num bit 10	Num grad bit 10	
06-30-23 15:09:Iter: [150/391]	Time 0.078 (0.077)	Data 0.002 (0.004)	Loss 0.385 (0.314)	Prec@1 85.938 (89.011)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:09:Num bit 10	Num grad bit 10	
06-30-23 15:09:Iter: [200/391]	Time 0.056 (0.074)	Data 0.002 (0.003)	Loss 0.259 (0.323)	Prec@1 90.625 (88.783)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:09:Num bit 10	Num grad bit 10	
06-30-23 15:09:Iter: [250/391]	Time 0.171 (0.073)	Data 0.002 (0.003)	Loss 0.307 (0.324)	Prec@1 88.281 (88.729)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:09:Num bit 10	Num grad bit 10	
06-30-23 15:09:Iter: [300/391]	Time 0.157 (0.082)	Data 0.001 (0.003)	Loss 0.268 (0.324)	Prec@1 89.844 (88.715)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:09:Num bit 10	Num grad bit 10	
06-30-23 15:09:Iter: [350/391]	Time 0.157 (0.093)	Data 0.002 (0.003)	Loss 0.319 (0.328)	Prec@1 88.281 (88.577)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:09:Test: [0/79]	Time: 0.3317(0.3317)	Loss: 0.488(0.488)	Prec@1: 83.594(83.594)	
06-30-23 15:09:Test: [50/79]	Time: 0.0251(0.0801)	Loss: 0.606(0.547)	Prec@1: 85.156(83.532)	
06-30-23 15:09:Test: [78/79]	Time: 0.0227(0.0614)	Loss: 0.622(0.551)	Prec@1: 87.500(83.370)	
06-30-23 15:09:Step 40 * Prec@1 83.370
06-30-23 15:09:Num bit 10	Num grad bit 10	
06-30-23 15:09:Iter: [0/391]	Time 0.320 (0.320)	Data 0.263 (0.263)	Loss 0.196 (0.196)	Prec@1 92.969 (92.969)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [50/391]	Time 0.071 (0.067)	Data 0.003 (0.007)	Loss 0.211 (0.293)	Prec@1 91.406 (89.614)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [100/391]	Time 0.068 (0.066)	Data 0.004 (0.005)	Loss 0.253 (0.303)	Prec@1 92.188 (89.527)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [150/391]	Time 0.097 (0.065)	Data 0.003 (0.004)	Loss 0.336 (0.306)	Prec@1 85.938 (89.337)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [200/391]	Time 0.069 (0.064)	Data 0.003 (0.004)	Loss 0.409 (0.307)	Prec@1 88.281 (89.303)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [250/391]	Time 0.079 (0.064)	Data 0.003 (0.003)	Loss 0.347 (0.308)	Prec@1 90.625 (89.283)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [300/391]	Time 0.063 (0.068)	Data 0.003 (0.003)	Loss 0.370 (0.314)	Prec@1 89.062 (89.138)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [350/391]	Time 0.074 (0.069)	Data 0.002 (0.003)	Loss 0.410 (0.315)	Prec@1 87.500 (89.098)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Test: [0/79]	Time: 0.2902(0.2902)	Loss: 0.556(0.556)	Prec@1: 82.031(82.031)	
06-30-23 15:10:Test: [50/79]	Time: 0.0252(0.0350)	Loss: 1.106(0.866)	Prec@1: 73.438(78.569)	
06-30-23 15:10:Test: [78/79]	Time: 0.0253(0.0320)	Loss: 1.981(0.881)	Prec@1: 75.000(78.380)	
06-30-23 15:10:Step 41 * Prec@1 78.380
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [0/391]	Time 0.331 (0.331)	Data 0.284 (0.284)	Loss 0.453 (0.453)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [50/391]	Time 0.071 (0.058)	Data 0.003 (0.008)	Loss 0.315 (0.338)	Prec@1 91.406 (88.879)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [100/391]	Time 0.063 (0.069)	Data 0.003 (0.005)	Loss 0.202 (0.332)	Prec@1 89.062 (88.629)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [150/391]	Time 0.157 (0.099)	Data 0.002 (0.004)	Loss 0.355 (0.328)	Prec@1 84.375 (88.638)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [200/391]	Time 0.170 (0.115)	Data 0.003 (0.003)	Loss 0.352 (0.323)	Prec@1 89.062 (88.856)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:10:Num bit 10	Num grad bit 10	
06-30-23 15:10:Iter: [250/391]	Time 0.057 (0.122)	Data 0.002 (0.003)	Loss 0.354 (0.331)	Prec@1 87.500 (88.627)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [300/391]	Time 0.061 (0.113)	Data 0.002 (0.003)	Loss 0.243 (0.335)	Prec@1 92.969 (88.520)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [350/391]	Time 0.068 (0.106)	Data 0.002 (0.003)	Loss 0.336 (0.335)	Prec@1 88.281 (88.497)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Test: [0/79]	Time: 0.2958(0.2958)	Loss: 0.907(0.907)	Prec@1: 81.250(81.250)	
06-30-23 15:11:Test: [50/79]	Time: 0.0252(0.0332)	Loss: 0.883(0.788)	Prec@1: 75.000(79.856)	
06-30-23 15:11:Test: [78/79]	Time: 0.0228(0.0313)	Loss: 0.690(0.770)	Prec@1: 87.500(79.790)	
06-30-23 15:11:Step 42 * Prec@1 79.790
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [0/391]	Time 0.379 (0.379)	Data 0.333 (0.333)	Loss 0.306 (0.306)	Prec@1 88.281 (88.281)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [50/391]	Time 0.079 (0.070)	Data 0.003 (0.009)	Loss 0.298 (0.343)	Prec@1 91.406 (88.036)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [100/391]	Time 0.099 (0.079)	Data 0.002 (0.005)	Loss 0.330 (0.339)	Prec@1 88.281 (88.490)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [150/391]	Time 0.061 (0.074)	Data 0.002 (0.004)	Loss 0.324 (0.344)	Prec@1 89.062 (88.198)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [200/391]	Time 0.062 (0.072)	Data 0.002 (0.004)	Loss 0.351 (0.348)	Prec@1 87.500 (88.266)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [250/391]	Time 0.050 (0.070)	Data 0.002 (0.004)	Loss 0.191 (0.351)	Prec@1 95.312 (88.151)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [300/391]	Time 0.086 (0.069)	Data 0.003 (0.003)	Loss 0.681 (0.353)	Prec@1 82.812 (88.061)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [350/391]	Time 0.160 (0.069)	Data 0.002 (0.003)	Loss 0.384 (0.357)	Prec@1 85.938 (87.934)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Test: [0/79]	Time: 0.3596(0.3596)	Loss: 0.735(0.735)	Prec@1: 79.688(79.688)	
06-30-23 15:11:Test: [50/79]	Time: 0.1144(0.1109)	Loss: 1.354(0.931)	Prec@1: 70.312(76.394)	
06-30-23 15:11:Test: [78/79]	Time: 0.1003(0.1091)	Loss: 1.284(0.923)	Prec@1: 68.750(76.570)	
06-30-23 15:11:Step 43 * Prec@1 76.570
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [0/391]	Time 0.436 (0.436)	Data 0.265 (0.265)	Loss 0.272 (0.272)	Prec@1 88.281 (88.281)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [50/391]	Time 0.157 (0.167)	Data 0.002 (0.007)	Loss 0.285 (0.346)	Prec@1 92.188 (87.944)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:11:Num bit 10	Num grad bit 10	
06-30-23 15:11:Iter: [100/391]	Time 0.052 (0.127)	Data 0.002 (0.005)	Loss 0.236 (0.343)	Prec@1 92.969 (88.127)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [150/391]	Time 0.063 (0.106)	Data 0.002 (0.004)	Loss 0.438 (0.350)	Prec@1 83.594 (87.940)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [200/391]	Time 0.063 (0.096)	Data 0.002 (0.003)	Loss 0.360 (0.356)	Prec@1 86.719 (87.757)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [250/391]	Time 0.078 (0.090)	Data 0.002 (0.003)	Loss 0.326 (0.358)	Prec@1 87.500 (87.699)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [300/391]	Time 0.049 (0.086)	Data 0.001 (0.003)	Loss 0.525 (0.365)	Prec@1 79.688 (87.526)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [350/391]	Time 0.069 (0.083)	Data 0.002 (0.003)	Loss 0.282 (0.369)	Prec@1 90.625 (87.400)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Test: [0/79]	Time: 0.3010(0.3010)	Loss: 0.844(0.844)	Prec@1: 73.438(73.438)	
06-30-23 15:12:Test: [50/79]	Time: 0.0256(0.0314)	Loss: 1.095(0.866)	Prec@1: 73.438(76.486)	
06-30-23 15:12:Test: [78/79]	Time: 0.0245(0.0294)	Loss: 0.632(0.865)	Prec@1: 81.250(76.330)	
06-30-23 15:12:Step 44 * Prec@1 76.330
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [0/391]	Time 0.313 (0.313)	Data 0.259 (0.259)	Loss 0.434 (0.434)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [50/391]	Time 0.065 (0.074)	Data 0.002 (0.007)	Loss 0.294 (0.362)	Prec@1 91.406 (87.929)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [100/391]	Time 0.063 (0.066)	Data 0.002 (0.005)	Loss 0.247 (0.354)	Prec@1 89.844 (88.065)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [150/391]	Time 0.058 (0.065)	Data 0.002 (0.004)	Loss 0.419 (0.362)	Prec@1 87.500 (87.717)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [200/391]	Time 0.157 (0.075)	Data 0.002 (0.003)	Loss 0.340 (0.361)	Prec@1 89.844 (87.679)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [250/391]	Time 0.157 (0.091)	Data 0.002 (0.003)	Loss 0.299 (0.370)	Prec@1 90.625 (87.460)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [300/391]	Time 0.157 (0.102)	Data 0.002 (0.003)	Loss 0.393 (0.372)	Prec@1 85.156 (87.240)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:12:Num bit 10	Num grad bit 10	
06-30-23 15:12:Iter: [350/391]	Time 0.073 (0.104)	Data 0.002 (0.003)	Loss 0.503 (0.373)	Prec@1 83.594 (87.204)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Test: [0/79]	Time: 0.2864(0.2864)	Loss: 1.433(1.433)	Prec@1: 66.406(66.406)	
06-30-23 15:13:Test: [50/79]	Time: 0.0265(0.0308)	Loss: 1.605(1.549)	Prec@1: 57.812(64.001)	
06-30-23 15:13:Test: [78/79]	Time: 0.0228(0.0290)	Loss: 2.007(1.568)	Prec@1: 62.500(63.770)	
06-30-23 15:13:Step 45 * Prec@1 63.770
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [0/391]	Time 0.326 (0.326)	Data 0.279 (0.279)	Loss 0.387 (0.387)	Prec@1 87.500 (87.500)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [50/391]	Time 0.057 (0.070)	Data 0.002 (0.008)	Loss 0.352 (0.376)	Prec@1 88.281 (87.270)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [100/391]	Time 0.047 (0.063)	Data 0.001 (0.005)	Loss 0.396 (0.374)	Prec@1 87.500 (87.299)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [150/391]	Time 0.043 (0.059)	Data 0.001 (0.004)	Loss 0.384 (0.381)	Prec@1 88.281 (86.910)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [200/391]	Time 0.156 (0.060)	Data 0.002 (0.003)	Loss 0.413 (0.384)	Prec@1 85.938 (86.765)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [250/391]	Time 0.061 (0.063)	Data 0.002 (0.003)	Loss 0.398 (0.387)	Prec@1 87.500 (86.632)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [300/391]	Time 0.082 (0.065)	Data 0.002 (0.003)	Loss 0.378 (0.388)	Prec@1 88.281 (86.576)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [350/391]	Time 0.047 (0.066)	Data 0.002 (0.003)	Loss 0.493 (0.392)	Prec@1 84.375 (86.512)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Test: [0/79]	Time: 0.2784(0.2784)	Loss: 0.520(0.520)	Prec@1: 82.812(82.812)	
06-30-23 15:13:Test: [50/79]	Time: 0.0276(0.0304)	Loss: 0.791(0.742)	Prec@1: 78.125(77.788)	
06-30-23 15:13:Test: [78/79]	Time: 0.0165(0.0426)	Loss: 0.760(0.742)	Prec@1: 81.250(78.020)	
06-30-23 15:13:Step 46 * Prec@1 78.020
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [0/391]	Time 0.302 (0.302)	Data 0.255 (0.255)	Loss 0.412 (0.412)	Prec@1 87.500 (87.500)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [50/391]	Time 0.159 (0.140)	Data 0.003 (0.007)	Loss 0.320 (0.375)	Prec@1 89.844 (87.224)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [100/391]	Time 0.157 (0.149)	Data 0.003 (0.005)	Loss 0.347 (0.372)	Prec@1 87.500 (87.461)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [150/391]	Time 0.157 (0.151)	Data 0.002 (0.004)	Loss 0.334 (0.368)	Prec@1 89.062 (87.578)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:13:Num bit 10	Num grad bit 10	
06-30-23 15:13:Iter: [200/391]	Time 0.080 (0.132)	Data 0.003 (0.003)	Loss 0.401 (0.374)	Prec@1 86.719 (87.302)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:14:Num bit 10	Num grad bit 10	
06-30-23 15:14:Iter: [250/391]	Time 0.071 (0.120)	Data 0.003 (0.003)	Loss 0.459 (0.377)	Prec@1 84.375 (87.204)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:14:Num bit 10	Num grad bit 10	
06-30-23 15:14:Iter: [300/391]	Time 0.074 (0.112)	Data 0.003 (0.003)	Loss 0.369 (0.385)	Prec@1 85.938 (86.843)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:14:Num bit 10	Num grad bit 10	
06-30-23 15:14:Iter: [350/391]	Time 0.068 (0.106)	Data 0.002 (0.003)	Loss 0.403 (0.390)	Prec@1 84.375 (86.641)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:14:Test: [0/79]	Time: 0.2887(0.2887)	Loss: 0.678(0.678)	Prec@1: 79.688(79.688)	
06-30-23 15:14:Test: [50/79]	Time: 0.0994(0.0367)	Loss: 0.812(0.772)	Prec@1: 74.219(77.558)	
06-30-23 15:14:Test: [78/79]	Time: 0.0262(0.0470)	Loss: 0.647(0.776)	Prec@1: 75.000(77.480)	
06-30-23 15:14:Step 47 * Prec@1 77.480
06-30-23 15:14:Num bit 10	Num grad bit 10	
06-30-23 15:14:Iter: [0/391]	Time 0.340 (0.340)	Data 0.285 (0.285)	Loss 0.246 (0.246)	Prec@1 94.531 (94.531)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:14:Num bit 10	Num grad bit 10	
06-30-23 15:14:Iter: [50/391]	Time 0.057 (0.061)	Data 0.002 (0.008)	Loss 0.296 (0.392)	Prec@1 87.500 (86.520)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:14:Num bit 10	Num grad bit 10	
06-30-23 15:14:Iter: [100/391]	Time 0.076 (0.062)	Data 0.002 (0.005)	Loss 0.352 (0.395)	Prec@1 86.719 (86.564)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:14:Num bit 10	Num grad bit 10	
06-30-23 15:14:Iter: [150/391]	Time 0.073 (0.065)	Data 0.002 (0.004)	Loss 0.574 (0.404)	Prec@1 82.031 (86.207)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:14:Num bit 10	Num grad bit 10	
06-30-23 15:14:Iter: [200/391]	Time 0.070 (0.064)	Data 0.003 (0.004)	Loss 0.435 (0.404)	Prec@1 84.375 (86.198)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:14:Num bit 10	Num grad bit 10	
06-30-23 15:14:Iter: [250/391]	Time 0.157 (0.070)	Data 0.002 (0.003)	Loss 0.440 (0.407)	Prec@1 83.594 (86.068)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:14:Num bit 10	Num grad bit 10	
06-30-23 15:14:Iter: [300/391]	Time 0.157 (0.085)	Data 0.002 (0.003)	Loss 0.335 (0.404)	Prec@1 87.500 (86.111)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:14:Num bit 10	Num grad bit 10	
06-30-23 15:14:Iter: [350/391]	Time 0.157 (0.095)	Data 0.002 (0.003)	Loss 0.402 (0.405)	Prec@1 86.719 (86.064)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:14:Test: [0/79]	Time: 0.2892(0.2892)	Loss: 0.513(0.513)	Prec@1: 78.906(78.906)	
06-30-23 15:14:Test: [50/79]	Time: 0.0249(0.0317)	Loss: 0.688(0.695)	Prec@1: 80.469(78.539)	
06-30-23 15:14:Test: [78/79]	Time: 0.0228(0.0295)	Loss: 0.645(0.692)	Prec@1: 75.000(78.370)	
06-30-23 15:14:Step 48 * Prec@1 78.370
06-30-23 15:14:Num bit 10	Num grad bit 10	
06-30-23 15:14:Iter: [0/391]	Time 0.296 (0.296)	Data 0.250 (0.250)	Loss 0.328 (0.328)	Prec@1 88.281 (88.281)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [50/391]	Time 0.055 (0.057)	Data 0.002 (0.007)	Loss 0.244 (0.376)	Prec@1 92.188 (87.255)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [100/391]	Time 0.074 (0.058)	Data 0.003 (0.004)	Loss 0.277 (0.405)	Prec@1 89.062 (86.293)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [150/391]	Time 0.048 (0.061)	Data 0.002 (0.004)	Loss 0.372 (0.411)	Prec@1 82.812 (86.046)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [200/391]	Time 0.062 (0.061)	Data 0.003 (0.003)	Loss 0.317 (0.413)	Prec@1 89.844 (85.887)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [250/391]	Time 0.156 (0.064)	Data 0.003 (0.003)	Loss 0.593 (0.417)	Prec@1 82.031 (85.835)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [300/391]	Time 0.071 (0.067)	Data 0.002 (0.003)	Loss 0.495 (0.420)	Prec@1 83.594 (85.719)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [350/391]	Time 0.075 (0.068)	Data 0.002 (0.003)	Loss 0.493 (0.420)	Prec@1 83.594 (85.706)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Test: [0/79]	Time: 0.3032(0.3032)	Loss: 0.600(0.600)	Prec@1: 82.031(82.031)	
06-30-23 15:15:Test: [50/79]	Time: 0.0248(0.0317)	Loss: 0.808(0.615)	Prec@1: 75.000(79.473)	
06-30-23 15:15:Test: [78/79]	Time: 0.0227(0.0296)	Loss: 0.462(0.619)	Prec@1: 87.500(79.330)	
06-30-23 15:15:Step 49 * Prec@1 79.330
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [0/391]	Time 0.354 (0.354)	Data 0.305 (0.305)	Loss 0.348 (0.348)	Prec@1 89.844 (89.844)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [50/391]	Time 0.068 (0.082)	Data 0.002 (0.008)	Loss 0.423 (0.410)	Prec@1 85.938 (85.830)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [100/391]	Time 0.157 (0.119)	Data 0.002 (0.005)	Loss 0.320 (0.416)	Prec@1 85.938 (85.489)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [150/391]	Time 0.157 (0.132)	Data 0.002 (0.004)	Loss 0.443 (0.432)	Prec@1 85.938 (85.115)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [200/391]	Time 0.048 (0.135)	Data 0.002 (0.003)	Loss 0.407 (0.424)	Prec@1 87.500 (85.242)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:15:Num bit 10	Num grad bit 10	
06-30-23 15:15:Iter: [250/391]	Time 0.048 (0.119)	Data 0.002 (0.003)	Loss 0.437 (0.418)	Prec@1 82.812 (85.499)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:16:Num bit 10	Num grad bit 10	
06-30-23 15:16:Iter: [300/391]	Time 0.070 (0.109)	Data 0.002 (0.003)	Loss 0.440 (0.420)	Prec@1 82.812 (85.530)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:16:Num bit 10	Num grad bit 10	
06-30-23 15:16:Iter: [350/391]	Time 0.061 (0.103)	Data 0.002 (0.003)	Loss 0.368 (0.419)	Prec@1 89.844 (85.617)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:16:Test: [0/79]	Time: 0.2858(0.2858)	Loss: 2.598(2.598)	Prec@1: 53.125(53.125)	
06-30-23 15:16:Test: [50/79]	Time: 0.0253(0.0307)	Loss: 2.908(2.505)	Prec@1: 42.969(53.079)	
06-30-23 15:16:Test: [78/79]	Time: 0.0234(0.0293)	Loss: 2.232(2.526)	Prec@1: 50.000(52.920)	
06-30-23 15:16:Step 50 * Prec@1 52.920
06-30-23 15:16:Num bit 10	Num grad bit 10	
06-30-23 15:16:Iter: [0/391]	Time 0.331 (0.331)	Data 0.278 (0.278)	Loss 0.463 (0.463)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:16:Num bit 10	Num grad bit 10	
06-30-23 15:16:Iter: [50/391]	Time 0.816 (0.096)	Data 0.002 (0.008)	Loss 0.602 (0.393)	Prec@1 75.781 (86.275)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:16:Num bit 10	Num grad bit 10	
06-30-23 15:16:Iter: [100/391]	Time 0.071 (0.080)	Data 0.002 (0.005)	Loss 0.378 (0.408)	Prec@1 90.625 (85.891)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:16:Num bit 10	Num grad bit 10	
06-30-23 15:16:Iter: [150/391]	Time 0.070 (0.077)	Data 0.002 (0.004)	Loss 0.376 (0.419)	Prec@1 87.500 (85.560)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:16:Num bit 10	Num grad bit 10	
06-30-23 15:16:Iter: [200/391]	Time 0.051 (0.074)	Data 0.002 (0.004)	Loss 0.351 (0.424)	Prec@1 89.844 (85.386)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:16:Num bit 10	Num grad bit 10	
06-30-23 15:16:Iter: [250/391]	Time 0.076 (0.073)	Data 0.003 (0.003)	Loss 0.379 (0.425)	Prec@1 88.281 (85.380)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:16:Num bit 10	Num grad bit 10	
06-30-23 15:16:Iter: [300/391]	Time 0.046 (0.073)	Data 0.002 (0.003)	Loss 0.497 (0.426)	Prec@1 82.031 (85.374)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:16:Num bit 10	Num grad bit 10	
06-30-23 15:16:Iter: [350/391]	Time 0.157 (0.083)	Data 0.002 (0.003)	Loss 0.270 (0.425)	Prec@1 91.406 (85.434)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:16:Test: [0/79]	Time: 0.3292(0.3292)	Loss: 2.381(2.381)	Prec@1: 53.906(53.906)	
06-30-23 15:16:Test: [50/79]	Time: 0.1014(0.1070)	Loss: 2.723(2.466)	Prec@1: 46.875(53.860)	
06-30-23 15:16:Test: [78/79]	Time: 0.1002(0.1054)	Loss: 2.345(2.477)	Prec@1: 62.500(53.800)	
06-30-23 15:16:Step 51 * Prec@1 53.800
06-30-23 15:16:Num bit 10	Num grad bit 10	
06-30-23 15:16:Iter: [0/391]	Time 0.430 (0.430)	Data 0.273 (0.273)	Loss 0.486 (0.486)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:16:Num bit 10	Num grad bit 10	
06-30-23 15:16:Iter: [50/391]	Time 0.072 (0.074)	Data 0.003 (0.008)	Loss 0.483 (0.409)	Prec@1 85.156 (85.463)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [100/391]	Time 0.071 (0.073)	Data 0.002 (0.005)	Loss 0.397 (0.422)	Prec@1 88.281 (85.381)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [150/391]	Time 0.060 (0.073)	Data 0.002 (0.004)	Loss 0.517 (0.424)	Prec@1 80.469 (85.487)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [200/391]	Time 0.071 (0.073)	Data 0.003 (0.004)	Loss 0.320 (0.420)	Prec@1 87.500 (85.491)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [250/391]	Time 0.062 (0.071)	Data 0.002 (0.003)	Loss 0.522 (0.426)	Prec@1 80.469 (85.268)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [300/391]	Time 0.067 (0.073)	Data 0.002 (0.003)	Loss 0.336 (0.426)	Prec@1 91.406 (85.400)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [350/391]	Time 0.072 (0.073)	Data 0.003 (0.003)	Loss 0.423 (0.428)	Prec@1 82.031 (85.368)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Test: [0/79]	Time: 0.2865(0.2865)	Loss: 0.934(0.934)	Prec@1: 69.531(69.531)	
06-30-23 15:17:Test: [50/79]	Time: 0.0261(0.0314)	Loss: 1.012(0.951)	Prec@1: 75.781(74.035)	
06-30-23 15:17:Test: [78/79]	Time: 0.0247(0.0299)	Loss: 0.929(0.956)	Prec@1: 62.500(74.130)	
06-30-23 15:17:Step 52 * Prec@1 74.130
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [0/391]	Time 0.324 (0.324)	Data 0.257 (0.257)	Loss 0.490 (0.490)	Prec@1 82.031 (82.031)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [50/391]	Time 0.064 (0.073)	Data 0.002 (0.007)	Loss 0.606 (0.418)	Prec@1 82.031 (85.646)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [100/391]	Time 0.169 (0.087)	Data 0.002 (0.005)	Loss 0.583 (0.435)	Prec@1 79.688 (85.226)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [150/391]	Time 0.157 (0.112)	Data 0.002 (0.004)	Loss 0.559 (0.440)	Prec@1 80.469 (85.115)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [200/391]	Time 0.157 (0.124)	Data 0.002 (0.004)	Loss 0.454 (0.438)	Prec@1 87.500 (85.110)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [250/391]	Time 0.067 (0.124)	Data 0.002 (0.003)	Loss 0.494 (0.437)	Prec@1 82.812 (85.122)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:17:Num bit 10	Num grad bit 10	
06-30-23 15:17:Iter: [300/391]	Time 0.064 (0.115)	Data 0.003 (0.003)	Loss 0.386 (0.438)	Prec@1 86.719 (85.052)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:18:Num bit 10	Num grad bit 10	
06-30-23 15:18:Iter: [350/391]	Time 0.072 (0.109)	Data 0.002 (0.003)	Loss 0.426 (0.438)	Prec@1 85.156 (85.056)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:18:Test: [0/79]	Time: 0.3611(0.3611)	Loss: 0.738(0.738)	Prec@1: 75.781(75.781)	
06-30-23 15:18:Test: [50/79]	Time: 0.0253(0.0324)	Loss: 1.001(0.821)	Prec@1: 68.750(74.571)	
06-30-23 15:18:Test: [78/79]	Time: 0.0332(0.0304)	Loss: 0.702(0.812)	Prec@1: 81.250(74.920)	
06-30-23 15:18:Step 53 * Prec@1 74.920
06-30-23 15:18:Num bit 10	Num grad bit 10	
06-30-23 15:18:Iter: [0/391]	Time 0.436 (0.436)	Data 0.343 (0.343)	Loss 0.463 (0.463)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:18:Num bit 10	Num grad bit 10	
06-30-23 15:18:Iter: [50/391]	Time 0.080 (0.075)	Data 0.003 (0.009)	Loss 0.539 (0.449)	Prec@1 81.250 (84.375)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:18:Num bit 10	Num grad bit 10	
06-30-23 15:18:Iter: [100/391]	Time 0.060 (0.082)	Data 0.002 (0.006)	Loss 0.557 (0.451)	Prec@1 83.594 (84.352)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:18:Num bit 10	Num grad bit 10	
06-30-23 15:18:Iter: [150/391]	Time 0.064 (0.075)	Data 0.003 (0.005)	Loss 0.517 (0.443)	Prec@1 78.906 (84.535)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:18:Num bit 10	Num grad bit 10	
06-30-23 15:18:Iter: [200/391]	Time 0.066 (0.074)	Data 0.003 (0.004)	Loss 0.448 (0.441)	Prec@1 87.500 (84.639)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:18:Num bit 10	Num grad bit 10	
06-30-23 15:18:Iter: [250/391]	Time 0.093 (0.074)	Data 0.003 (0.004)	Loss 0.370 (0.446)	Prec@1 86.719 (84.471)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:18:Num bit 10	Num grad bit 10	
06-30-23 15:18:Iter: [300/391]	Time 0.071 (0.073)	Data 0.005 (0.004)	Loss 0.333 (0.446)	Prec@1 89.844 (84.531)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:18:Num bit 10	Num grad bit 10	
06-30-23 15:18:Iter: [350/391]	Time 0.157 (0.078)	Data 0.002 (0.003)	Loss 0.581 (0.449)	Prec@1 80.469 (84.444)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:18:Test: [0/79]	Time: 0.4565(0.4565)	Loss: 1.367(1.367)	Prec@1: 65.625(65.625)	
06-30-23 15:18:Test: [50/79]	Time: 0.1013(0.1122)	Loss: 1.409(1.355)	Prec@1: 70.312(67.616)	
06-30-23 15:18:Test: [78/79]	Time: 0.1003(0.1098)	Loss: 0.964(1.366)	Prec@1: 75.000(67.340)	
06-30-23 15:18:Step 54 * Prec@1 67.340
06-30-23 15:18:Num bit 10	Num grad bit 10	
06-30-23 15:18:Iter: [0/391]	Time 0.548 (0.548)	Data 0.380 (0.380)	Loss 0.666 (0.666)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:18:Num bit 10	Num grad bit 10	
06-30-23 15:18:Iter: [50/391]	Time 0.078 (0.124)	Data 0.003 (0.009)	Loss 0.385 (0.443)	Prec@1 86.719 (85.034)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [100/391]	Time 0.066 (0.098)	Data 0.002 (0.006)	Loss 0.301 (0.456)	Prec@1 89.844 (84.445)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [150/391]	Time 0.058 (0.088)	Data 0.002 (0.005)	Loss 0.533 (0.450)	Prec@1 80.469 (84.406)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [200/391]	Time 0.055 (0.081)	Data 0.002 (0.004)	Loss 0.400 (0.457)	Prec@1 83.594 (84.274)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [250/391]	Time 0.055 (0.077)	Data 0.003 (0.004)	Loss 0.423 (0.455)	Prec@1 83.594 (84.247)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [300/391]	Time 0.061 (0.074)	Data 0.002 (0.004)	Loss 0.424 (0.455)	Prec@1 84.375 (84.188)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [350/391]	Time 0.061 (0.076)	Data 0.002 (0.003)	Loss 0.641 (0.456)	Prec@1 79.688 (84.190)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Test: [0/79]	Time: 0.3363(0.3363)	Loss: 0.792(0.792)	Prec@1: 75.000(75.000)	
06-30-23 15:19:Test: [50/79]	Time: 0.0267(0.0333)	Loss: 0.975(0.886)	Prec@1: 73.438(75.138)	
06-30-23 15:19:Test: [78/79]	Time: 0.0365(0.0312)	Loss: 0.852(0.880)	Prec@1: 75.000(74.930)	
06-30-23 15:19:Step 55 * Prec@1 74.930
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [0/391]	Time 0.368 (0.368)	Data 0.298 (0.298)	Loss 0.406 (0.406)	Prec@1 89.062 (89.062)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [50/391]	Time 0.068 (0.074)	Data 0.002 (0.008)	Loss 0.548 (0.486)	Prec@1 81.250 (83.716)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [100/391]	Time 0.052 (0.071)	Data 0.002 (0.005)	Loss 0.482 (0.481)	Prec@1 83.594 (83.555)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [150/391]	Time 0.053 (0.073)	Data 0.003 (0.004)	Loss 0.478 (0.478)	Prec@1 82.031 (83.687)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [200/391]	Time 0.168 (0.095)	Data 0.002 (0.004)	Loss 0.450 (0.471)	Prec@1 82.031 (83.827)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [250/391]	Time 0.168 (0.109)	Data 0.002 (0.004)	Loss 0.374 (0.472)	Prec@1 85.938 (83.746)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:19:Num bit 10	Num grad bit 10	
06-30-23 15:19:Iter: [300/391]	Time 0.062 (0.115)	Data 0.002 (0.003)	Loss 0.393 (0.471)	Prec@1 83.594 (83.812)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [350/391]	Time 0.058 (0.108)	Data 0.002 (0.003)	Loss 0.574 (0.469)	Prec@1 78.906 (83.943)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Test: [0/79]	Time: 0.3334(0.3334)	Loss: 0.626(0.626)	Prec@1: 79.688(79.688)	
06-30-23 15:20:Test: [50/79]	Time: 0.0269(0.0332)	Loss: 0.878(0.738)	Prec@1: 75.781(76.884)	
06-30-23 15:20:Test: [78/79]	Time: 0.0227(0.0307)	Loss: 0.482(0.735)	Prec@1: 87.500(76.580)	
06-30-23 15:20:Step 56 * Prec@1 76.580
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [0/391]	Time 0.393 (0.393)	Data 0.326 (0.326)	Loss 0.383 (0.383)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [50/391]	Time 0.069 (0.071)	Data 0.002 (0.009)	Loss 0.533 (0.464)	Prec@1 83.594 (84.130)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [100/391]	Time 0.050 (0.063)	Data 0.002 (0.005)	Loss 0.578 (0.478)	Prec@1 79.688 (83.671)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [150/391]	Time 0.056 (0.059)	Data 0.002 (0.004)	Loss 0.424 (0.484)	Prec@1 85.938 (83.428)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [200/391]	Time 0.053 (0.064)	Data 0.002 (0.004)	Loss 0.456 (0.485)	Prec@1 83.594 (83.493)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [250/391]	Time 0.053 (0.063)	Data 0.002 (0.003)	Loss 0.394 (0.487)	Prec@1 88.281 (83.398)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [300/391]	Time 0.070 (0.062)	Data 0.002 (0.003)	Loss 0.531 (0.487)	Prec@1 81.250 (83.417)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [350/391]	Time 0.064 (0.063)	Data 0.002 (0.003)	Loss 0.636 (0.493)	Prec@1 82.031 (83.195)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Test: [0/79]	Time: 0.3985(0.3985)	Loss: 0.552(0.552)	Prec@1: 79.688(79.688)	
06-30-23 15:20:Test: [50/79]	Time: 0.0177(0.0552)	Loss: 0.902(0.770)	Prec@1: 72.656(75.934)	
06-30-23 15:20:Test: [78/79]	Time: 0.0160(0.0419)	Loss: 1.339(0.771)	Prec@1: 75.000(75.840)	
06-30-23 15:20:Step 57 * Prec@1 75.840
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [0/391]	Time 0.404 (0.404)	Data 0.353 (0.353)	Loss 0.503 (0.503)	Prec@1 78.906 (78.906)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [50/391]	Time 0.159 (0.157)	Data 0.003 (0.009)	Loss 0.513 (0.484)	Prec@1 84.375 (83.502)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [100/391]	Time 0.159 (0.158)	Data 0.002 (0.006)	Loss 0.485 (0.491)	Prec@1 78.125 (83.246)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:20:Num bit 10	Num grad bit 10	
06-30-23 15:20:Iter: [150/391]	Time 0.054 (0.154)	Data 0.002 (0.005)	Loss 0.562 (0.499)	Prec@1 82.812 (82.895)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Num bit 10	Num grad bit 10	
06-30-23 15:21:Iter: [200/391]	Time 0.075 (0.134)	Data 0.003 (0.004)	Loss 0.455 (0.500)	Prec@1 82.812 (82.902)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Num bit 10	Num grad bit 10	
06-30-23 15:21:Iter: [250/391]	Time 0.079 (0.123)	Data 0.003 (0.004)	Loss 0.485 (0.499)	Prec@1 83.594 (82.850)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Num bit 10	Num grad bit 10	
06-30-23 15:21:Iter: [300/391]	Time 0.062 (0.115)	Data 0.002 (0.004)	Loss 0.339 (0.496)	Prec@1 87.500 (82.960)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Num bit 10	Num grad bit 10	
06-30-23 15:21:Iter: [350/391]	Time 0.063 (0.108)	Data 0.002 (0.003)	Loss 0.591 (0.499)	Prec@1 79.688 (82.853)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Test: [0/79]	Time: 0.4101(0.4101)	Loss: 1.217(1.217)	Prec@1: 64.062(64.062)	
06-30-23 15:21:Test: [50/79]	Time: 0.0253(0.0561)	Loss: 1.631(1.398)	Prec@1: 58.594(64.874)	
06-30-23 15:21:Test: [78/79]	Time: 0.0226(0.0454)	Loss: 1.456(1.369)	Prec@1: 68.750(64.870)	
06-30-23 15:21:Step 58 * Prec@1 64.870
06-30-23 15:21:Num bit 10	Num grad bit 10	
06-30-23 15:21:Iter: [0/391]	Time 0.343 (0.343)	Data 0.285 (0.285)	Loss 0.474 (0.474)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Num bit 10	Num grad bit 10	
06-30-23 15:21:Iter: [50/391]	Time 0.071 (0.074)	Data 0.002 (0.008)	Loss 0.390 (0.501)	Prec@1 86.719 (82.874)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Num bit 10	Num grad bit 10	
06-30-23 15:21:Iter: [100/391]	Time 0.075 (0.073)	Data 0.003 (0.005)	Loss 0.511 (0.521)	Prec@1 83.594 (82.256)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Num bit 10	Num grad bit 10	
06-30-23 15:21:Iter: [150/391]	Time 0.062 (0.071)	Data 0.002 (0.004)	Loss 0.482 (0.522)	Prec@1 80.469 (82.218)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Num bit 10	Num grad bit 10	
06-30-23 15:21:Iter: [200/391]	Time 0.061 (0.073)	Data 0.002 (0.004)	Loss 0.522 (0.521)	Prec@1 79.688 (82.191)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Num bit 10	Num grad bit 10	
06-30-23 15:21:Iter: [250/391]	Time 0.157 (0.088)	Data 0.002 (0.003)	Loss 0.727 (0.523)	Prec@1 76.562 (82.112)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Num bit 10	Num grad bit 10	
06-30-23 15:21:Iter: [300/391]	Time 0.166 (0.100)	Data 0.008 (0.003)	Loss 0.689 (0.526)	Prec@1 77.344 (82.075)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Num bit 10	Num grad bit 10	
06-30-23 15:21:Iter: [350/391]	Time 0.057 (0.107)	Data 0.002 (0.003)	Loss 0.510 (0.527)	Prec@1 82.031 (82.011)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:21:Test: [0/79]	Time: 0.3627(0.3627)	Loss: 1.616(1.616)	Prec@1: 66.406(66.406)	
06-30-23 15:22:Test: [50/79]	Time: 0.0249(0.0317)	Loss: 1.574(1.548)	Prec@1: 65.625(61.703)	
06-30-23 15:22:Test: [78/79]	Time: 0.0229(0.0293)	Loss: 1.686(1.554)	Prec@1: 62.500(61.480)	
06-30-23 15:22:Step 59 * Prec@1 61.480
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [0/391]	Time 0.348 (0.348)	Data 0.294 (0.294)	Loss 0.472 (0.472)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [50/391]	Time 0.071 (0.075)	Data 0.003 (0.008)	Loss 0.463 (0.475)	Prec@1 83.594 (83.931)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [100/391]	Time 0.075 (0.074)	Data 0.002 (0.005)	Loss 0.570 (0.510)	Prec@1 77.344 (82.565)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [150/391]	Time 0.059 (0.072)	Data 0.002 (0.004)	Loss 0.431 (0.531)	Prec@1 85.938 (81.861)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [200/391]	Time 0.072 (0.075)	Data 0.002 (0.004)	Loss 0.513 (0.533)	Prec@1 81.250 (81.779)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [250/391]	Time 0.067 (0.075)	Data 0.002 (0.004)	Loss 0.483 (0.535)	Prec@1 83.594 (81.698)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [300/391]	Time 0.078 (0.074)	Data 0.003 (0.003)	Loss 0.589 (0.534)	Prec@1 78.906 (81.751)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [350/391]	Time 0.063 (0.072)	Data 0.003 (0.003)	Loss 0.557 (0.533)	Prec@1 82.812 (81.740)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:22:Test: [0/79]	Time: 0.3799(0.3799)	Loss: 0.624(0.624)	Prec@1: 75.781(75.781)	
06-30-23 15:22:Test: [50/79]	Time: 0.0170(0.0462)	Loss: 0.959(0.827)	Prec@1: 67.188(74.740)	
06-30-23 15:22:Test: [78/79]	Time: 0.0158(0.0359)	Loss: 1.008(0.818)	Prec@1: 56.250(74.560)	
06-30-23 15:22:Step 60 * Prec@1 74.560
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [0/391]	Time 0.449 (0.449)	Data 0.290 (0.290)	Loss 0.546 (0.546)	Prec@1 81.250 (81.250)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [50/391]	Time 0.159 (0.163)	Data 0.002 (0.008)	Loss 0.609 (0.535)	Prec@1 77.344 (82.292)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [100/391]	Time 0.159 (0.161)	Data 0.003 (0.005)	Loss 0.435 (0.555)	Prec@1 82.812 (81.513)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [150/391]	Time 0.062 (0.153)	Data 0.002 (0.004)	Loss 0.513 (0.560)	Prec@1 79.688 (80.960)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:22:Num bit 10	Num grad bit 10	
06-30-23 15:22:Iter: [200/391]	Time 0.061 (0.132)	Data 0.002 (0.004)	Loss 0.537 (0.563)	Prec@1 80.469 (80.830)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [250/391]	Time 0.077 (0.121)	Data 0.002 (0.003)	Loss 0.417 (0.558)	Prec@1 85.938 (80.880)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [300/391]	Time 0.079 (0.113)	Data 0.003 (0.003)	Loss 0.590 (0.555)	Prec@1 78.125 (80.936)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [350/391]	Time 0.070 (0.107)	Data 0.002 (0.003)	Loss 0.584 (0.564)	Prec@1 75.781 (80.660)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Test: [0/79]	Time: 0.3835(0.3835)	Loss: 0.987(0.987)	Prec@1: 69.531(69.531)	
06-30-23 15:23:Test: [50/79]	Time: 0.0248(0.0545)	Loss: 1.358(1.243)	Prec@1: 63.281(64.200)	
06-30-23 15:23:Test: [78/79]	Time: 0.0227(0.0443)	Loss: 1.229(1.240)	Prec@1: 75.000(64.350)	
06-30-23 15:23:Step 61 * Prec@1 64.350
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [0/391]	Time 0.338 (0.338)	Data 0.289 (0.289)	Loss 0.590 (0.590)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [50/391]	Time 0.078 (0.071)	Data 0.002 (0.008)	Loss 0.467 (0.568)	Prec@1 82.812 (80.576)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [100/391]	Time 0.058 (0.072)	Data 0.002 (0.005)	Loss 0.644 (0.613)	Prec@1 74.219 (79.162)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [150/391]	Time 0.066 (0.072)	Data 0.002 (0.004)	Loss 0.655 (0.639)	Prec@1 80.469 (78.399)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [200/391]	Time 0.069 (0.074)	Data 0.003 (0.004)	Loss 0.919 (0.647)	Prec@1 70.312 (77.919)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [250/391]	Time 0.062 (0.068)	Data 0.003 (0.004)	Loss 0.739 (0.643)	Prec@1 75.781 (78.060)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [300/391]	Time 0.062 (0.067)	Data 0.003 (0.003)	Loss 0.563 (0.640)	Prec@1 80.469 (78.154)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [350/391]	Time 0.065 (0.067)	Data 0.002 (0.003)	Loss 0.758 (0.642)	Prec@1 78.125 (78.094)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Test: [0/79]	Time: 0.3006(0.3006)	Loss: 1.026(1.026)	Prec@1: 64.844(64.844)	
06-30-23 15:23:Test: [50/79]	Time: 0.0190(0.0261)	Loss: 1.240(1.065)	Prec@1: 55.469(64.537)	
06-30-23 15:23:Test: [78/79]	Time: 0.0208(0.0237)	Loss: 0.999(1.058)	Prec@1: 50.000(64.580)	
06-30-23 15:23:Step 62 * Prec@1 64.580
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [0/391]	Time 0.412 (0.412)	Data 0.349 (0.349)	Loss 0.786 (0.786)	Prec@1 71.875 (71.875)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [50/391]	Time 0.058 (0.062)	Data 0.002 (0.009)	Loss 0.923 (0.735)	Prec@1 71.875 (75.015)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [100/391]	Time 0.069 (0.066)	Data 0.003 (0.006)	Loss 0.685 (0.808)	Prec@1 75.781 (72.842)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [150/391]	Time 0.050 (0.065)	Data 0.002 (0.005)	Loss 0.714 (0.791)	Prec@1 75.781 (73.313)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [200/391]	Time 0.047 (0.064)	Data 0.002 (0.004)	Loss 1.082 (0.816)	Prec@1 63.281 (72.365)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:23:Num bit 10	Num grad bit 10	
06-30-23 15:23:Iter: [250/391]	Time 0.066 (0.062)	Data 0.003 (0.004)	Loss 0.943 (0.839)	Prec@1 67.969 (71.545)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:24:Num bit 10	Num grad bit 10	
06-30-23 15:24:Iter: [300/391]	Time 0.161 (0.068)	Data 0.002 (0.004)	Loss 0.818 (0.856)	Prec@1 75.781 (71.000)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:24:Num bit 10	Num grad bit 10	
06-30-23 15:24:Iter: [350/391]	Time 0.165 (0.073)	Data 0.007 (0.003)	Loss 0.909 (0.869)	Prec@1 67.969 (70.673)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:24:Test: [0/79]	Time: 0.3771(0.3771)	Loss: 1.818(1.818)	Prec@1: 45.312(45.312)	
06-30-23 15:24:Test: [50/79]	Time: 0.0169(0.0454)	Loss: 1.660(1.755)	Prec@1: 53.125(45.542)	
06-30-23 15:24:Test: [78/79]	Time: 0.0162(0.0528)	Loss: 2.196(1.742)	Prec@1: 31.250(45.650)	
06-30-23 15:24:Step 63 * Prec@1 45.650
06-30-23 15:24:Num bit 10	Num grad bit 10	
06-30-23 15:24:Iter: [0/391]	Time 0.346 (0.346)	Data 0.300 (0.300)	Loss 1.093 (1.093)	Prec@1 61.719 (61.719)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:24:Num bit 10	Num grad bit 10	
06-30-23 15:24:Iter: [50/391]	Time 0.056 (0.102)	Data 0.002 (0.008)	Loss 1.448 (1.313)	Prec@1 54.688 (56.526)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:24:Num bit 10	Num grad bit 10	
06-30-23 15:24:Iter: [100/391]	Time 0.052 (0.100)	Data 0.002 (0.005)	Loss 1.388 (1.395)	Prec@1 53.125 (53.380)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:24:Num bit 10	Num grad bit 10	
06-30-23 15:24:Iter: [150/391]	Time 0.043 (0.099)	Data 0.001 (0.004)	Loss 1.445 (1.367)	Prec@1 49.219 (54.299)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:24:Num bit 10	Num grad bit 10	
06-30-23 15:24:Iter: [200/391]	Time 0.043 (0.098)	Data 0.002 (0.004)	Loss 1.321 (1.355)	Prec@1 55.469 (54.447)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:24:Num bit 10	Num grad bit 10	
06-30-23 15:24:Iter: [250/391]	Time 0.038 (0.098)	Data 0.001 (0.003)	Loss 1.538 (1.366)	Prec@1 52.344 (53.990)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:24:Num bit 10	Num grad bit 10	
06-30-23 15:24:Iter: [300/391]	Time 0.042 (0.098)	Data 0.002 (0.003)	Loss 1.664 (1.402)	Prec@1 46.875 (53.164)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:24:Num bit 10	Num grad bit 10	
06-30-23 15:24:Iter: [350/391]	Time 0.053 (0.098)	Data 0.002 (0.003)	Loss 1.987 (1.451)	Prec@1 39.844 (51.729)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:24:Test: [0/79]	Time: 0.3742(0.3742)	Loss: 1.972(1.972)	Prec@1: 38.281(38.281)	
06-30-23 15:24:Test: [50/79]	Time: 0.0173(0.0406)	Loss: 1.800(2.143)	Prec@1: 47.656(37.331)	
06-30-23 15:24:Test: [78/79]	Time: 0.0154(0.0500)	Loss: 1.766(2.147)	Prec@1: 43.750(37.080)	
06-30-23 15:24:Step 64 * Prec@1 37.080
06-30-23 15:24:Num bit 10	Num grad bit 10	
06-30-23 15:24:Iter: [0/391]	Time 0.317 (0.317)	Data 0.278 (0.278)	Loss 2.375 (2.375)	Prec@1 41.406 (41.406)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [50/391]	Time 0.062 (0.103)	Data 0.003 (0.008)	Loss 2.522 (2.655)	Prec@1 25.000 (30.162)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [100/391]	Time 0.060 (0.101)	Data 0.002 (0.005)	Loss 1.692 (2.441)	Prec@1 43.750 (29.610)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [150/391]	Time 0.055 (0.100)	Data 0.002 (0.004)	Loss 3.271 (2.263)	Prec@1 24.219 (32.171)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [200/391]	Time 0.044 (0.099)	Data 0.002 (0.004)	Loss 3.686 (2.587)	Prec@1 15.625 (30.624)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [250/391]	Time 0.045 (0.099)	Data 0.002 (0.003)	Loss 2.081 (2.543)	Prec@1 35.938 (30.403)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [300/391]	Time 0.159 (0.099)	Data 0.002 (0.003)	Loss 2.145 (2.501)	Prec@1 35.156 (30.778)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [350/391]	Time 0.159 (0.098)	Data 0.002 (0.003)	Loss 3.198 (2.635)	Prec@1 22.656 (30.280)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Test: [0/79]	Time: 0.3950(0.3950)	Loss: 5.608(5.608)	Prec@1: 14.062(14.062)	
06-30-23 15:25:Test: [50/79]	Time: 0.0963(0.0623)	Loss: 5.749(6.064)	Prec@1: 14.062(13.894)	
06-30-23 15:25:Test: [78/79]	Time: 0.0154(0.0464)	Loss: 5.583(6.095)	Prec@1: 18.750(13.560)	
06-30-23 15:25:Step 65 * Prec@1 13.560
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [0/391]	Time 0.337 (0.337)	Data 0.295 (0.295)	Loss 12.450 (12.450)	Prec@1 15.625 (15.625)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [50/391]	Time 0.075 (0.067)	Data 0.004 (0.008)	Loss 2.079 (4.080)	Prec@1 28.125 (21.446)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [100/391]	Time 0.064 (0.064)	Data 0.003 (0.005)	Loss 3.408 (3.711)	Prec@1 21.094 (22.718)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [150/391]	Time 0.072 (0.066)	Data 0.003 (0.004)	Loss 2.122 (3.533)	Prec@1 17.969 (22.708)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [200/391]	Time 0.071 (0.067)	Data 0.003 (0.004)	Loss 2.279 (3.280)	Prec@1 25.000 (23.912)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:25:Num bit 10	Num grad bit 10	
06-30-23 15:25:Iter: [250/391]	Time 0.066 (0.067)	Data 0.003 (0.004)	Loss 1.847 (3.011)	Prec@1 32.031 (25.601)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [300/391]	Time 0.043 (0.065)	Data 0.002 (0.004)	Loss 1.963 (2.826)	Prec@1 25.000 (26.710)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [350/391]	Time 0.151 (0.065)	Data 0.001 (0.003)	Loss 1.985 (2.693)	Prec@1 31.250 (27.564)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Test: [0/79]	Time: 0.2906(0.2906)	Loss: 1.915(1.915)	Prec@1: 41.406(41.406)	
06-30-23 15:26:Test: [50/79]	Time: 0.0175(0.0314)	Loss: 2.278(2.034)	Prec@1: 29.688(34.773)	
06-30-23 15:26:Test: [78/79]	Time: 0.0187(0.0328)	Loss: 1.710(2.040)	Prec@1: 31.250(34.840)	
06-30-23 15:26:Step 66 * Prec@1 34.840
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [0/391]	Time 0.413 (0.413)	Data 0.304 (0.304)	Loss 3.724 (3.724)	Prec@1 16.406 (16.406)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [50/391]	Time 0.063 (0.080)	Data 0.003 (0.008)	Loss 1.988 (2.729)	Prec@1 30.469 (21.752)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [100/391]	Time 0.058 (0.078)	Data 0.003 (0.005)	Loss 1.817 (2.291)	Prec@1 28.125 (27.344)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [150/391]	Time 0.049 (0.076)	Data 0.002 (0.004)	Loss 1.642 (2.089)	Prec@1 40.625 (31.374)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [200/391]	Time 0.044 (0.076)	Data 0.002 (0.004)	Loss 1.577 (1.973)	Prec@1 42.188 (33.757)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [250/391]	Time 0.162 (0.075)	Data 0.003 (0.003)	Loss 1.516 (1.887)	Prec@1 49.219 (35.807)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [300/391]	Time 0.058 (0.075)	Data 0.002 (0.003)	Loss 1.487 (1.842)	Prec@1 46.094 (37.139)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [350/391]	Time 0.058 (0.075)	Data 0.003 (0.003)	Loss 1.276 (1.812)	Prec@1 52.344 (38.125)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Test: [0/79]	Time: 0.2969(0.2969)	Loss: 2.516(2.516)	Prec@1: 29.688(29.688)	
06-30-23 15:26:Test: [50/79]	Time: 0.0172(0.0315)	Loss: 2.522(2.530)	Prec@1: 32.031(33.517)	
06-30-23 15:26:Test: [78/79]	Time: 0.0156(0.0265)	Loss: 2.897(2.520)	Prec@1: 37.500(33.780)	
06-30-23 15:26:Step 67 * Prec@1 33.780
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [0/391]	Time 0.438 (0.438)	Data 0.279 (0.279)	Loss 2.991 (2.991)	Prec@1 21.875 (21.875)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [50/391]	Time 0.045 (0.072)	Data 0.002 (0.008)	Loss 1.954 (2.857)	Prec@1 32.031 (26.042)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [100/391]	Time 0.041 (0.073)	Data 0.002 (0.005)	Loss 2.197 (2.432)	Prec@1 29.688 (28.775)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [150/391]	Time 0.161 (0.072)	Data 0.003 (0.004)	Loss 1.706 (2.217)	Prec@1 35.938 (31.219)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [200/391]	Time 0.050 (0.071)	Data 0.002 (0.004)	Loss 1.640 (2.113)	Prec@1 45.312 (33.131)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:26:Num bit 10	Num grad bit 10	
06-30-23 15:26:Iter: [250/391]	Time 0.065 (0.069)	Data 0.002 (0.003)	Loss 3.482 (2.067)	Prec@1 28.125 (34.313)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:27:Num bit 10	Num grad bit 10	
06-30-23 15:27:Iter: [300/391]	Time 0.077 (0.068)	Data 0.003 (0.003)	Loss 2.204 (2.184)	Prec@1 34.375 (33.957)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:27:Num bit 10	Num grad bit 10	
06-30-23 15:27:Iter: [350/391]	Time 0.061 (0.068)	Data 0.002 (0.003)	Loss 2.150 (2.198)	Prec@1 34.375 (34.072)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:27:Test: [0/79]	Time: 0.3020(0.3020)	Loss: 452.685(452.685)	Prec@1: 9.375(9.375)	
06-30-23 15:27:Test: [50/79]	Time: 0.0168(0.0231)	Loss: 529.264(456.341)	Prec@1: 10.156(10.325)	
06-30-23 15:27:Test: [78/79]	Time: 0.0152(0.0209)	Loss: 192.960(453.554)	Prec@1: 18.750(10.110)	
06-30-23 15:27:Step 68 * Prec@1 10.110
06-30-23 15:27:Num bit 10	Num grad bit 10	
06-30-23 15:27:Iter: [0/391]	Time 0.343 (0.343)	Data 0.293 (0.293)	Loss 12.409 (12.409)	Prec@1 10.938 (10.938)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:27:Num bit 10	Num grad bit 10	
06-30-23 15:27:Iter: [50/391]	Time 0.069 (0.066)	Data 0.003 (0.008)	Loss 33325904.000 (45864319.585)	Prec@1 11.719 (12.086)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:27:Num bit 10	Num grad bit 10	
06-30-23 15:27:Iter: [100/391]	Time 0.063 (0.065)	Data 0.003 (0.005)	Loss 3941667.500 (31328630.842)	Prec@1 13.281 (11.703)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:27:Num bit 10	Num grad bit 10	
06-30-23 15:27:Iter: [150/391]	Time 0.041 (0.062)	Data 0.002 (0.004)	Loss 21801264.000 (22239028.680)	Prec@1 8.594 (11.315)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:27:Num bit 10	Num grad bit 10	
06-30-23 15:27:Iter: [200/391]	Time 0.064 (0.062)	Data 0.003 (0.004)	Loss 30583974.000 (40882689.168)	Prec@1 10.156 (11.346)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:27:Num bit 10	Num grad bit 10	
06-30-23 15:27:Iter: [250/391]	Time 0.052 (0.061)	Data 0.003 (0.004)	Loss 102678688.000 (55775846.580)	Prec@1 8.594 (11.379)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:27:Num bit 10	Num grad bit 10	
06-30-23 15:27:Iter: [300/391]	Time 0.073 (0.062)	Data 0.003 (0.003)	Loss 3987.769 (47226395.314)	Prec@1 9.375 (11.148)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:27:Num bit 10	Num grad bit 10	
06-30-23 15:27:Iter: [350/391]	Time 0.038 (0.062)	Data 0.001 (0.003)	Loss 5545519616.000 (201412116.106)	Prec@1 13.281 (10.913)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:27:Test: [0/79]	Time: 0.3074(0.3074)	Loss: 1857968768.000(1857968768.000)	Prec@1: 11.719(11.719)	
06-30-23 15:27:Test: [50/79]	Time: 0.0169(0.0231)	Loss: 1843372672.000(1950351046.275)	Prec@1: 7.031(9.758)	
06-30-23 15:27:Test: [78/79]	Time: 0.0152(0.0210)	Loss: 1585251712.000(1922884667.597)	Prec@1: 18.750(9.760)	
06-30-23 15:27:Step 69 * Prec@1 9.760
06-30-23 15:27:Num bit 10	Num grad bit 10	
06-30-23 15:27:Iter: [0/391]	Time 0.329 (0.329)	Data 0.285 (0.285)	Loss 287116128.000 (287116128.000)	Prec@1 13.281 (13.281)	Training FLOPS ratio: 0.097656 (0.097656)	
06-30-23 15:27:Num bit 10	Num grad bit 10	
06-30-23 15:27:Iter: [50/391]	Time 0.065 (0.059)	Data 0.002 (0.008)	Loss 1440633856.000 (766726488.627)	Prec@1 13.281 (11.351)	Training FLOPS ratio: 0.097656 (0.097656)	
