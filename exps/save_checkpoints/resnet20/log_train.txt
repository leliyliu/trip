10-10-22 13:29:start training resnet20
10-10-22 13:29:Num bit 8	Num grad bit 8	
10-10-22 13:29:Iter: [0/391]	Time 0.309 (0.309)	Data 0.273 (0.273)	Loss 3.503 (3.503)	Prec@1 8.594 (8.594)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:29:Num bit 8	Num grad bit 8	
10-10-22 13:29:Iter: [50/391]	Time 0.070 (0.057)	Data 0.003 (0.007)	Loss 2.159 (2.347)	Prec@1 16.406 (17.953)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:29:Num bit 8	Num grad bit 8	
10-10-22 13:29:Iter: [100/391]	Time 0.035 (0.050)	Data 0.001 (0.005)	Loss 1.911 (2.162)	Prec@1 22.656 (21.542)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:29:Num bit 8	Num grad bit 8	
10-10-22 13:29:Iter: [150/391]	Time 0.035 (0.046)	Data 0.002 (0.004)	Loss 1.983 (2.070)	Prec@1 27.344 (23.722)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:29:Num bit 8	Num grad bit 8	
10-10-22 13:29:Iter: [200/391]	Time 0.051 (0.045)	Data 0.002 (0.003)	Loss 1.780 (2.011)	Prec@1 34.375 (25.451)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:29:Num bit 8	Num grad bit 8	
10-10-22 13:29:Iter: [250/391]	Time 0.033 (0.044)	Data 0.001 (0.003)	Loss 1.803 (1.968)	Prec@1 37.500 (26.836)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:29:Num bit 8	Num grad bit 8	
10-10-22 13:29:Iter: [300/391]	Time 0.041 (0.043)	Data 0.001 (0.003)	Loss 1.770 (1.935)	Prec@1 33.594 (27.658)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:29:Num bit 8	Num grad bit 8	
10-10-22 13:29:Iter: [350/391]	Time 0.032 (0.043)	Data 0.001 (0.003)	Loss 1.803 (1.913)	Prec@1 28.125 (28.381)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:29:Test: [0/79]	Time: 0.2601(0.2601)	Loss: 1.794(1.794)	Prec@1: 32.031(32.031)	
10-10-22 13:30:Test: [50/79]	Time: 0.0187(0.0211)	Loss: 1.737(1.840)	Prec@1: 37.500(32.154)	
10-10-22 13:30:Test: [78/79]	Time: 0.0171(0.0207)	Loss: 1.640(1.841)	Prec@1: 6.250(32.550)	
10-10-22 13:30:Step 0 * Prec@1 32.550
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [0/391]	Time 0.339 (0.339)	Data 0.293 (0.293)	Loss 2.019 (2.019)	Prec@1 23.438 (23.438)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [50/391]	Time 0.047 (0.058)	Data 0.001 (0.008)	Loss 1.709 (1.800)	Prec@1 35.156 (32.629)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [100/391]	Time 0.043 (0.051)	Data 0.002 (0.005)	Loss 1.741 (1.767)	Prec@1 34.375 (33.292)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [150/391]	Time 0.035 (0.047)	Data 0.002 (0.004)	Loss 1.664 (1.748)	Prec@1 36.719 (33.863)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [200/391]	Time 0.031 (0.047)	Data 0.001 (0.003)	Loss 1.605 (1.725)	Prec@1 35.938 (34.639)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [250/391]	Time 0.044 (0.046)	Data 0.001 (0.003)	Loss 1.621 (1.713)	Prec@1 35.938 (35.184)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [300/391]	Time 0.031 (0.044)	Data 0.001 (0.003)	Loss 1.595 (1.702)	Prec@1 40.625 (35.790)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [350/391]	Time 0.042 (0.042)	Data 0.001 (0.002)	Loss 1.583 (1.690)	Prec@1 35.156 (36.144)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Test: [0/79]	Time: 0.2967(0.2967)	Loss: 1.564(1.564)	Prec@1: 40.625(40.625)	
10-10-22 13:30:Test: [50/79]	Time: 0.0219(0.0294)	Loss: 1.597(1.637)	Prec@1: 39.844(39.491)	
10-10-22 13:30:Test: [78/79]	Time: 0.0174(0.0259)	Loss: 1.466(1.642)	Prec@1: 18.750(39.700)	
10-10-22 13:30:Step 1 * Prec@1 39.700
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [0/391]	Time 0.294 (0.294)	Data 0.259 (0.259)	Loss 1.865 (1.865)	Prec@1 28.906 (28.906)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [50/391]	Time 0.037 (0.045)	Data 0.002 (0.007)	Loss 1.694 (1.660)	Prec@1 40.625 (38.327)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [100/391]	Time 0.044 (0.042)	Data 0.001 (0.004)	Loss 1.553 (1.630)	Prec@1 45.312 (39.395)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [150/391]	Time 0.051 (0.044)	Data 0.002 (0.003)	Loss 1.608 (1.609)	Prec@1 32.031 (39.958)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [200/391]	Time 0.046 (0.043)	Data 0.001 (0.003)	Loss 1.662 (1.601)	Prec@1 34.375 (40.388)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [250/391]	Time 0.042 (0.043)	Data 0.002 (0.003)	Loss 1.548 (1.592)	Prec@1 41.406 (40.784)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [300/391]	Time 0.064 (0.043)	Data 0.002 (0.003)	Loss 1.520 (1.585)	Prec@1 42.969 (41.069)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [350/391]	Time 0.052 (0.045)	Data 0.002 (0.002)	Loss 1.486 (1.580)	Prec@1 42.188 (41.237)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Test: [0/79]	Time: 0.3026(0.3026)	Loss: 1.436(1.436)	Prec@1: 42.969(42.969)	
10-10-22 13:30:Test: [50/79]	Time: 0.0173(0.0225)	Loss: 1.615(1.551)	Prec@1: 35.938(42.785)	
10-10-22 13:30:Test: [78/79]	Time: 0.0151(0.0206)	Loss: 1.555(1.560)	Prec@1: 6.250(42.520)	
10-10-22 13:30:Step 2 * Prec@1 42.520
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [0/391]	Time 0.329 (0.329)	Data 0.292 (0.292)	Loss 1.711 (1.711)	Prec@1 35.156 (35.156)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [50/391]	Time 0.046 (0.040)	Data 0.003 (0.007)	Loss 1.574 (1.589)	Prec@1 42.188 (41.146)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [100/391]	Time 0.040 (0.043)	Data 0.001 (0.005)	Loss 1.635 (1.575)	Prec@1 35.938 (41.600)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [150/391]	Time 0.033 (0.042)	Data 0.001 (0.004)	Loss 1.461 (1.549)	Prec@1 49.219 (42.731)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [200/391]	Time 0.080 (0.042)	Data 0.003 (0.003)	Loss 1.504 (1.537)	Prec@1 41.406 (43.221)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [250/391]	Time 0.050 (0.044)	Data 0.002 (0.003)	Loss 1.613 (1.526)	Prec@1 41.406 (43.616)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [300/391]	Time 0.065 (0.045)	Data 0.002 (0.003)	Loss 1.493 (1.516)	Prec@1 45.312 (44.067)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [350/391]	Time 0.052 (0.045)	Data 0.004 (0.003)	Loss 1.447 (1.511)	Prec@1 47.656 (44.195)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:30:Test: [0/79]	Time: 0.3059(0.3059)	Loss: 1.334(1.334)	Prec@1: 47.656(47.656)	
10-10-22 13:30:Test: [50/79]	Time: 0.0167(0.0274)	Loss: 1.544(1.480)	Prec@1: 42.188(44.501)	
10-10-22 13:30:Test: [78/79]	Time: 0.0149(0.0238)	Loss: 1.423(1.489)	Prec@1: 43.750(44.530)	
10-10-22 13:30:Step 3 * Prec@1 44.530
10-10-22 13:30:Num bit 8	Num grad bit 8	
10-10-22 13:30:Iter: [0/391]	Time 0.386 (0.386)	Data 0.346 (0.346)	Loss 2.002 (2.002)	Prec@1 30.469 (30.469)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [50/391]	Time 0.069 (0.047)	Data 0.003 (0.008)	Loss 1.447 (1.549)	Prec@1 44.531 (43.076)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [100/391]	Time 0.043 (0.046)	Data 0.003 (0.005)	Loss 1.438 (1.514)	Prec@1 41.406 (44.114)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [150/391]	Time 0.040 (0.045)	Data 0.001 (0.004)	Loss 1.386 (1.490)	Prec@1 50.000 (45.028)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [200/391]	Time 0.031 (0.042)	Data 0.001 (0.003)	Loss 1.558 (1.469)	Prec@1 42.188 (46.067)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [250/391]	Time 0.039 (0.042)	Data 0.004 (0.003)	Loss 1.322 (1.455)	Prec@1 50.781 (46.486)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [300/391]	Time 0.039 (0.041)	Data 0.001 (0.003)	Loss 1.293 (1.444)	Prec@1 50.781 (46.942)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [350/391]	Time 0.032 (0.041)	Data 0.001 (0.003)	Loss 1.356 (1.437)	Prec@1 53.906 (47.218)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Test: [0/79]	Time: 0.2721(0.2721)	Loss: 1.325(1.325)	Prec@1: 53.125(53.125)	
10-10-22 13:31:Test: [50/79]	Time: 0.0169(0.0221)	Loss: 1.468(1.417)	Prec@1: 42.969(49.357)	
10-10-22 13:31:Test: [78/79]	Time: 0.0161(0.0206)	Loss: 1.296(1.424)	Prec@1: 43.750(48.770)	
10-10-22 13:31:Step 4 * Prec@1 48.770
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [0/391]	Time 0.322 (0.322)	Data 0.285 (0.285)	Loss 1.876 (1.876)	Prec@1 25.781 (25.781)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [50/391]	Time 0.038 (0.045)	Data 0.002 (0.007)	Loss 1.454 (1.520)	Prec@1 48.438 (43.827)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [100/391]	Time 0.054 (0.045)	Data 0.002 (0.005)	Loss 1.348 (1.467)	Prec@1 51.562 (46.171)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [150/391]	Time 0.044 (0.042)	Data 0.001 (0.004)	Loss 1.414 (1.435)	Prec@1 46.094 (47.527)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [200/391]	Time 0.036 (0.041)	Data 0.001 (0.003)	Loss 1.257 (1.421)	Prec@1 53.125 (47.967)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [250/391]	Time 0.046 (0.041)	Data 0.002 (0.003)	Loss 1.256 (1.406)	Prec@1 53.906 (48.596)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [300/391]	Time 0.033 (0.041)	Data 0.001 (0.003)	Loss 1.236 (1.391)	Prec@1 59.375 (49.138)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [350/391]	Time 0.031 (0.041)	Data 0.001 (0.002)	Loss 1.263 (1.384)	Prec@1 58.594 (49.403)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Test: [0/79]	Time: 0.2948(0.2948)	Loss: 1.267(1.267)	Prec@1: 55.469(55.469)	
10-10-22 13:31:Test: [50/79]	Time: 0.0164(0.0262)	Loss: 1.357(1.342)	Prec@1: 53.906(51.256)	
10-10-22 13:31:Test: [78/79]	Time: 0.0149(0.0261)	Loss: 1.130(1.341)	Prec@1: 43.750(51.400)	
10-10-22 13:31:Step 5 * Prec@1 51.400
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [0/391]	Time 0.338 (0.338)	Data 0.291 (0.291)	Loss 1.916 (1.916)	Prec@1 28.125 (28.125)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [50/391]	Time 0.049 (0.051)	Data 0.002 (0.007)	Loss 1.316 (1.499)	Prec@1 51.562 (45.343)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [100/391]	Time 0.063 (0.047)	Data 0.002 (0.005)	Loss 1.399 (1.429)	Prec@1 50.000 (47.881)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [150/391]	Time 0.032 (0.044)	Data 0.001 (0.004)	Loss 1.451 (1.402)	Prec@1 44.531 (48.701)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [200/391]	Time 0.036 (0.043)	Data 0.002 (0.003)	Loss 1.572 (1.384)	Prec@1 49.219 (49.444)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [250/391]	Time 0.039 (0.042)	Data 0.002 (0.003)	Loss 1.210 (1.365)	Prec@1 57.812 (50.106)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [300/391]	Time 0.035 (0.043)	Data 0.001 (0.003)	Loss 1.181 (1.353)	Prec@1 56.250 (50.514)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [350/391]	Time 0.041 (0.043)	Data 0.001 (0.003)	Loss 1.209 (1.341)	Prec@1 56.250 (51.064)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Test: [0/79]	Time: 0.2993(0.2993)	Loss: 1.330(1.330)	Prec@1: 49.219(49.219)	
10-10-22 13:31:Test: [50/79]	Time: 0.0228(0.0232)	Loss: 1.404(1.371)	Prec@1: 53.125(51.900)	
10-10-22 13:31:Test: [78/79]	Time: 0.0151(0.0214)	Loss: 1.094(1.363)	Prec@1: 68.750(51.920)	
10-10-22 13:31:Step 6 * Prec@1 51.920
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [0/391]	Time 0.366 (0.366)	Data 0.328 (0.328)	Loss 1.802 (1.802)	Prec@1 40.625 (40.625)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [50/391]	Time 0.054 (0.052)	Data 0.001 (0.008)	Loss 1.323 (1.477)	Prec@1 53.125 (46.354)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [100/391]	Time 0.042 (0.048)	Data 0.001 (0.005)	Loss 1.389 (1.393)	Prec@1 53.125 (49.520)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:31:Num bit 8	Num grad bit 8	
10-10-22 13:31:Iter: [150/391]	Time 0.033 (0.045)	Data 0.001 (0.004)	Loss 1.265 (1.354)	Prec@1 53.125 (50.983)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [200/391]	Time 0.037 (0.043)	Data 0.001 (0.003)	Loss 1.264 (1.328)	Prec@1 52.344 (51.912)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [250/391]	Time 0.057 (0.043)	Data 0.003 (0.003)	Loss 1.138 (1.312)	Prec@1 60.938 (52.596)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [300/391]	Time 0.036 (0.043)	Data 0.002 (0.003)	Loss 1.402 (1.305)	Prec@1 48.438 (52.904)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [350/391]	Time 0.033 (0.042)	Data 0.001 (0.003)	Loss 1.165 (1.297)	Prec@1 63.281 (53.167)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Test: [0/79]	Time: 0.3484(0.3484)	Loss: 1.319(1.319)	Prec@1: 51.562(51.562)	
10-10-22 13:32:Test: [50/79]	Time: 0.0196(0.0295)	Loss: 1.352(1.281)	Prec@1: 53.906(54.059)	
10-10-22 13:32:Test: [78/79]	Time: 0.0189(0.0268)	Loss: 1.130(1.283)	Prec@1: 56.250(53.520)	
10-10-22 13:32:Step 7 * Prec@1 53.520
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [0/391]	Time 0.352 (0.352)	Data 0.301 (0.301)	Loss 1.843 (1.843)	Prec@1 34.375 (34.375)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [50/391]	Time 0.040 (0.045)	Data 0.002 (0.008)	Loss 1.372 (1.430)	Prec@1 46.875 (47.457)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [100/391]	Time 0.061 (0.043)	Data 0.002 (0.005)	Loss 1.254 (1.355)	Prec@1 50.781 (50.062)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [150/391]	Time 0.042 (0.047)	Data 0.002 (0.004)	Loss 1.362 (1.323)	Prec@1 56.250 (51.754)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [200/391]	Time 0.078 (0.044)	Data 0.002 (0.003)	Loss 1.170 (1.294)	Prec@1 54.688 (52.950)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [250/391]	Time 0.034 (0.044)	Data 0.001 (0.003)	Loss 1.306 (1.282)	Prec@1 51.562 (53.545)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [300/391]	Time 0.036 (0.043)	Data 0.002 (0.003)	Loss 1.246 (1.273)	Prec@1 57.812 (53.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [350/391]	Time 0.047 (0.043)	Data 0.002 (0.003)	Loss 1.298 (1.260)	Prec@1 47.656 (54.396)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Test: [0/79]	Time: 0.2829(0.2829)	Loss: 1.223(1.223)	Prec@1: 53.906(53.906)	
10-10-22 13:32:Test: [50/79]	Time: 0.0163(0.0217)	Loss: 1.388(1.296)	Prec@1: 55.469(54.412)	
10-10-22 13:32:Test: [78/79]	Time: 0.0150(0.0198)	Loss: 1.122(1.300)	Prec@1: 50.000(54.460)	
10-10-22 13:32:Step 8 * Prec@1 54.460
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [0/391]	Time 0.362 (0.362)	Data 0.314 (0.314)	Loss 1.894 (1.894)	Prec@1 30.469 (30.469)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [50/391]	Time 0.039 (0.052)	Data 0.002 (0.008)	Loss 1.347 (1.381)	Prec@1 47.656 (50.107)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [100/391]	Time 0.041 (0.045)	Data 0.001 (0.005)	Loss 1.330 (1.325)	Prec@1 55.469 (52.506)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [150/391]	Time 0.047 (0.051)	Data 0.002 (0.004)	Loss 1.166 (1.288)	Prec@1 56.250 (53.508)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [200/391]	Time 0.036 (0.049)	Data 0.001 (0.004)	Loss 1.145 (1.259)	Prec@1 62.500 (54.513)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [250/391]	Time 0.041 (0.048)	Data 0.001 (0.003)	Loss 1.227 (1.246)	Prec@1 53.125 (54.983)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [300/391]	Time 0.034 (0.047)	Data 0.001 (0.003)	Loss 1.090 (1.232)	Prec@1 60.156 (55.565)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [350/391]	Time 0.086 (0.047)	Data 0.002 (0.003)	Loss 1.275 (1.225)	Prec@1 53.125 (55.852)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Test: [0/79]	Time: 0.2787(0.2787)	Loss: 1.239(1.239)	Prec@1: 53.906(53.906)	
10-10-22 13:32:Test: [50/79]	Time: 0.0170(0.0216)	Loss: 1.263(1.185)	Prec@1: 57.031(58.578)	
10-10-22 13:32:Test: [78/79]	Time: 0.0149(0.0197)	Loss: 0.948(1.191)	Prec@1: 62.500(58.120)	
10-10-22 13:32:Step 9 * Prec@1 58.120
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [0/391]	Time 0.361 (0.361)	Data 0.321 (0.321)	Loss 1.846 (1.846)	Prec@1 32.031 (32.031)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [50/391]	Time 0.032 (0.043)	Data 0.001 (0.008)	Loss 1.164 (1.335)	Prec@1 53.906 (51.746)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [100/391]	Time 0.040 (0.042)	Data 0.002 (0.005)	Loss 1.298 (1.272)	Prec@1 51.562 (54.185)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [150/391]	Time 0.033 (0.045)	Data 0.001 (0.004)	Loss 1.192 (1.235)	Prec@1 53.906 (55.412)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:32:Num bit 8	Num grad bit 8	
10-10-22 13:32:Iter: [200/391]	Time 0.036 (0.043)	Data 0.001 (0.003)	Loss 1.200 (1.217)	Prec@1 56.250 (56.172)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [250/391]	Time 0.039 (0.044)	Data 0.002 (0.003)	Loss 0.948 (1.201)	Prec@1 65.625 (56.801)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [300/391]	Time 0.046 (0.044)	Data 0.002 (0.003)	Loss 1.219 (1.193)	Prec@1 56.250 (57.192)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [350/391]	Time 0.055 (0.044)	Data 0.007 (0.003)	Loss 1.073 (1.181)	Prec@1 63.281 (57.675)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Test: [0/79]	Time: 0.2716(0.2716)	Loss: 1.120(1.120)	Prec@1: 59.375(59.375)	
10-10-22 13:33:Test: [50/79]	Time: 0.0162(0.0215)	Loss: 1.215(1.166)	Prec@1: 60.938(59.206)	
10-10-22 13:33:Test: [78/79]	Time: 0.0148(0.0196)	Loss: 0.988(1.178)	Prec@1: 62.500(58.870)	
10-10-22 13:33:Step 10 * Prec@1 58.870
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [0/391]	Time 0.351 (0.351)	Data 0.314 (0.314)	Loss 2.008 (2.008)	Prec@1 34.375 (34.375)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [50/391]	Time 0.092 (0.060)	Data 0.004 (0.008)	Loss 1.155 (1.321)	Prec@1 57.031 (52.114)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [100/391]	Time 0.034 (0.053)	Data 0.002 (0.005)	Loss 1.071 (1.256)	Prec@1 59.375 (54.819)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [150/391]	Time 0.038 (0.051)	Data 0.002 (0.004)	Loss 1.329 (1.218)	Prec@1 50.781 (56.348)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [200/391]	Time 0.048 (0.049)	Data 0.001 (0.004)	Loss 1.200 (1.194)	Prec@1 60.938 (57.070)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [250/391]	Time 0.040 (0.049)	Data 0.002 (0.003)	Loss 1.226 (1.183)	Prec@1 58.594 (57.420)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [300/391]	Time 0.036 (0.047)	Data 0.002 (0.003)	Loss 1.000 (1.172)	Prec@1 65.625 (57.807)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [350/391]	Time 0.034 (0.045)	Data 0.001 (0.003)	Loss 1.182 (1.161)	Prec@1 60.156 (58.293)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Test: [0/79]	Time: 0.2733(0.2733)	Loss: 1.100(1.100)	Prec@1: 59.375(59.375)	
10-10-22 13:33:Test: [50/79]	Time: 0.0160(0.0227)	Loss: 1.293(1.131)	Prec@1: 57.031(60.417)	
10-10-22 13:33:Test: [78/79]	Time: 0.0149(0.0204)	Loss: 0.994(1.140)	Prec@1: 68.750(59.930)	
10-10-22 13:33:Step 11 * Prec@1 59.930
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [0/391]	Time 0.313 (0.313)	Data 0.273 (0.273)	Loss 1.716 (1.716)	Prec@1 42.969 (42.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [50/391]	Time 0.038 (0.053)	Data 0.002 (0.007)	Loss 0.985 (1.285)	Prec@1 67.969 (54.381)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [100/391]	Time 0.049 (0.047)	Data 0.002 (0.005)	Loss 1.192 (1.225)	Prec@1 55.469 (56.436)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [150/391]	Time 0.051 (0.047)	Data 0.002 (0.004)	Loss 0.999 (1.185)	Prec@1 60.156 (57.781)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [200/391]	Time 0.036 (0.045)	Data 0.001 (0.003)	Loss 1.019 (1.164)	Prec@1 61.719 (58.687)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [250/391]	Time 0.036 (0.043)	Data 0.001 (0.003)	Loss 1.150 (1.149)	Prec@1 57.031 (59.213)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [300/391]	Time 0.038 (0.043)	Data 0.002 (0.003)	Loss 1.277 (1.137)	Prec@1 53.125 (59.461)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [350/391]	Time 0.036 (0.042)	Data 0.002 (0.002)	Loss 1.144 (1.128)	Prec@1 55.469 (59.758)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Test: [0/79]	Time: 0.3291(0.3291)	Loss: 1.004(1.004)	Prec@1: 66.406(66.406)	
10-10-22 13:33:Test: [50/79]	Time: 0.0241(0.0292)	Loss: 1.198(1.113)	Prec@1: 59.375(60.861)	
10-10-22 13:33:Test: [78/79]	Time: 0.0149(0.0266)	Loss: 0.945(1.123)	Prec@1: 75.000(60.530)	
10-10-22 13:33:Step 12 * Prec@1 60.530
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [0/391]	Time 0.371 (0.371)	Data 0.330 (0.330)	Loss 1.530 (1.530)	Prec@1 48.438 (48.438)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [50/391]	Time 0.039 (0.052)	Data 0.002 (0.008)	Loss 1.184 (1.276)	Prec@1 59.375 (54.519)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [100/391]	Time 0.038 (0.046)	Data 0.002 (0.005)	Loss 1.004 (1.197)	Prec@1 63.281 (57.441)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [150/391]	Time 0.039 (0.045)	Data 0.002 (0.004)	Loss 1.017 (1.153)	Prec@1 64.062 (59.184)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:33:Num bit 8	Num grad bit 8	
10-10-22 13:33:Iter: [200/391]	Time 0.041 (0.047)	Data 0.001 (0.004)	Loss 1.019 (1.123)	Prec@1 60.938 (60.024)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [250/391]	Time 0.038 (0.045)	Data 0.001 (0.003)	Loss 1.162 (1.111)	Prec@1 57.031 (60.374)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [300/391]	Time 0.042 (0.044)	Data 0.002 (0.003)	Loss 1.038 (1.098)	Prec@1 59.375 (60.784)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [350/391]	Time 0.042 (0.047)	Data 0.002 (0.003)	Loss 0.958 (1.087)	Prec@1 63.281 (61.147)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Test: [0/79]	Time: 0.2750(0.2750)	Loss: 0.976(0.976)	Prec@1: 59.375(59.375)	
10-10-22 13:34:Test: [50/79]	Time: 0.0165(0.0238)	Loss: 1.146(1.093)	Prec@1: 62.500(62.408)	
10-10-22 13:34:Test: [78/79]	Time: 0.0173(0.0225)	Loss: 0.937(1.103)	Prec@1: 56.250(62.120)	
10-10-22 13:34:Step 13 * Prec@1 62.120
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [0/391]	Time 0.415 (0.415)	Data 0.374 (0.374)	Loss 1.752 (1.752)	Prec@1 35.938 (35.938)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [50/391]	Time 0.046 (0.057)	Data 0.002 (0.010)	Loss 1.155 (1.245)	Prec@1 60.156 (55.974)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [100/391]	Time 0.041 (0.048)	Data 0.001 (0.006)	Loss 1.060 (1.169)	Prec@1 60.938 (58.609)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [150/391]	Time 0.037 (0.045)	Data 0.001 (0.004)	Loss 1.077 (1.132)	Prec@1 61.719 (59.685)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [200/391]	Time 0.034 (0.051)	Data 0.001 (0.004)	Loss 0.968 (1.109)	Prec@1 62.500 (60.677)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [250/391]	Time 0.128 (0.050)	Data 0.005 (0.004)	Loss 0.809 (1.090)	Prec@1 74.219 (61.211)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [300/391]	Time 0.037 (0.049)	Data 0.002 (0.003)	Loss 1.143 (1.078)	Prec@1 59.375 (61.675)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [350/391]	Time 0.057 (0.049)	Data 0.003 (0.003)	Loss 1.039 (1.067)	Prec@1 60.156 (62.048)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Test: [0/79]	Time: 0.2931(0.2931)	Loss: 0.984(0.984)	Prec@1: 64.062(64.062)	
10-10-22 13:34:Test: [50/79]	Time: 0.0161(0.0233)	Loss: 1.191(1.081)	Prec@1: 60.938(62.439)	
10-10-22 13:34:Test: [78/79]	Time: 0.0147(0.0207)	Loss: 0.957(1.088)	Prec@1: 62.500(61.970)	
10-10-22 13:34:Step 14 * Prec@1 61.970
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [0/391]	Time 0.348 (0.348)	Data 0.297 (0.297)	Loss 1.826 (1.826)	Prec@1 36.719 (36.719)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [50/391]	Time 0.051 (0.061)	Data 0.002 (0.008)	Loss 0.953 (1.225)	Prec@1 65.625 (55.147)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [100/391]	Time 0.036 (0.054)	Data 0.001 (0.005)	Loss 0.842 (1.160)	Prec@1 74.219 (58.207)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [150/391]	Time 0.034 (0.052)	Data 0.003 (0.004)	Loss 1.028 (1.112)	Prec@1 66.406 (60.006)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [200/391]	Time 0.038 (0.048)	Data 0.002 (0.003)	Loss 1.184 (1.086)	Prec@1 52.344 (60.922)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [250/391]	Time 0.040 (0.047)	Data 0.002 (0.003)	Loss 1.192 (1.070)	Prec@1 62.500 (61.492)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [300/391]	Time 0.045 (0.047)	Data 0.001 (0.003)	Loss 0.940 (1.056)	Prec@1 67.188 (62.007)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [350/391]	Time 0.032 (0.046)	Data 0.002 (0.003)	Loss 0.883 (1.046)	Prec@1 66.406 (62.418)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Test: [0/79]	Time: 0.3669(0.3669)	Loss: 0.859(0.859)	Prec@1: 67.188(67.188)	
10-10-22 13:34:Test: [50/79]	Time: 0.0167(0.0530)	Loss: 1.094(1.006)	Prec@1: 64.844(65.196)	
10-10-22 13:34:Test: [78/79]	Time: 0.0156(0.0409)	Loss: 1.046(1.016)	Prec@1: 50.000(64.460)	
10-10-22 13:34:Step 15 * Prec@1 64.460
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [0/391]	Time 0.329 (0.329)	Data 0.292 (0.292)	Loss 1.708 (1.708)	Prec@1 46.875 (46.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [50/391]	Time 0.034 (0.040)	Data 0.001 (0.007)	Loss 1.231 (1.203)	Prec@1 56.250 (57.276)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [100/391]	Time 0.038 (0.043)	Data 0.001 (0.005)	Loss 1.016 (1.124)	Prec@1 64.062 (60.280)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:34:Num bit 8	Num grad bit 8	
10-10-22 13:34:Iter: [150/391]	Time 0.033 (0.043)	Data 0.002 (0.004)	Loss 1.019 (1.080)	Prec@1 62.500 (61.988)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [200/391]	Time 0.052 (0.042)	Data 0.004 (0.003)	Loss 0.788 (1.058)	Prec@1 71.094 (62.737)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [250/391]	Time 0.035 (0.041)	Data 0.001 (0.003)	Loss 0.870 (1.040)	Prec@1 65.625 (63.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [300/391]	Time 0.116 (0.041)	Data 0.007 (0.003)	Loss 1.087 (1.029)	Prec@1 62.500 (63.517)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [350/391]	Time 0.036 (0.042)	Data 0.002 (0.003)	Loss 0.907 (1.017)	Prec@1 64.844 (63.965)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Test: [0/79]	Time: 0.3041(0.3041)	Loss: 0.856(0.856)	Prec@1: 67.969(67.969)	
10-10-22 13:35:Test: [50/79]	Time: 0.0190(0.0264)	Loss: 1.127(1.007)	Prec@1: 60.938(65.227)	
10-10-22 13:35:Test: [78/79]	Time: 0.0150(0.0238)	Loss: 1.079(1.018)	Prec@1: 43.750(65.050)	
10-10-22 13:35:Step 16 * Prec@1 65.050
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [0/391]	Time 0.342 (0.342)	Data 0.307 (0.307)	Loss 1.687 (1.687)	Prec@1 38.281 (38.281)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [50/391]	Time 0.032 (0.057)	Data 0.001 (0.008)	Loss 0.991 (1.189)	Prec@1 64.844 (57.659)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [100/391]	Time 0.035 (0.046)	Data 0.001 (0.005)	Loss 0.992 (1.108)	Prec@1 64.844 (60.821)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [150/391]	Time 0.035 (0.043)	Data 0.001 (0.004)	Loss 0.984 (1.062)	Prec@1 66.406 (62.174)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [200/391]	Time 0.038 (0.046)	Data 0.002 (0.003)	Loss 0.921 (1.038)	Prec@1 67.188 (63.048)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [250/391]	Time 0.036 (0.045)	Data 0.001 (0.003)	Loss 1.079 (1.022)	Prec@1 55.469 (63.751)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [300/391]	Time 0.131 (0.045)	Data 0.004 (0.003)	Loss 0.881 (1.007)	Prec@1 71.875 (64.312)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [350/391]	Time 0.056 (0.045)	Data 0.002 (0.003)	Loss 0.954 (1.000)	Prec@1 65.625 (64.563)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Test: [0/79]	Time: 0.3299(0.3299)	Loss: 0.909(0.909)	Prec@1: 67.188(67.188)	
10-10-22 13:35:Test: [50/79]	Time: 0.0183(0.0243)	Loss: 1.162(1.055)	Prec@1: 60.156(64.415)	
10-10-22 13:35:Test: [78/79]	Time: 0.0168(0.0221)	Loss: 1.113(1.066)	Prec@1: 62.500(64.040)	
10-10-22 13:35:Step 17 * Prec@1 64.040
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [0/391]	Time 0.384 (0.384)	Data 0.349 (0.349)	Loss 1.926 (1.926)	Prec@1 34.375 (34.375)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [50/391]	Time 0.124 (0.047)	Data 0.005 (0.008)	Loss 0.985 (1.161)	Prec@1 66.406 (58.134)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [100/391]	Time 0.036 (0.049)	Data 0.002 (0.006)	Loss 1.008 (1.078)	Prec@1 64.844 (61.641)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [150/391]	Time 0.044 (0.048)	Data 0.002 (0.004)	Loss 0.833 (1.030)	Prec@1 75.000 (63.276)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [200/391]	Time 0.038 (0.046)	Data 0.002 (0.004)	Loss 0.856 (1.003)	Prec@1 72.656 (64.428)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [250/391]	Time 0.059 (0.046)	Data 0.003 (0.003)	Loss 0.850 (0.986)	Prec@1 71.875 (64.869)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [300/391]	Time 0.040 (0.048)	Data 0.002 (0.003)	Loss 0.908 (0.975)	Prec@1 67.969 (65.347)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [350/391]	Time 0.035 (0.046)	Data 0.001 (0.003)	Loss 0.939 (0.967)	Prec@1 74.219 (65.718)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Test: [0/79]	Time: 0.2994(0.2994)	Loss: 0.851(0.851)	Prec@1: 66.406(66.406)	
10-10-22 13:35:Test: [50/79]	Time: 0.0271(0.0245)	Loss: 1.104(1.008)	Prec@1: 64.062(65.717)	
10-10-22 13:35:Test: [78/79]	Time: 0.0150(0.0229)	Loss: 1.097(1.014)	Prec@1: 62.500(65.600)	
10-10-22 13:35:Step 18 * Prec@1 65.600
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [0/391]	Time 0.670 (0.670)	Data 0.607 (0.607)	Loss 1.657 (1.657)	Prec@1 36.719 (36.719)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [50/391]	Time 0.036 (0.055)	Data 0.001 (0.013)	Loss 1.059 (1.143)	Prec@1 64.062 (58.762)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [100/391]	Time 0.045 (0.050)	Data 0.002 (0.008)	Loss 1.075 (1.054)	Prec@1 61.719 (62.539)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:35:Num bit 8	Num grad bit 8	
10-10-22 13:35:Iter: [150/391]	Time 0.047 (0.049)	Data 0.002 (0.006)	Loss 0.839 (1.018)	Prec@1 72.656 (63.649)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [200/391]	Time 0.035 (0.046)	Data 0.001 (0.005)	Loss 0.870 (0.993)	Prec@1 71.875 (64.587)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [250/391]	Time 0.102 (0.045)	Data 0.005 (0.004)	Loss 0.660 (0.975)	Prec@1 77.344 (65.223)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [300/391]	Time 0.035 (0.045)	Data 0.001 (0.004)	Loss 0.971 (0.961)	Prec@1 62.500 (65.716)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [350/391]	Time 0.048 (0.045)	Data 0.005 (0.003)	Loss 1.081 (0.952)	Prec@1 63.281 (65.974)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Test: [0/79]	Time: 0.3546(0.3546)	Loss: 0.845(0.845)	Prec@1: 71.094(71.094)	
10-10-22 13:36:Test: [50/79]	Time: 0.0187(0.0252)	Loss: 1.156(0.978)	Prec@1: 64.062(66.544)	
10-10-22 13:36:Test: [78/79]	Time: 0.0151(0.0233)	Loss: 0.934(0.977)	Prec@1: 62.500(66.730)	
10-10-22 13:36:Step 19 * Prec@1 66.730
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [0/391]	Time 0.563 (0.563)	Data 0.519 (0.519)	Loss 1.856 (1.856)	Prec@1 37.500 (37.500)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [50/391]	Time 0.054 (0.061)	Data 0.002 (0.012)	Loss 0.807 (1.106)	Prec@1 74.219 (61.075)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [100/391]	Time 0.196 (0.057)	Data 0.006 (0.007)	Loss 1.146 (1.025)	Prec@1 59.375 (63.923)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [150/391]	Time 0.035 (0.059)	Data 0.001 (0.006)	Loss 0.904 (0.976)	Prec@1 67.969 (65.604)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [200/391]	Time 0.036 (0.056)	Data 0.001 (0.005)	Loss 1.201 (0.956)	Prec@1 54.688 (66.084)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [250/391]	Time 0.037 (0.052)	Data 0.002 (0.004)	Loss 0.988 (0.941)	Prec@1 60.938 (66.627)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [300/391]	Time 0.049 (0.051)	Data 0.001 (0.004)	Loss 0.977 (0.934)	Prec@1 59.375 (66.920)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [350/391]	Time 0.032 (0.051)	Data 0.001 (0.003)	Loss 0.888 (0.927)	Prec@1 72.656 (67.285)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Test: [0/79]	Time: 0.3283(0.3283)	Loss: 0.916(0.916)	Prec@1: 67.969(67.969)	
10-10-22 13:36:Test: [50/79]	Time: 0.0160(0.0233)	Loss: 1.146(1.004)	Prec@1: 64.062(65.349)	
10-10-22 13:36:Test: [78/79]	Time: 0.0151(0.0209)	Loss: 0.814(1.004)	Prec@1: 68.750(65.280)	
10-10-22 13:36:Step 20 * Prec@1 65.280
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [0/391]	Time 0.401 (0.401)	Data 0.311 (0.311)	Loss 1.643 (1.643)	Prec@1 42.969 (42.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [50/391]	Time 0.035 (0.049)	Data 0.001 (0.008)	Loss 1.019 (1.079)	Prec@1 62.500 (62.684)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [100/391]	Time 0.036 (0.048)	Data 0.002 (0.005)	Loss 0.876 (1.012)	Prec@1 67.969 (64.418)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [150/391]	Time 0.035 (0.044)	Data 0.001 (0.004)	Loss 0.844 (0.980)	Prec@1 70.312 (65.630)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [200/391]	Time 0.036 (0.044)	Data 0.002 (0.003)	Loss 0.926 (0.954)	Prec@1 64.844 (66.383)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [250/391]	Time 0.037 (0.043)	Data 0.002 (0.003)	Loss 1.052 (0.940)	Prec@1 67.188 (66.836)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [300/391]	Time 0.040 (0.043)	Data 0.001 (0.003)	Loss 0.825 (0.924)	Prec@1 75.781 (67.361)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [350/391]	Time 0.036 (0.043)	Data 0.002 (0.003)	Loss 0.883 (0.917)	Prec@1 68.750 (67.682)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Test: [0/79]	Time: 0.3644(0.3644)	Loss: 0.873(0.873)	Prec@1: 68.750(68.750)	
10-10-22 13:36:Test: [50/79]	Time: 0.0305(0.0368)	Loss: 1.057(0.965)	Prec@1: 60.938(66.789)	
10-10-22 13:36:Test: [78/79]	Time: 0.0241(0.0344)	Loss: 0.833(0.968)	Prec@1: 81.250(66.920)	
10-10-22 13:36:Step 21 * Prec@1 66.920
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [0/391]	Time 0.356 (0.356)	Data 0.299 (0.299)	Loss 1.605 (1.605)	Prec@1 39.844 (39.844)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [50/391]	Time 0.035 (0.045)	Data 0.001 (0.007)	Loss 0.875 (1.027)	Prec@1 72.656 (63.649)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [100/391]	Time 0.036 (0.046)	Data 0.001 (0.005)	Loss 0.780 (0.969)	Prec@1 71.094 (65.687)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [150/391]	Time 0.066 (0.045)	Data 0.004 (0.004)	Loss 0.826 (0.946)	Prec@1 70.312 (66.355)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:36:Num bit 8	Num grad bit 8	
10-10-22 13:36:Iter: [200/391]	Time 0.035 (0.044)	Data 0.002 (0.003)	Loss 0.778 (0.926)	Prec@1 74.219 (67.083)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [250/391]	Time 0.037 (0.043)	Data 0.001 (0.003)	Loss 0.928 (0.915)	Prec@1 66.406 (67.570)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [300/391]	Time 0.042 (0.043)	Data 0.002 (0.003)	Loss 0.920 (0.905)	Prec@1 68.750 (67.997)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [350/391]	Time 0.035 (0.046)	Data 0.001 (0.003)	Loss 0.846 (0.894)	Prec@1 73.438 (68.389)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Test: [0/79]	Time: 0.6876(0.6876)	Loss: 0.891(0.891)	Prec@1: 64.062(64.062)	
10-10-22 13:37:Test: [50/79]	Time: 0.0162(0.0302)	Loss: 1.144(0.964)	Prec@1: 61.719(66.728)	
10-10-22 13:37:Test: [78/79]	Time: 0.0201(0.0257)	Loss: 0.801(0.966)	Prec@1: 62.500(66.910)	
10-10-22 13:37:Step 22 * Prec@1 66.910
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [0/391]	Time 0.419 (0.419)	Data 0.379 (0.379)	Loss 1.662 (1.662)	Prec@1 40.625 (40.625)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [50/391]	Time 0.049 (0.047)	Data 0.001 (0.009)	Loss 0.926 (1.083)	Prec@1 70.312 (61.918)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [100/391]	Time 0.049 (0.047)	Data 0.002 (0.006)	Loss 0.867 (0.988)	Prec@1 68.750 (65.215)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [150/391]	Time 0.035 (0.043)	Data 0.001 (0.004)	Loss 0.983 (0.944)	Prec@1 65.625 (66.696)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [200/391]	Time 0.033 (0.042)	Data 0.001 (0.004)	Loss 0.848 (0.925)	Prec@1 71.094 (67.312)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [250/391]	Time 0.034 (0.041)	Data 0.001 (0.003)	Loss 0.872 (0.908)	Prec@1 73.438 (67.826)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [300/391]	Time 0.113 (0.043)	Data 0.008 (0.003)	Loss 0.935 (0.896)	Prec@1 65.625 (68.239)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [350/391]	Time 0.037 (0.043)	Data 0.002 (0.003)	Loss 0.875 (0.885)	Prec@1 71.875 (68.697)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Test: [0/79]	Time: 0.3168(0.3168)	Loss: 0.805(0.805)	Prec@1: 71.094(71.094)	
10-10-22 13:37:Test: [50/79]	Time: 0.0166(0.0224)	Loss: 1.047(0.941)	Prec@1: 62.500(67.816)	
10-10-22 13:37:Test: [78/79]	Time: 0.0152(0.0204)	Loss: 0.514(0.943)	Prec@1: 81.250(67.820)	
10-10-22 13:37:Step 23 * Prec@1 67.820
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [0/391]	Time 0.461 (0.461)	Data 0.422 (0.422)	Loss 1.569 (1.569)	Prec@1 39.844 (39.844)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [50/391]	Time 0.036 (0.052)	Data 0.002 (0.010)	Loss 0.929 (1.070)	Prec@1 71.875 (61.412)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [100/391]	Time 0.036 (0.045)	Data 0.002 (0.006)	Loss 0.912 (0.984)	Prec@1 66.406 (64.550)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [150/391]	Time 0.031 (0.046)	Data 0.001 (0.005)	Loss 0.815 (0.938)	Prec@1 70.312 (66.339)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [200/391]	Time 0.037 (0.044)	Data 0.001 (0.004)	Loss 0.736 (0.918)	Prec@1 70.312 (67.343)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [250/391]	Time 0.041 (0.043)	Data 0.002 (0.003)	Loss 0.832 (0.900)	Prec@1 72.656 (68.065)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [300/391]	Time 0.035 (0.042)	Data 0.001 (0.003)	Loss 0.921 (0.888)	Prec@1 67.188 (68.381)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [350/391]	Time 0.035 (0.041)	Data 0.001 (0.003)	Loss 0.957 (0.880)	Prec@1 69.531 (68.750)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Test: [0/79]	Time: 0.3355(0.3355)	Loss: 0.834(0.834)	Prec@1: 69.531(69.531)	
10-10-22 13:37:Test: [50/79]	Time: 0.0161(0.0231)	Loss: 1.101(0.927)	Prec@1: 57.031(68.061)	
10-10-22 13:37:Test: [78/79]	Time: 0.0150(0.0207)	Loss: 0.813(0.933)	Prec@1: 68.750(68.130)	
10-10-22 13:37:Step 24 * Prec@1 68.130
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [0/391]	Time 0.539 (0.539)	Data 0.379 (0.379)	Loss 1.374 (1.374)	Prec@1 53.125 (53.125)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [50/391]	Time 0.039 (0.056)	Data 0.002 (0.010)	Loss 0.901 (1.029)	Prec@1 68.750 (63.343)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [100/391]	Time 0.032 (0.046)	Data 0.001 (0.006)	Loss 0.934 (0.958)	Prec@1 61.719 (66.027)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [150/391]	Time 0.032 (0.042)	Data 0.002 (0.004)	Loss 0.823 (0.920)	Prec@1 71.875 (67.519)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [200/391]	Time 0.040 (0.043)	Data 0.002 (0.004)	Loss 0.983 (0.896)	Prec@1 64.062 (68.439)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [250/391]	Time 0.034 (0.042)	Data 0.001 (0.003)	Loss 0.815 (0.877)	Prec@1 71.094 (69.039)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:37:Num bit 8	Num grad bit 8	
10-10-22 13:37:Iter: [300/391]	Time 0.035 (0.041)	Data 0.002 (0.003)	Loss 0.682 (0.866)	Prec@1 74.219 (69.350)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [350/391]	Time 0.039 (0.041)	Data 0.001 (0.003)	Loss 0.725 (0.858)	Prec@1 77.344 (69.700)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Test: [0/79]	Time: 0.3665(0.3665)	Loss: 0.753(0.753)	Prec@1: 73.438(73.438)	
10-10-22 13:38:Test: [50/79]	Time: 0.0183(0.0252)	Loss: 1.063(0.901)	Prec@1: 62.500(69.041)	
10-10-22 13:38:Test: [78/79]	Time: 0.0149(0.0225)	Loss: 0.619(0.900)	Prec@1: 75.000(69.160)	
10-10-22 13:38:Step 25 * Prec@1 69.160
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [0/391]	Time 0.366 (0.366)	Data 0.306 (0.306)	Loss 1.678 (1.678)	Prec@1 47.656 (47.656)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [50/391]	Time 0.048 (0.054)	Data 0.002 (0.008)	Loss 0.878 (1.007)	Prec@1 67.188 (64.047)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [100/391]	Time 0.039 (0.046)	Data 0.003 (0.005)	Loss 0.947 (0.932)	Prec@1 65.625 (66.948)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [150/391]	Time 0.035 (0.046)	Data 0.001 (0.004)	Loss 0.715 (0.895)	Prec@1 78.125 (68.326)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [200/391]	Time 0.041 (0.044)	Data 0.002 (0.003)	Loss 0.680 (0.877)	Prec@1 76.562 (68.960)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [250/391]	Time 0.039 (0.045)	Data 0.002 (0.003)	Loss 0.834 (0.862)	Prec@1 65.625 (69.491)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [300/391]	Time 0.031 (0.044)	Data 0.001 (0.003)	Loss 0.690 (0.847)	Prec@1 75.000 (69.970)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [350/391]	Time 0.035 (0.043)	Data 0.001 (0.003)	Loss 0.855 (0.840)	Prec@1 67.969 (70.208)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Test: [0/79]	Time: 0.3672(0.3672)	Loss: 0.684(0.684)	Prec@1: 73.438(73.438)	
10-10-22 13:38:Test: [50/79]	Time: 0.0162(0.0252)	Loss: 1.040(0.855)	Prec@1: 61.719(70.205)	
10-10-22 13:38:Test: [78/79]	Time: 0.0152(0.0220)	Loss: 0.682(0.854)	Prec@1: 68.750(70.260)	
10-10-22 13:38:Step 26 * Prec@1 70.260
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [0/391]	Time 0.444 (0.444)	Data 0.402 (0.402)	Loss 1.373 (1.373)	Prec@1 53.906 (53.906)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [50/391]	Time 0.036 (0.049)	Data 0.001 (0.010)	Loss 0.666 (0.985)	Prec@1 77.344 (65.656)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [100/391]	Time 0.047 (0.050)	Data 0.002 (0.006)	Loss 0.857 (0.915)	Prec@1 71.094 (67.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [150/391]	Time 0.054 (0.049)	Data 0.002 (0.005)	Loss 0.630 (0.877)	Prec@1 74.219 (69.262)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [200/391]	Time 0.035 (0.046)	Data 0.001 (0.004)	Loss 1.065 (0.854)	Prec@1 59.375 (70.122)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [250/391]	Time 0.037 (0.044)	Data 0.002 (0.003)	Loss 0.936 (0.840)	Prec@1 62.500 (70.502)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [300/391]	Time 0.036 (0.042)	Data 0.001 (0.003)	Loss 0.817 (0.831)	Prec@1 67.969 (70.754)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [350/391]	Time 0.037 (0.045)	Data 0.002 (0.003)	Loss 0.693 (0.824)	Prec@1 73.438 (71.009)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Test: [0/79]	Time: 0.3068(0.3068)	Loss: 0.748(0.748)	Prec@1: 74.219(74.219)	
10-10-22 13:38:Test: [50/79]	Time: 0.0170(0.0225)	Loss: 1.090(0.870)	Prec@1: 63.281(70.267)	
10-10-22 13:38:Test: [78/79]	Time: 0.0150(0.0205)	Loss: 0.694(0.869)	Prec@1: 75.000(70.520)	
10-10-22 13:38:Step 27 * Prec@1 70.520
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [0/391]	Time 0.429 (0.429)	Data 0.391 (0.391)	Loss 1.420 (1.420)	Prec@1 41.406 (41.406)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [50/391]	Time 0.032 (0.053)	Data 0.001 (0.010)	Loss 0.938 (0.987)	Prec@1 72.656 (65.196)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [100/391]	Time 0.039 (0.045)	Data 0.002 (0.006)	Loss 0.883 (0.902)	Prec@1 67.969 (68.201)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [150/391]	Time 0.035 (0.045)	Data 0.002 (0.004)	Loss 0.859 (0.860)	Prec@1 74.219 (69.635)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [200/391]	Time 0.037 (0.043)	Data 0.001 (0.004)	Loss 0.628 (0.840)	Prec@1 78.906 (70.281)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [250/391]	Time 0.040 (0.042)	Data 0.002 (0.003)	Loss 0.768 (0.828)	Prec@1 73.438 (70.702)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [300/391]	Time 0.042 (0.041)	Data 0.002 (0.003)	Loss 0.901 (0.820)	Prec@1 67.188 (71.031)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:38:Num bit 8	Num grad bit 8	
10-10-22 13:38:Iter: [350/391]	Time 0.032 (0.043)	Data 0.001 (0.003)	Loss 0.662 (0.813)	Prec@1 76.562 (71.296)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Test: [0/79]	Time: 0.4106(0.4106)	Loss: 0.829(0.829)	Prec@1: 69.531(69.531)	
10-10-22 13:39:Test: [50/79]	Time: 0.0189(0.0256)	Loss: 1.131(0.949)	Prec@1: 60.156(68.750)	
10-10-22 13:39:Test: [78/79]	Time: 0.0197(0.0229)	Loss: 0.568(0.951)	Prec@1: 75.000(68.810)	
10-10-22 13:39:Step 28 * Prec@1 68.810
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [0/391]	Time 0.612 (0.612)	Data 0.574 (0.574)	Loss 1.841 (1.841)	Prec@1 35.156 (35.156)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [50/391]	Time 0.047 (0.053)	Data 0.002 (0.013)	Loss 1.021 (0.990)	Prec@1 65.625 (65.242)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [100/391]	Time 0.045 (0.047)	Data 0.002 (0.007)	Loss 0.742 (0.904)	Prec@1 74.219 (68.170)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [150/391]	Time 0.056 (0.048)	Data 0.002 (0.006)	Loss 0.828 (0.868)	Prec@1 70.312 (69.350)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [200/391]	Time 0.034 (0.046)	Data 0.001 (0.005)	Loss 0.725 (0.852)	Prec@1 72.656 (69.885)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [250/391]	Time 0.050 (0.047)	Data 0.001 (0.004)	Loss 0.702 (0.836)	Prec@1 76.562 (70.596)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [300/391]	Time 0.035 (0.046)	Data 0.002 (0.004)	Loss 0.516 (0.824)	Prec@1 80.469 (70.967)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [350/391]	Time 0.039 (0.045)	Data 0.002 (0.004)	Loss 0.681 (0.815)	Prec@1 73.438 (71.216)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Test: [0/79]	Time: 0.3093(0.3093)	Loss: 0.807(0.807)	Prec@1: 69.531(69.531)	
10-10-22 13:39:Test: [50/79]	Time: 0.0175(0.0231)	Loss: 1.088(0.888)	Prec@1: 63.281(70.267)	
10-10-22 13:39:Test: [78/79]	Time: 0.0151(0.0206)	Loss: 0.732(0.892)	Prec@1: 75.000(70.070)	
10-10-22 13:39:Step 29 * Prec@1 70.070
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [0/391]	Time 0.360 (0.360)	Data 0.319 (0.319)	Loss 1.792 (1.792)	Prec@1 44.531 (44.531)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [50/391]	Time 0.044 (0.055)	Data 0.002 (0.008)	Loss 0.884 (0.987)	Prec@1 68.750 (65.135)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [100/391]	Time 0.032 (0.046)	Data 0.001 (0.005)	Loss 0.762 (0.898)	Prec@1 71.875 (68.224)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [150/391]	Time 0.037 (0.047)	Data 0.002 (0.004)	Loss 1.064 (0.857)	Prec@1 67.188 (69.614)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [200/391]	Time 0.031 (0.046)	Data 0.001 (0.004)	Loss 0.886 (0.835)	Prec@1 73.438 (70.379)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [250/391]	Time 0.035 (0.043)	Data 0.001 (0.003)	Loss 0.729 (0.821)	Prec@1 71.094 (70.835)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [300/391]	Time 0.034 (0.047)	Data 0.001 (0.003)	Loss 0.783 (0.811)	Prec@1 71.875 (71.146)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [350/391]	Time 0.035 (0.046)	Data 0.001 (0.003)	Loss 0.751 (0.800)	Prec@1 74.219 (71.588)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Test: [0/79]	Time: 0.4381(0.4381)	Loss: 0.826(0.826)	Prec@1: 71.875(71.875)	
10-10-22 13:39:Test: [50/79]	Time: 0.0163(0.0257)	Loss: 1.065(0.874)	Prec@1: 61.719(70.420)	
10-10-22 13:39:Test: [78/79]	Time: 0.0187(0.0237)	Loss: 0.503(0.865)	Prec@1: 87.500(70.520)	
10-10-22 13:39:Step 30 * Prec@1 70.520
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [0/391]	Time 0.424 (0.424)	Data 0.384 (0.384)	Loss 1.853 (1.853)	Prec@1 41.406 (41.406)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [50/391]	Time 0.057 (0.056)	Data 0.002 (0.010)	Loss 0.799 (0.932)	Prec@1 75.000 (66.820)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [100/391]	Time 0.041 (0.051)	Data 0.001 (0.006)	Loss 0.608 (0.868)	Prec@1 76.562 (69.276)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [150/391]	Time 0.042 (0.057)	Data 0.001 (0.005)	Loss 0.794 (0.845)	Prec@1 76.562 (70.281)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [200/391]	Time 0.035 (0.052)	Data 0.001 (0.004)	Loss 0.724 (0.829)	Prec@1 73.438 (70.923)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [250/391]	Time 0.035 (0.051)	Data 0.001 (0.004)	Loss 0.782 (0.806)	Prec@1 73.438 (71.750)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [300/391]	Time 0.033 (0.049)	Data 0.001 (0.003)	Loss 0.855 (0.796)	Prec@1 65.625 (72.116)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:39:Num bit 8	Num grad bit 8	
10-10-22 13:39:Iter: [350/391]	Time 0.041 (0.048)	Data 0.002 (0.003)	Loss 0.816 (0.789)	Prec@1 69.531 (72.258)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Test: [0/79]	Time: 0.3017(0.3017)	Loss: 0.720(0.720)	Prec@1: 77.344(77.344)	
10-10-22 13:40:Test: [50/79]	Time: 0.0162(0.0260)	Loss: 1.064(0.848)	Prec@1: 63.281(71.737)	
10-10-22 13:40:Test: [78/79]	Time: 0.0155(0.0227)	Loss: 0.531(0.846)	Prec@1: 68.750(71.580)	
10-10-22 13:40:Step 31 * Prec@1 71.580
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [0/391]	Time 0.335 (0.335)	Data 0.295 (0.295)	Loss 1.760 (1.760)	Prec@1 42.969 (42.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [50/391]	Time 0.036 (0.044)	Data 0.002 (0.007)	Loss 0.778 (0.918)	Prec@1 73.438 (67.525)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [100/391]	Time 0.035 (0.040)	Data 0.001 (0.004)	Loss 0.986 (0.851)	Prec@1 67.969 (69.941)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [150/391]	Time 0.033 (0.045)	Data 0.002 (0.004)	Loss 0.700 (0.823)	Prec@1 74.219 (71.047)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [200/391]	Time 0.036 (0.043)	Data 0.002 (0.003)	Loss 0.848 (0.814)	Prec@1 71.875 (71.428)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [250/391]	Time 0.035 (0.042)	Data 0.002 (0.003)	Loss 0.575 (0.801)	Prec@1 76.562 (71.813)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [300/391]	Time 0.035 (0.042)	Data 0.001 (0.003)	Loss 0.709 (0.793)	Prec@1 69.531 (72.062)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [350/391]	Time 0.036 (0.042)	Data 0.002 (0.003)	Loss 0.839 (0.786)	Prec@1 70.312 (72.342)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Test: [0/79]	Time: 0.3702(0.3702)	Loss: 0.762(0.762)	Prec@1: 68.750(68.750)	
10-10-22 13:40:Test: [50/79]	Time: 0.0162(0.0231)	Loss: 1.066(0.859)	Prec@1: 61.719(71.017)	
10-10-22 13:40:Test: [78/79]	Time: 0.0148(0.0207)	Loss: 0.622(0.863)	Prec@1: 81.250(70.890)	
10-10-22 13:40:Step 32 * Prec@1 70.890
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [0/391]	Time 0.380 (0.380)	Data 0.337 (0.337)	Loss 1.693 (1.693)	Prec@1 39.844 (39.844)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [50/391]	Time 0.031 (0.039)	Data 0.001 (0.008)	Loss 0.735 (0.927)	Prec@1 74.219 (67.433)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [100/391]	Time 0.033 (0.037)	Data 0.001 (0.005)	Loss 0.865 (0.854)	Prec@1 64.062 (69.910)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [150/391]	Time 0.031 (0.047)	Data 0.001 (0.004)	Loss 0.721 (0.818)	Prec@1 77.344 (71.249)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [200/391]	Time 0.035 (0.044)	Data 0.001 (0.003)	Loss 0.644 (0.797)	Prec@1 75.000 (71.964)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [250/391]	Time 0.182 (0.044)	Data 0.006 (0.003)	Loss 0.812 (0.790)	Prec@1 64.844 (72.261)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [300/391]	Time 0.109 (0.044)	Data 0.004 (0.003)	Loss 0.664 (0.781)	Prec@1 74.219 (72.545)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [350/391]	Time 0.055 (0.043)	Data 0.002 (0.003)	Loss 0.680 (0.775)	Prec@1 78.906 (72.839)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Test: [0/79]	Time: 0.3014(0.3014)	Loss: 0.668(0.668)	Prec@1: 77.344(77.344)	
10-10-22 13:40:Test: [50/79]	Time: 0.0184(0.0236)	Loss: 1.031(0.823)	Prec@1: 60.938(72.641)	
10-10-22 13:40:Test: [78/79]	Time: 0.0172(0.0219)	Loss: 0.373(0.822)	Prec@1: 81.250(72.340)	
10-10-22 13:40:Step 33 * Prec@1 72.340
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [0/391]	Time 0.439 (0.439)	Data 0.398 (0.398)	Loss 1.464 (1.464)	Prec@1 46.094 (46.094)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [50/391]	Time 0.039 (0.049)	Data 0.002 (0.009)	Loss 0.864 (0.901)	Prec@1 67.188 (67.724)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [100/391]	Time 0.033 (0.043)	Data 0.001 (0.006)	Loss 0.752 (0.839)	Prec@1 74.219 (70.204)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [150/391]	Time 0.031 (0.048)	Data 0.001 (0.005)	Loss 0.824 (0.812)	Prec@1 76.562 (71.378)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [200/391]	Time 0.037 (0.046)	Data 0.002 (0.004)	Loss 0.705 (0.794)	Prec@1 76.562 (72.081)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [250/391]	Time 0.035 (0.044)	Data 0.001 (0.003)	Loss 0.816 (0.783)	Prec@1 70.312 (72.395)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [300/391]	Time 0.037 (0.044)	Data 0.002 (0.003)	Loss 0.653 (0.771)	Prec@1 77.344 (72.789)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [350/391]	Time 0.034 (0.044)	Data 0.002 (0.003)	Loss 0.896 (0.764)	Prec@1 67.188 (73.046)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:40:Test: [0/79]	Time: 0.3907(0.3907)	Loss: 0.674(0.674)	Prec@1: 74.219(74.219)	
10-10-22 13:40:Test: [50/79]	Time: 0.0184(0.0300)	Loss: 1.035(0.804)	Prec@1: 60.938(72.763)	
10-10-22 13:40:Test: [78/79]	Time: 0.0149(0.0254)	Loss: 0.429(0.803)	Prec@1: 87.500(72.780)	
10-10-22 13:40:Step 34 * Prec@1 72.780
10-10-22 13:40:Num bit 8	Num grad bit 8	
10-10-22 13:40:Iter: [0/391]	Time 0.374 (0.374)	Data 0.332 (0.332)	Loss 1.637 (1.637)	Prec@1 45.312 (45.312)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [50/391]	Time 0.035 (0.043)	Data 0.001 (0.008)	Loss 0.726 (0.919)	Prec@1 70.312 (67.463)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [100/391]	Time 0.037 (0.040)	Data 0.002 (0.005)	Loss 0.690 (0.825)	Prec@1 78.906 (70.823)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [150/391]	Time 0.036 (0.040)	Data 0.002 (0.004)	Loss 0.785 (0.797)	Prec@1 71.875 (71.818)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [200/391]	Time 0.126 (0.043)	Data 0.007 (0.003)	Loss 0.767 (0.779)	Prec@1 77.344 (72.392)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [250/391]	Time 0.037 (0.042)	Data 0.002 (0.003)	Loss 0.720 (0.766)	Prec@1 74.219 (72.827)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [300/391]	Time 0.036 (0.041)	Data 0.001 (0.003)	Loss 0.596 (0.761)	Prec@1 78.906 (73.051)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [350/391]	Time 0.038 (0.042)	Data 0.002 (0.003)	Loss 0.691 (0.756)	Prec@1 72.656 (73.273)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Test: [0/79]	Time: 0.4495(0.4495)	Loss: 0.679(0.679)	Prec@1: 72.656(72.656)	
10-10-22 13:41:Test: [50/79]	Time: 0.0165(0.0314)	Loss: 1.015(0.819)	Prec@1: 65.625(72.044)	
10-10-22 13:41:Test: [78/79]	Time: 0.0186(0.0272)	Loss: 0.392(0.818)	Prec@1: 81.250(72.340)	
10-10-22 13:41:Step 35 * Prec@1 72.340
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [0/391]	Time 0.310 (0.310)	Data 0.271 (0.271)	Loss 1.400 (1.400)	Prec@1 47.656 (47.656)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [50/391]	Time 0.036 (0.049)	Data 0.002 (0.007)	Loss 0.790 (0.905)	Prec@1 73.438 (68.168)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [100/391]	Time 0.035 (0.044)	Data 0.001 (0.004)	Loss 0.721 (0.835)	Prec@1 71.875 (70.668)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [150/391]	Time 0.110 (0.048)	Data 0.005 (0.004)	Loss 0.558 (0.797)	Prec@1 81.250 (72.015)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [200/391]	Time 0.037 (0.046)	Data 0.002 (0.003)	Loss 0.680 (0.774)	Prec@1 75.000 (72.940)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [250/391]	Time 0.038 (0.044)	Data 0.002 (0.003)	Loss 0.683 (0.761)	Prec@1 75.000 (73.332)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [300/391]	Time 0.036 (0.044)	Data 0.001 (0.003)	Loss 0.769 (0.751)	Prec@1 70.312 (73.572)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [350/391]	Time 0.037 (0.044)	Data 0.002 (0.003)	Loss 0.833 (0.744)	Prec@1 70.312 (73.851)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Test: [0/79]	Time: 0.3634(0.3634)	Loss: 0.714(0.714)	Prec@1: 75.000(75.000)	
10-10-22 13:41:Test: [50/79]	Time: 0.0162(0.0270)	Loss: 0.967(0.782)	Prec@1: 65.625(73.775)	
10-10-22 13:41:Test: [78/79]	Time: 0.0149(0.0232)	Loss: 0.424(0.781)	Prec@1: 81.250(73.480)	
10-10-22 13:41:Step 36 * Prec@1 73.480
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [0/391]	Time 0.420 (0.420)	Data 0.384 (0.384)	Loss 1.681 (1.681)	Prec@1 45.312 (45.312)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [50/391]	Time 0.034 (0.045)	Data 0.001 (0.009)	Loss 0.762 (0.907)	Prec@1 74.219 (67.570)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [100/391]	Time 0.035 (0.040)	Data 0.001 (0.005)	Loss 0.583 (0.826)	Prec@1 80.469 (70.924)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [150/391]	Time 0.035 (0.039)	Data 0.001 (0.004)	Loss 0.888 (0.786)	Prec@1 70.312 (72.248)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [200/391]	Time 0.033 (0.043)	Data 0.002 (0.004)	Loss 0.688 (0.768)	Prec@1 79.688 (72.913)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [250/391]	Time 0.038 (0.044)	Data 0.001 (0.003)	Loss 0.614 (0.756)	Prec@1 75.781 (73.207)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [300/391]	Time 0.037 (0.043)	Data 0.002 (0.003)	Loss 0.613 (0.746)	Prec@1 74.219 (73.588)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [350/391]	Time 0.088 (0.042)	Data 0.005 (0.003)	Loss 0.573 (0.739)	Prec@1 78.125 (73.903)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Test: [0/79]	Time: 0.3695(0.3695)	Loss: 0.759(0.759)	Prec@1: 72.656(72.656)	
10-10-22 13:41:Test: [50/79]	Time: 0.0164(0.0236)	Loss: 0.920(0.807)	Prec@1: 65.625(73.208)	
10-10-22 13:41:Test: [78/79]	Time: 0.0147(0.0210)	Loss: 0.327(0.805)	Prec@1: 93.750(72.760)	
10-10-22 13:41:Step 37 * Prec@1 72.760
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [0/391]	Time 0.378 (0.378)	Data 0.344 (0.344)	Loss 1.755 (1.755)	Prec@1 41.406 (41.406)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:41:Num bit 8	Num grad bit 8	
10-10-22 13:41:Iter: [50/391]	Time 0.040 (0.041)	Data 0.001 (0.008)	Loss 0.872 (0.890)	Prec@1 66.406 (69.026)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [100/391]	Time 0.034 (0.047)	Data 0.002 (0.005)	Loss 0.626 (0.792)	Prec@1 77.344 (72.401)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [150/391]	Time 0.039 (0.050)	Data 0.001 (0.004)	Loss 0.644 (0.756)	Prec@1 76.562 (73.484)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [200/391]	Time 0.043 (0.049)	Data 0.001 (0.003)	Loss 0.834 (0.742)	Prec@1 67.969 (73.884)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [250/391]	Time 0.038 (0.047)	Data 0.002 (0.003)	Loss 0.625 (0.733)	Prec@1 81.250 (74.035)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [300/391]	Time 0.034 (0.046)	Data 0.002 (0.003)	Loss 0.725 (0.729)	Prec@1 76.562 (74.240)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [350/391]	Time 0.040 (0.046)	Data 0.001 (0.003)	Loss 0.662 (0.726)	Prec@1 78.906 (74.390)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Test: [0/79]	Time: 0.3774(0.3774)	Loss: 0.745(0.745)	Prec@1: 75.781(75.781)	
10-10-22 13:42:Test: [50/79]	Time: 0.0169(0.0237)	Loss: 1.029(0.787)	Prec@1: 68.750(73.575)	
10-10-22 13:42:Test: [78/79]	Time: 0.0148(0.0210)	Loss: 0.435(0.782)	Prec@1: 75.000(73.680)	
10-10-22 13:42:Step 38 * Prec@1 73.680
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [0/391]	Time 0.436 (0.436)	Data 0.348 (0.348)	Loss 1.488 (1.488)	Prec@1 53.906 (53.906)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [50/391]	Time 0.043 (0.063)	Data 0.002 (0.009)	Loss 0.912 (0.869)	Prec@1 69.531 (69.194)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [100/391]	Time 0.048 (0.054)	Data 0.004 (0.006)	Loss 0.658 (0.807)	Prec@1 76.562 (71.890)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [150/391]	Time 0.032 (0.050)	Data 0.001 (0.004)	Loss 0.602 (0.764)	Prec@1 78.125 (73.184)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [200/391]	Time 0.062 (0.047)	Data 0.001 (0.004)	Loss 0.605 (0.741)	Prec@1 78.125 (73.923)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [250/391]	Time 0.043 (0.048)	Data 0.001 (0.003)	Loss 0.782 (0.730)	Prec@1 69.531 (74.359)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [300/391]	Time 0.034 (0.047)	Data 0.001 (0.003)	Loss 0.663 (0.722)	Prec@1 75.000 (74.582)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [350/391]	Time 0.035 (0.045)	Data 0.001 (0.003)	Loss 0.814 (0.719)	Prec@1 68.750 (74.748)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Test: [0/79]	Time: 0.4054(0.4054)	Loss: 0.671(0.671)	Prec@1: 78.125(78.125)	
10-10-22 13:42:Test: [50/79]	Time: 0.0165(0.0246)	Loss: 0.992(0.779)	Prec@1: 69.531(73.836)	
10-10-22 13:42:Test: [78/79]	Time: 0.0153(0.0226)	Loss: 0.328(0.772)	Prec@1: 81.250(73.780)	
10-10-22 13:42:Step 39 * Prec@1 73.780
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [0/391]	Time 0.814 (0.814)	Data 0.777 (0.777)	Loss 1.534 (1.534)	Prec@1 46.875 (46.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [50/391]	Time 0.039 (0.057)	Data 0.002 (0.017)	Loss 0.814 (0.859)	Prec@1 70.312 (69.761)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [100/391]	Time 0.047 (0.051)	Data 0.002 (0.010)	Loss 0.720 (0.786)	Prec@1 74.219 (72.471)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [150/391]	Time 0.053 (0.052)	Data 0.002 (0.007)	Loss 0.612 (0.754)	Prec@1 79.688 (73.582)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [200/391]	Time 0.060 (0.054)	Data 0.004 (0.006)	Loss 0.585 (0.735)	Prec@1 78.125 (74.192)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [250/391]	Time 0.032 (0.054)	Data 0.001 (0.005)	Loss 0.699 (0.728)	Prec@1 74.219 (74.368)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [300/391]	Time 0.043 (0.052)	Data 0.002 (0.005)	Loss 0.668 (0.715)	Prec@1 75.000 (74.821)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [350/391]	Time 0.033 (0.051)	Data 0.001 (0.004)	Loss 0.681 (0.710)	Prec@1 73.438 (75.085)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:42:Test: [0/79]	Time: 0.3491(0.3491)	Loss: 0.694(0.694)	Prec@1: 76.562(76.562)	
10-10-22 13:42:Test: [50/79]	Time: 0.0169(0.0269)	Loss: 0.960(0.784)	Prec@1: 67.969(73.851)	
10-10-22 13:42:Test: [78/79]	Time: 0.0210(0.0255)	Loss: 0.283(0.783)	Prec@1: 87.500(73.420)	
10-10-22 13:42:Step 40 * Prec@1 73.420
10-10-22 13:42:Num bit 8	Num grad bit 8	
10-10-22 13:42:Iter: [0/391]	Time 0.442 (0.442)	Data 0.402 (0.402)	Loss 1.348 (1.348)	Prec@1 48.438 (48.438)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [50/391]	Time 0.055 (0.059)	Data 0.002 (0.010)	Loss 0.602 (0.870)	Prec@1 79.688 (69.026)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [100/391]	Time 0.040 (0.049)	Data 0.001 (0.006)	Loss 0.592 (0.776)	Prec@1 75.781 (72.115)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [150/391]	Time 0.035 (0.044)	Data 0.001 (0.004)	Loss 0.674 (0.749)	Prec@1 78.906 (73.417)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [200/391]	Time 0.035 (0.042)	Data 0.001 (0.003)	Loss 0.644 (0.728)	Prec@1 78.125 (74.269)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [250/391]	Time 0.035 (0.047)	Data 0.002 (0.003)	Loss 0.640 (0.713)	Prec@1 77.344 (74.838)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [300/391]	Time 0.035 (0.045)	Data 0.002 (0.003)	Loss 0.621 (0.706)	Prec@1 78.906 (75.052)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [350/391]	Time 0.036 (0.044)	Data 0.002 (0.003)	Loss 0.679 (0.697)	Prec@1 76.562 (75.456)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Test: [0/79]	Time: 0.3483(0.3483)	Loss: 0.721(0.721)	Prec@1: 75.000(75.000)	
10-10-22 13:43:Test: [50/79]	Time: 0.0161(0.0229)	Loss: 0.981(0.788)	Prec@1: 66.406(73.928)	
10-10-22 13:43:Test: [78/79]	Time: 0.0165(0.0214)	Loss: 0.248(0.785)	Prec@1: 100.000(74.000)	
10-10-22 13:43:Step 41 * Prec@1 74.000
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [0/391]	Time 0.431 (0.431)	Data 0.391 (0.391)	Loss 1.686 (1.686)	Prec@1 44.531 (44.531)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [50/391]	Time 0.038 (0.047)	Data 0.002 (0.009)	Loss 0.737 (0.865)	Prec@1 78.125 (69.347)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [100/391]	Time 0.031 (0.046)	Data 0.001 (0.006)	Loss 0.615 (0.776)	Prec@1 77.344 (72.563)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [150/391]	Time 0.035 (0.042)	Data 0.001 (0.004)	Loss 0.783 (0.748)	Prec@1 72.656 (73.670)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [200/391]	Time 0.035 (0.040)	Data 0.001 (0.004)	Loss 0.665 (0.725)	Prec@1 75.781 (74.382)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [250/391]	Time 0.048 (0.044)	Data 0.002 (0.003)	Loss 0.720 (0.712)	Prec@1 74.219 (74.816)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [300/391]	Time 0.036 (0.043)	Data 0.002 (0.003)	Loss 0.626 (0.703)	Prec@1 81.250 (75.283)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [350/391]	Time 0.032 (0.043)	Data 0.001 (0.003)	Loss 0.638 (0.697)	Prec@1 79.688 (75.438)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Test: [0/79]	Time: 0.4779(0.4779)	Loss: 0.657(0.657)	Prec@1: 72.656(72.656)	
10-10-22 13:43:Test: [50/79]	Time: 0.0189(0.0282)	Loss: 0.915(0.767)	Prec@1: 71.875(74.479)	
10-10-22 13:43:Test: [78/79]	Time: 0.0151(0.0245)	Loss: 0.358(0.764)	Prec@1: 87.500(74.500)	
10-10-22 13:43:Step 42 * Prec@1 74.500
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [0/391]	Time 0.539 (0.539)	Data 0.495 (0.495)	Loss 1.592 (1.592)	Prec@1 45.312 (45.312)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [50/391]	Time 0.041 (0.055)	Data 0.002 (0.012)	Loss 0.687 (0.829)	Prec@1 75.781 (70.680)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [100/391]	Time 0.036 (0.049)	Data 0.001 (0.007)	Loss 0.803 (0.750)	Prec@1 75.000 (73.530)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [150/391]	Time 0.179 (0.049)	Data 0.009 (0.005)	Loss 0.601 (0.717)	Prec@1 78.906 (74.684)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [200/391]	Time 0.035 (0.051)	Data 0.001 (0.005)	Loss 0.663 (0.704)	Prec@1 78.125 (75.086)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [250/391]	Time 0.041 (0.049)	Data 0.002 (0.004)	Loss 0.597 (0.697)	Prec@1 78.906 (75.398)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [300/391]	Time 0.037 (0.048)	Data 0.002 (0.004)	Loss 0.565 (0.690)	Prec@1 80.469 (75.638)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [350/391]	Time 0.037 (0.047)	Data 0.002 (0.003)	Loss 0.605 (0.685)	Prec@1 77.344 (75.944)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Test: [0/79]	Time: 0.3464(0.3464)	Loss: 0.741(0.741)	Prec@1: 73.438(73.438)	
10-10-22 13:43:Test: [50/79]	Time: 0.0225(0.0273)	Loss: 0.954(0.751)	Prec@1: 72.656(74.985)	
10-10-22 13:43:Test: [78/79]	Time: 0.0152(0.0238)	Loss: 0.458(0.752)	Prec@1: 75.000(74.860)	
10-10-22 13:43:Step 43 * Prec@1 74.860
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [0/391]	Time 0.329 (0.329)	Data 0.295 (0.295)	Loss 1.367 (1.367)	Prec@1 50.781 (50.781)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:43:Num bit 8	Num grad bit 8	
10-10-22 13:43:Iter: [50/391]	Time 0.042 (0.040)	Data 0.001 (0.007)	Loss 0.839 (0.813)	Prec@1 73.438 (71.783)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [100/391]	Time 0.134 (0.043)	Data 0.005 (0.004)	Loss 0.620 (0.762)	Prec@1 77.344 (73.368)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [150/391]	Time 0.032 (0.050)	Data 0.001 (0.004)	Loss 0.641 (0.724)	Prec@1 75.000 (74.783)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [200/391]	Time 0.032 (0.045)	Data 0.001 (0.003)	Loss 0.627 (0.707)	Prec@1 76.562 (75.303)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [250/391]	Time 0.034 (0.043)	Data 0.001 (0.003)	Loss 0.506 (0.700)	Prec@1 83.594 (75.458)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [300/391]	Time 0.036 (0.043)	Data 0.001 (0.003)	Loss 0.677 (0.690)	Prec@1 76.562 (75.753)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [350/391]	Time 0.081 (0.043)	Data 0.002 (0.003)	Loss 0.522 (0.682)	Prec@1 80.469 (76.088)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Test: [0/79]	Time: 0.3188(0.3188)	Loss: 0.680(0.680)	Prec@1: 72.656(72.656)	
10-10-22 13:44:Test: [50/79]	Time: 0.0162(0.0224)	Loss: 0.952(0.794)	Prec@1: 71.875(73.912)	
10-10-22 13:44:Test: [78/79]	Time: 0.0161(0.0204)	Loss: 0.420(0.794)	Prec@1: 81.250(73.630)	
10-10-22 13:44:Step 44 * Prec@1 73.630
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [0/391]	Time 0.369 (0.369)	Data 0.333 (0.333)	Loss 1.587 (1.587)	Prec@1 49.219 (49.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [50/391]	Time 0.041 (0.047)	Data 0.001 (0.008)	Loss 0.534 (0.826)	Prec@1 81.250 (71.063)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [100/391]	Time 0.035 (0.051)	Data 0.002 (0.005)	Loss 0.622 (0.763)	Prec@1 79.688 (72.997)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [150/391]	Time 0.035 (0.047)	Data 0.001 (0.004)	Loss 0.640 (0.728)	Prec@1 75.781 (74.343)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [200/391]	Time 0.136 (0.045)	Data 0.004 (0.003)	Loss 0.731 (0.704)	Prec@1 72.656 (75.093)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [250/391]	Time 0.036 (0.045)	Data 0.002 (0.003)	Loss 0.510 (0.693)	Prec@1 84.375 (75.458)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [300/391]	Time 0.048 (0.044)	Data 0.002 (0.003)	Loss 0.648 (0.686)	Prec@1 73.438 (75.724)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [350/391]	Time 0.033 (0.045)	Data 0.002 (0.003)	Loss 0.486 (0.680)	Prec@1 81.250 (75.970)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Test: [0/79]	Time: 0.3744(0.3744)	Loss: 0.671(0.671)	Prec@1: 75.000(75.000)	
10-10-22 13:44:Test: [50/79]	Time: 0.0182(0.0253)	Loss: 0.899(0.786)	Prec@1: 73.438(73.453)	
10-10-22 13:44:Test: [78/79]	Time: 0.0149(0.0224)	Loss: 0.427(0.787)	Prec@1: 75.000(73.210)	
10-10-22 13:44:Step 45 * Prec@1 73.210
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [0/391]	Time 0.344 (0.344)	Data 0.303 (0.303)	Loss 1.429 (1.429)	Prec@1 51.562 (51.562)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [50/391]	Time 0.042 (0.066)	Data 0.002 (0.009)	Loss 0.757 (0.807)	Prec@1 73.438 (71.737)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [100/391]	Time 0.037 (0.052)	Data 0.002 (0.005)	Loss 0.617 (0.738)	Prec@1 77.344 (74.072)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [150/391]	Time 0.041 (0.050)	Data 0.002 (0.004)	Loss 0.751 (0.713)	Prec@1 71.875 (74.943)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [200/391]	Time 0.039 (0.048)	Data 0.003 (0.004)	Loss 0.490 (0.701)	Prec@1 82.031 (75.420)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [250/391]	Time 0.042 (0.045)	Data 0.002 (0.003)	Loss 0.670 (0.688)	Prec@1 77.344 (75.912)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [300/391]	Time 0.037 (0.045)	Data 0.002 (0.003)	Loss 0.765 (0.683)	Prec@1 73.438 (76.030)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [350/391]	Time 0.036 (0.046)	Data 0.003 (0.003)	Loss 0.583 (0.677)	Prec@1 77.344 (76.313)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Test: [0/79]	Time: 0.3366(0.3366)	Loss: 0.681(0.681)	Prec@1: 76.562(76.562)	
10-10-22 13:44:Test: [50/79]	Time: 0.0168(0.0280)	Loss: 0.877(0.758)	Prec@1: 75.000(75.276)	
10-10-22 13:44:Test: [78/79]	Time: 0.0152(0.0241)	Loss: 0.397(0.752)	Prec@1: 81.250(75.270)	
10-10-22 13:44:Step 46 * Prec@1 75.270
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [0/391]	Time 0.381 (0.381)	Data 0.341 (0.341)	Loss 1.835 (1.835)	Prec@1 39.844 (39.844)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [50/391]	Time 0.040 (0.061)	Data 0.002 (0.009)	Loss 0.943 (0.836)	Prec@1 65.625 (70.604)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:44:Num bit 8	Num grad bit 8	
10-10-22 13:44:Iter: [100/391]	Time 0.035 (0.049)	Data 0.001 (0.005)	Loss 0.571 (0.761)	Prec@1 84.375 (73.345)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [150/391]	Time 0.038 (0.047)	Data 0.002 (0.004)	Loss 0.517 (0.732)	Prec@1 82.031 (74.260)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [200/391]	Time 0.038 (0.045)	Data 0.002 (0.004)	Loss 0.542 (0.708)	Prec@1 81.250 (75.225)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [250/391]	Time 0.039 (0.044)	Data 0.001 (0.003)	Loss 0.698 (0.699)	Prec@1 78.906 (75.442)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [300/391]	Time 0.064 (0.045)	Data 0.002 (0.003)	Loss 0.762 (0.690)	Prec@1 74.219 (75.789)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [350/391]	Time 0.039 (0.044)	Data 0.001 (0.003)	Loss 0.600 (0.683)	Prec@1 79.688 (76.124)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Test: [0/79]	Time: 1.4464(1.4464)	Loss: 0.747(0.747)	Prec@1: 72.656(72.656)	
10-10-22 13:45:Test: [50/79]	Time: 0.0164(0.0474)	Loss: 0.948(0.801)	Prec@1: 66.406(73.453)	
10-10-22 13:45:Test: [78/79]	Time: 0.0151(0.0375)	Loss: 0.471(0.793)	Prec@1: 81.250(73.530)	
10-10-22 13:45:Step 47 * Prec@1 73.530
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [0/391]	Time 0.427 (0.427)	Data 0.386 (0.386)	Loss 1.464 (1.464)	Prec@1 51.562 (51.562)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [50/391]	Time 0.036 (0.051)	Data 0.002 (0.009)	Loss 0.633 (0.812)	Prec@1 80.469 (71.002)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [100/391]	Time 0.036 (0.045)	Data 0.001 (0.006)	Loss 0.633 (0.753)	Prec@1 75.000 (73.600)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [150/391]	Time 0.033 (0.047)	Data 0.001 (0.005)	Loss 0.508 (0.723)	Prec@1 84.375 (74.715)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [200/391]	Time 0.032 (0.044)	Data 0.001 (0.004)	Loss 0.583 (0.702)	Prec@1 78.906 (75.319)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [250/391]	Time 0.035 (0.043)	Data 0.001 (0.003)	Loss 0.704 (0.690)	Prec@1 75.000 (75.679)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [300/391]	Time 0.035 (0.041)	Data 0.001 (0.003)	Loss 0.677 (0.681)	Prec@1 70.312 (75.950)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [350/391]	Time 0.039 (0.043)	Data 0.003 (0.003)	Loss 0.612 (0.673)	Prec@1 75.781 (76.289)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Test: [0/79]	Time: 0.4094(0.4094)	Loss: 0.640(0.640)	Prec@1: 75.781(75.781)	
10-10-22 13:45:Test: [50/79]	Time: 0.0187(0.0279)	Loss: 0.856(0.766)	Prec@1: 73.438(74.280)	
10-10-22 13:45:Test: [78/79]	Time: 0.0175(0.0248)	Loss: 0.409(0.763)	Prec@1: 87.500(74.570)	
10-10-22 13:45:Step 48 * Prec@1 74.570
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [0/391]	Time 0.427 (0.427)	Data 0.386 (0.386)	Loss 1.595 (1.595)	Prec@1 46.875 (46.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [50/391]	Time 0.038 (0.052)	Data 0.002 (0.010)	Loss 0.679 (0.800)	Prec@1 74.219 (72.273)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [100/391]	Time 0.038 (0.046)	Data 0.002 (0.006)	Loss 0.776 (0.723)	Prec@1 75.000 (74.938)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [150/391]	Time 0.039 (0.043)	Data 0.002 (0.005)	Loss 0.597 (0.705)	Prec@1 78.906 (75.678)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [200/391]	Time 0.042 (0.045)	Data 0.001 (0.004)	Loss 0.729 (0.689)	Prec@1 78.125 (76.185)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [250/391]	Time 0.035 (0.043)	Data 0.001 (0.003)	Loss 0.667 (0.676)	Prec@1 75.781 (76.653)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [300/391]	Time 0.036 (0.042)	Data 0.001 (0.003)	Loss 0.635 (0.668)	Prec@1 75.781 (76.890)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [350/391]	Time 0.034 (0.041)	Data 0.001 (0.003)	Loss 0.494 (0.662)	Prec@1 84.375 (77.039)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Test: [0/79]	Time: 1.5801(1.5801)	Loss: 0.661(0.661)	Prec@1: 75.781(75.781)	
10-10-22 13:45:Test: [50/79]	Time: 0.0192(0.0507)	Loss: 0.948(0.768)	Prec@1: 69.531(74.831)	
10-10-22 13:45:Test: [78/79]	Time: 0.0175(0.0395)	Loss: 0.492(0.764)	Prec@1: 81.250(74.760)	
10-10-22 13:45:Step 49 * Prec@1 74.760
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [0/391]	Time 0.451 (0.451)	Data 0.411 (0.411)	Loss 1.424 (1.424)	Prec@1 46.094 (46.094)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [50/391]	Time 0.112 (0.045)	Data 0.001 (0.010)	Loss 0.532 (0.792)	Prec@1 79.688 (72.381)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [100/391]	Time 0.036 (0.045)	Data 0.002 (0.006)	Loss 0.721 (0.725)	Prec@1 75.000 (74.350)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:45:Num bit 8	Num grad bit 8	
10-10-22 13:45:Iter: [150/391]	Time 0.036 (0.043)	Data 0.002 (0.005)	Loss 0.626 (0.694)	Prec@1 82.031 (75.683)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [200/391]	Time 0.032 (0.045)	Data 0.001 (0.004)	Loss 0.846 (0.678)	Prec@1 71.094 (76.294)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [250/391]	Time 0.037 (0.045)	Data 0.002 (0.004)	Loss 0.476 (0.665)	Prec@1 82.812 (76.771)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [300/391]	Time 0.034 (0.043)	Data 0.001 (0.003)	Loss 0.619 (0.659)	Prec@1 80.469 (77.040)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [350/391]	Time 0.043 (0.042)	Data 0.001 (0.003)	Loss 0.730 (0.654)	Prec@1 75.000 (77.239)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Test: [0/79]	Time: 0.4067(0.4067)	Loss: 0.605(0.605)	Prec@1: 78.125(78.125)	
10-10-22 13:46:Test: [50/79]	Time: 0.0804(0.0384)	Loss: 0.919(0.766)	Prec@1: 71.875(74.494)	
10-10-22 13:46:Test: [78/79]	Time: 0.0186(0.0340)	Loss: 0.572(0.760)	Prec@1: 75.000(74.900)	
10-10-22 13:46:Step 50 * Prec@1 74.900
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [0/391]	Time 0.367 (0.367)	Data 0.331 (0.331)	Loss 1.744 (1.744)	Prec@1 42.188 (42.188)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [50/391]	Time 0.072 (0.046)	Data 0.001 (0.008)	Loss 0.766 (0.792)	Prec@1 74.219 (72.411)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [100/391]	Time 0.036 (0.045)	Data 0.002 (0.005)	Loss 0.538 (0.721)	Prec@1 82.812 (75.046)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [150/391]	Time 0.038 (0.043)	Data 0.002 (0.004)	Loss 0.648 (0.687)	Prec@1 78.906 (76.118)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [200/391]	Time 0.035 (0.042)	Data 0.002 (0.003)	Loss 0.707 (0.668)	Prec@1 75.000 (76.803)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [250/391]	Time 0.041 (0.043)	Data 0.001 (0.003)	Loss 0.693 (0.660)	Prec@1 78.125 (77.011)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [300/391]	Time 0.041 (0.042)	Data 0.001 (0.003)	Loss 0.582 (0.653)	Prec@1 76.562 (77.227)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [350/391]	Time 0.038 (0.041)	Data 0.001 (0.003)	Loss 0.613 (0.647)	Prec@1 79.688 (77.426)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Test: [0/79]	Time: 0.3512(0.3512)	Loss: 0.632(0.632)	Prec@1: 77.344(77.344)	
10-10-22 13:46:Test: [50/79]	Time: 0.0159(0.0243)	Loss: 0.951(0.787)	Prec@1: 70.312(73.882)	
10-10-22 13:46:Test: [78/79]	Time: 0.0150(0.0214)	Loss: 0.360(0.780)	Prec@1: 87.500(74.090)	
10-10-22 13:46:Step 51 * Prec@1 74.090
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [0/391]	Time 0.677 (0.677)	Data 0.613 (0.613)	Loss 1.649 (1.649)	Prec@1 46.094 (46.094)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [50/391]	Time 0.056 (0.064)	Data 0.002 (0.014)	Loss 0.775 (0.786)	Prec@1 69.531 (72.381)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [100/391]	Time 0.041 (0.055)	Data 0.002 (0.008)	Loss 0.631 (0.710)	Prec@1 80.469 (75.116)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [150/391]	Time 0.042 (0.051)	Data 0.002 (0.006)	Loss 0.520 (0.688)	Prec@1 78.906 (75.849)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [200/391]	Time 0.035 (0.047)	Data 0.001 (0.005)	Loss 0.579 (0.671)	Prec@1 79.688 (76.419)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [250/391]	Time 0.032 (0.049)	Data 0.001 (0.004)	Loss 0.646 (0.655)	Prec@1 76.562 (77.029)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [300/391]	Time 0.035 (0.047)	Data 0.001 (0.004)	Loss 0.531 (0.644)	Prec@1 79.688 (77.396)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [350/391]	Time 0.036 (0.047)	Data 0.002 (0.004)	Loss 0.478 (0.639)	Prec@1 85.156 (77.655)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Test: [0/79]	Time: 0.3951(0.3951)	Loss: 0.636(0.636)	Prec@1: 75.781(75.781)	
10-10-22 13:46:Test: [50/79]	Time: 0.0184(0.0260)	Loss: 0.908(0.786)	Prec@1: 69.531(74.479)	
10-10-22 13:46:Test: [78/79]	Time: 0.0152(0.0233)	Loss: 0.577(0.782)	Prec@1: 75.000(74.610)	
10-10-22 13:46:Step 52 * Prec@1 74.610
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [0/391]	Time 0.406 (0.406)	Data 0.369 (0.369)	Loss 1.779 (1.779)	Prec@1 42.969 (42.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [50/391]	Time 0.048 (0.048)	Data 0.006 (0.009)	Loss 0.719 (0.801)	Prec@1 75.781 (72.748)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [100/391]	Time 0.034 (0.043)	Data 0.001 (0.005)	Loss 0.613 (0.718)	Prec@1 79.688 (75.294)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:46:Num bit 8	Num grad bit 8	
10-10-22 13:46:Iter: [150/391]	Time 0.116 (0.049)	Data 0.007 (0.004)	Loss 0.588 (0.691)	Prec@1 77.344 (75.999)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [200/391]	Time 0.037 (0.047)	Data 0.001 (0.004)	Loss 0.772 (0.671)	Prec@1 71.094 (76.597)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [250/391]	Time 0.043 (0.045)	Data 0.001 (0.003)	Loss 0.659 (0.654)	Prec@1 73.438 (77.204)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [300/391]	Time 0.034 (0.045)	Data 0.001 (0.003)	Loss 0.563 (0.647)	Prec@1 76.562 (77.435)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [350/391]	Time 0.036 (0.044)	Data 0.002 (0.003)	Loss 0.711 (0.642)	Prec@1 73.438 (77.557)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Test: [0/79]	Time: 0.3634(0.3634)	Loss: 0.663(0.663)	Prec@1: 75.000(75.000)	
10-10-22 13:47:Test: [50/79]	Time: 0.0164(0.0302)	Loss: 0.872(0.760)	Prec@1: 72.656(74.954)	
10-10-22 13:47:Test: [78/79]	Time: 0.0150(0.0263)	Loss: 0.417(0.754)	Prec@1: 81.250(75.190)	
10-10-22 13:47:Step 53 * Prec@1 75.190
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [0/391]	Time 0.334 (0.334)	Data 0.298 (0.298)	Loss 1.142 (1.142)	Prec@1 63.281 (63.281)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [50/391]	Time 0.036 (0.042)	Data 0.001 (0.007)	Loss 0.544 (0.761)	Prec@1 80.469 (73.805)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [100/391]	Time 0.034 (0.039)	Data 0.001 (0.004)	Loss 0.706 (0.697)	Prec@1 79.688 (75.627)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [150/391]	Time 0.054 (0.043)	Data 0.002 (0.004)	Loss 0.561 (0.672)	Prec@1 79.688 (76.469)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [200/391]	Time 0.032 (0.042)	Data 0.001 (0.003)	Loss 0.662 (0.652)	Prec@1 80.469 (77.181)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [250/391]	Time 0.039 (0.043)	Data 0.002 (0.003)	Loss 0.554 (0.643)	Prec@1 82.031 (77.509)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [300/391]	Time 0.036 (0.042)	Data 0.001 (0.003)	Loss 0.798 (0.637)	Prec@1 73.438 (77.795)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [350/391]	Time 0.036 (0.043)	Data 0.001 (0.003)	Loss 0.607 (0.632)	Prec@1 78.125 (77.989)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Test: [0/79]	Time: 0.3452(0.3452)	Loss: 0.683(0.683)	Prec@1: 75.781(75.781)	
10-10-22 13:47:Test: [50/79]	Time: 0.0200(0.0264)	Loss: 0.889(0.741)	Prec@1: 71.094(75.720)	
10-10-22 13:47:Test: [78/79]	Time: 0.0185(0.0241)	Loss: 0.547(0.739)	Prec@1: 75.000(75.610)	
10-10-22 13:47:Step 54 * Prec@1 75.610
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [0/391]	Time 0.375 (0.375)	Data 0.329 (0.329)	Loss 1.499 (1.499)	Prec@1 45.312 (45.312)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [50/391]	Time 0.039 (0.057)	Data 0.001 (0.009)	Loss 0.692 (0.778)	Prec@1 72.656 (72.947)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [100/391]	Time 0.035 (0.048)	Data 0.001 (0.005)	Loss 0.715 (0.708)	Prec@1 73.438 (75.394)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [150/391]	Time 0.040 (0.048)	Data 0.003 (0.004)	Loss 0.490 (0.677)	Prec@1 82.812 (76.495)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [200/391]	Time 0.054 (0.045)	Data 0.003 (0.004)	Loss 0.524 (0.659)	Prec@1 85.156 (77.025)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [250/391]	Time 0.033 (0.046)	Data 0.003 (0.003)	Loss 0.647 (0.649)	Prec@1 77.344 (77.394)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [300/391]	Time 0.036 (0.044)	Data 0.001 (0.003)	Loss 0.689 (0.642)	Prec@1 73.438 (77.562)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [350/391]	Time 0.037 (0.043)	Data 0.001 (0.003)	Loss 0.652 (0.636)	Prec@1 76.562 (77.818)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Test: [0/79]	Time: 0.2958(0.2958)	Loss: 0.623(0.623)	Prec@1: 80.469(80.469)	
10-10-22 13:47:Test: [50/79]	Time: 0.0163(0.0231)	Loss: 0.938(0.740)	Prec@1: 69.531(76.042)	
10-10-22 13:47:Test: [78/79]	Time: 0.0149(0.0206)	Loss: 0.535(0.739)	Prec@1: 81.250(75.810)	
10-10-22 13:47:Step 55 * Prec@1 75.810
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [0/391]	Time 0.404 (0.404)	Data 0.351 (0.351)	Loss 1.462 (1.462)	Prec@1 50.781 (50.781)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [50/391]	Time 0.174 (0.049)	Data 0.002 (0.009)	Loss 0.692 (0.774)	Prec@1 70.312 (72.871)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [100/391]	Time 0.048 (0.047)	Data 0.002 (0.005)	Loss 0.610 (0.710)	Prec@1 83.594 (75.077)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [150/391]	Time 0.042 (0.045)	Data 0.002 (0.004)	Loss 0.614 (0.673)	Prec@1 76.562 (76.392)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [200/391]	Time 0.043 (0.045)	Data 0.002 (0.004)	Loss 0.527 (0.656)	Prec@1 83.594 (77.041)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:47:Num bit 8	Num grad bit 8	
10-10-22 13:47:Iter: [250/391]	Time 0.038 (0.045)	Data 0.001 (0.003)	Loss 0.569 (0.642)	Prec@1 80.469 (77.468)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [300/391]	Time 0.041 (0.043)	Data 0.002 (0.003)	Loss 0.648 (0.635)	Prec@1 78.906 (77.715)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [350/391]	Time 0.037 (0.045)	Data 0.002 (0.003)	Loss 0.525 (0.630)	Prec@1 85.156 (77.865)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Test: [0/79]	Time: 0.4209(0.4209)	Loss: 0.649(0.649)	Prec@1: 79.688(79.688)	
10-10-22 13:48:Test: [50/79]	Time: 0.0167(0.0252)	Loss: 0.965(0.742)	Prec@1: 68.750(75.230)	
10-10-22 13:48:Test: [78/79]	Time: 0.0176(0.0227)	Loss: 0.497(0.739)	Prec@1: 75.000(75.370)	
10-10-22 13:48:Step 56 * Prec@1 75.370
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [0/391]	Time 0.371 (0.371)	Data 0.332 (0.332)	Loss 1.729 (1.729)	Prec@1 43.750 (43.750)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [50/391]	Time 0.045 (0.047)	Data 0.002 (0.008)	Loss 0.617 (0.771)	Prec@1 73.438 (72.963)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [100/391]	Time 0.070 (0.049)	Data 0.003 (0.006)	Loss 0.527 (0.695)	Prec@1 83.594 (75.534)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [150/391]	Time 0.044 (0.046)	Data 0.001 (0.004)	Loss 0.654 (0.667)	Prec@1 82.031 (76.692)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [200/391]	Time 0.037 (0.044)	Data 0.002 (0.004)	Loss 0.645 (0.650)	Prec@1 75.000 (77.352)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [250/391]	Time 0.072 (0.045)	Data 0.005 (0.003)	Loss 0.484 (0.638)	Prec@1 80.469 (77.879)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [300/391]	Time 0.046 (0.045)	Data 0.002 (0.003)	Loss 0.721 (0.634)	Prec@1 75.000 (78.039)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [350/391]	Time 0.038 (0.045)	Data 0.001 (0.003)	Loss 0.529 (0.629)	Prec@1 82.031 (78.118)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Test: [0/79]	Time: 0.2792(0.2792)	Loss: 0.715(0.715)	Prec@1: 75.000(75.000)	
10-10-22 13:48:Test: [50/79]	Time: 0.0202(0.0256)	Loss: 1.026(0.756)	Prec@1: 70.312(75.061)	
10-10-22 13:48:Test: [78/79]	Time: 0.0190(0.0237)	Loss: 0.432(0.753)	Prec@1: 81.250(75.090)	
10-10-22 13:48:Step 57 * Prec@1 75.090
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [0/391]	Time 0.336 (0.336)	Data 0.284 (0.284)	Loss 2.064 (2.064)	Prec@1 32.812 (32.812)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [50/391]	Time 0.045 (0.052)	Data 0.002 (0.007)	Loss 0.749 (0.784)	Prec@1 76.562 (72.534)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [100/391]	Time 0.064 (0.048)	Data 0.002 (0.005)	Loss 0.664 (0.715)	Prec@1 76.562 (75.062)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [150/391]	Time 0.037 (0.044)	Data 0.001 (0.004)	Loss 0.534 (0.677)	Prec@1 82.031 (76.454)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [200/391]	Time 0.113 (0.044)	Data 0.004 (0.003)	Loss 0.658 (0.654)	Prec@1 77.344 (77.107)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [250/391]	Time 0.036 (0.046)	Data 0.001 (0.003)	Loss 0.546 (0.643)	Prec@1 82.812 (77.446)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [300/391]	Time 0.038 (0.046)	Data 0.002 (0.003)	Loss 0.602 (0.638)	Prec@1 80.469 (77.668)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [350/391]	Time 0.082 (0.046)	Data 0.003 (0.003)	Loss 0.658 (0.633)	Prec@1 76.562 (77.853)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Test: [0/79]	Time: 0.2794(0.2794)	Loss: 0.657(0.657)	Prec@1: 75.781(75.781)	
10-10-22 13:48:Test: [50/79]	Time: 0.0163(0.0219)	Loss: 0.984(0.758)	Prec@1: 68.750(74.893)	
10-10-22 13:48:Test: [78/79]	Time: 0.0150(0.0199)	Loss: 0.637(0.753)	Prec@1: 62.500(74.900)	
10-10-22 13:48:Step 58 * Prec@1 74.900
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [0/391]	Time 0.338 (0.338)	Data 0.301 (0.301)	Loss 1.633 (1.633)	Prec@1 53.125 (53.125)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [50/391]	Time 0.033 (0.039)	Data 0.001 (0.007)	Loss 0.735 (0.787)	Prec@1 72.656 (72.687)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [100/391]	Time 0.045 (0.046)	Data 0.004 (0.005)	Loss 0.649 (0.713)	Prec@1 79.688 (74.845)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [150/391]	Time 0.036 (0.044)	Data 0.001 (0.004)	Loss 0.495 (0.684)	Prec@1 82.812 (76.009)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [200/391]	Time 0.033 (0.045)	Data 0.001 (0.003)	Loss 0.550 (0.671)	Prec@1 79.688 (76.543)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:48:Num bit 8	Num grad bit 8	
10-10-22 13:48:Iter: [250/391]	Time 0.037 (0.044)	Data 0.002 (0.003)	Loss 0.662 (0.660)	Prec@1 73.438 (76.930)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [300/391]	Time 0.040 (0.045)	Data 0.001 (0.003)	Loss 0.569 (0.653)	Prec@1 78.906 (77.256)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [350/391]	Time 0.038 (0.045)	Data 0.001 (0.003)	Loss 0.590 (0.646)	Prec@1 78.125 (77.504)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Test: [0/79]	Time: 0.3108(0.3108)	Loss: 0.682(0.682)	Prec@1: 73.438(73.438)	
10-10-22 13:49:Test: [50/79]	Time: 0.0162(0.0222)	Loss: 0.928(0.767)	Prec@1: 68.750(74.127)	
10-10-22 13:49:Test: [78/79]	Time: 0.0147(0.0206)	Loss: 0.602(0.768)	Prec@1: 75.000(74.090)	
10-10-22 13:49:Step 59 * Prec@1 74.090
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [0/391]	Time 0.357 (0.357)	Data 0.311 (0.311)	Loss 1.656 (1.656)	Prec@1 49.219 (49.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [50/391]	Time 0.031 (0.042)	Data 0.001 (0.007)	Loss 0.792 (0.817)	Prec@1 75.781 (71.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [100/391]	Time 0.043 (0.040)	Data 0.001 (0.005)	Loss 0.603 (0.727)	Prec@1 75.781 (74.899)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [150/391]	Time 0.037 (0.041)	Data 0.002 (0.004)	Loss 0.600 (0.688)	Prec@1 78.125 (76.071)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [200/391]	Time 0.127 (0.040)	Data 0.002 (0.003)	Loss 0.593 (0.670)	Prec@1 81.250 (76.757)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [250/391]	Time 0.037 (0.041)	Data 0.002 (0.003)	Loss 0.636 (0.660)	Prec@1 76.562 (77.082)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [300/391]	Time 0.129 (0.041)	Data 0.003 (0.003)	Loss 0.633 (0.654)	Prec@1 82.031 (77.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [350/391]	Time 0.052 (0.042)	Data 0.002 (0.003)	Loss 0.707 (0.649)	Prec@1 67.188 (77.339)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Test: [0/79]	Time: 0.2928(0.2928)	Loss: 0.767(0.767)	Prec@1: 75.781(75.781)	
10-10-22 13:49:Test: [50/79]	Time: 0.0182(0.0238)	Loss: 1.141(0.888)	Prec@1: 62.500(71.109)	
10-10-22 13:49:Test: [78/79]	Time: 0.0170(0.0218)	Loss: 0.640(0.892)	Prec@1: 56.250(71.000)	
10-10-22 13:49:Step 60 * Prec@1 71.000
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [0/391]	Time 0.365 (0.365)	Data 0.325 (0.325)	Loss 1.770 (1.770)	Prec@1 44.531 (44.531)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [50/391]	Time 0.051 (0.071)	Data 0.001 (0.009)	Loss 0.675 (0.861)	Prec@1 73.438 (70.558)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [100/391]	Time 0.037 (0.054)	Data 0.002 (0.005)	Loss 0.750 (0.774)	Prec@1 74.219 (73.128)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [150/391]	Time 0.054 (0.053)	Data 0.002 (0.004)	Loss 0.659 (0.728)	Prec@1 74.219 (74.710)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [200/391]	Time 0.038 (0.051)	Data 0.001 (0.004)	Loss 0.681 (0.710)	Prec@1 76.562 (75.338)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [250/391]	Time 0.042 (0.051)	Data 0.002 (0.003)	Loss 0.781 (0.702)	Prec@1 75.000 (75.514)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [300/391]	Time 0.037 (0.050)	Data 0.001 (0.003)	Loss 0.489 (0.690)	Prec@1 82.812 (75.984)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [350/391]	Time 0.031 (0.048)	Data 0.001 (0.003)	Loss 0.567 (0.681)	Prec@1 78.906 (76.284)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Test: [0/79]	Time: 0.4115(0.4115)	Loss: 0.868(0.868)	Prec@1: 74.219(74.219)	
10-10-22 13:49:Test: [50/79]	Time: 0.0174(0.0261)	Loss: 1.221(0.902)	Prec@1: 65.625(70.772)	
10-10-22 13:49:Test: [78/79]	Time: 0.0161(0.0227)	Loss: 0.621(0.904)	Prec@1: 68.750(70.440)	
10-10-22 13:49:Step 61 * Prec@1 70.440
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [0/391]	Time 0.339 (0.339)	Data 0.302 (0.302)	Loss 1.759 (1.759)	Prec@1 46.094 (46.094)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [50/391]	Time 0.034 (0.040)	Data 0.001 (0.007)	Loss 0.833 (0.816)	Prec@1 74.219 (71.569)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [100/391]	Time 0.037 (0.038)	Data 0.002 (0.004)	Loss 0.591 (0.737)	Prec@1 82.031 (74.203)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [150/391]	Time 0.035 (0.040)	Data 0.001 (0.004)	Loss 0.765 (0.702)	Prec@1 75.000 (75.481)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [200/391]	Time 0.037 (0.040)	Data 0.002 (0.003)	Loss 0.676 (0.685)	Prec@1 81.250 (76.069)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [250/391]	Time 0.038 (0.042)	Data 0.002 (0.003)	Loss 0.530 (0.676)	Prec@1 81.250 (76.503)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [300/391]	Time 0.031 (0.042)	Data 0.001 (0.003)	Loss 0.677 (0.669)	Prec@1 77.344 (76.817)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:49:Num bit 8	Num grad bit 8	
10-10-22 13:49:Iter: [350/391]	Time 0.031 (0.040)	Data 0.001 (0.003)	Loss 0.563 (0.664)	Prec@1 79.688 (76.934)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Test: [0/79]	Time: 0.6458(0.6458)	Loss: 0.818(0.818)	Prec@1: 74.219(74.219)	
10-10-22 13:50:Test: [50/79]	Time: 0.0168(0.0378)	Loss: 1.344(0.960)	Prec@1: 63.281(69.179)	
10-10-22 13:50:Test: [78/79]	Time: 0.0152(0.0304)	Loss: 0.626(0.956)	Prec@1: 75.000(69.170)	
10-10-22 13:50:Step 62 * Prec@1 69.170
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [0/391]	Time 0.335 (0.335)	Data 0.299 (0.299)	Loss 1.775 (1.775)	Prec@1 42.969 (42.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [50/391]	Time 0.037 (0.051)	Data 0.002 (0.007)	Loss 0.791 (0.836)	Prec@1 74.219 (70.895)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [100/391]	Time 0.046 (0.047)	Data 0.001 (0.005)	Loss 0.670 (0.760)	Prec@1 77.344 (73.554)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [150/391]	Time 0.033 (0.047)	Data 0.001 (0.004)	Loss 0.543 (0.725)	Prec@1 82.031 (74.741)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [200/391]	Time 0.046 (0.047)	Data 0.001 (0.003)	Loss 0.656 (0.700)	Prec@1 77.344 (75.692)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [250/391]	Time 0.139 (0.051)	Data 0.008 (0.003)	Loss 0.655 (0.687)	Prec@1 77.344 (76.096)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [300/391]	Time 0.043 (0.052)	Data 0.002 (0.003)	Loss 0.624 (0.673)	Prec@1 78.906 (76.495)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [350/391]	Time 0.037 (0.052)	Data 0.001 (0.003)	Loss 0.496 (0.668)	Prec@1 82.031 (76.667)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Test: [0/79]	Time: 0.3964(0.3964)	Loss: 0.675(0.675)	Prec@1: 76.562(76.562)	
10-10-22 13:50:Test: [50/79]	Time: 0.0161(0.0278)	Loss: 1.004(0.811)	Prec@1: 66.406(72.947)	
10-10-22 13:50:Test: [78/79]	Time: 0.0197(0.0252)	Loss: 0.609(0.814)	Prec@1: 87.500(72.600)	
10-10-22 13:50:Step 63 * Prec@1 72.600
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [0/391]	Time 0.334 (0.334)	Data 0.293 (0.293)	Loss 1.779 (1.779)	Prec@1 45.312 (45.312)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [50/391]	Time 0.035 (0.054)	Data 0.002 (0.008)	Loss 0.813 (0.846)	Prec@1 71.875 (70.895)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [100/391]	Time 0.038 (0.045)	Data 0.001 (0.004)	Loss 0.676 (0.755)	Prec@1 72.656 (73.817)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [150/391]	Time 0.041 (0.049)	Data 0.002 (0.004)	Loss 0.723 (0.718)	Prec@1 74.219 (75.047)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [200/391]	Time 0.038 (0.047)	Data 0.002 (0.003)	Loss 0.568 (0.692)	Prec@1 82.812 (75.964)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [250/391]	Time 0.035 (0.047)	Data 0.001 (0.003)	Loss 0.756 (0.684)	Prec@1 76.562 (76.217)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [300/391]	Time 0.037 (0.046)	Data 0.002 (0.003)	Loss 0.703 (0.673)	Prec@1 76.562 (76.594)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [350/391]	Time 0.032 (0.046)	Data 0.002 (0.003)	Loss 0.590 (0.664)	Prec@1 82.031 (76.881)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Test: [0/79]	Time: 0.3265(0.3265)	Loss: 0.656(0.656)	Prec@1: 78.125(78.125)	
10-10-22 13:50:Test: [50/79]	Time: 0.0166(0.0227)	Loss: 1.073(0.810)	Prec@1: 63.281(73.269)	
10-10-22 13:50:Test: [78/79]	Time: 0.0170(0.0206)	Loss: 0.604(0.808)	Prec@1: 81.250(73.020)	
10-10-22 13:50:Step 64 * Prec@1 73.020
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [0/391]	Time 0.400 (0.400)	Data 0.360 (0.360)	Loss 1.603 (1.603)	Prec@1 49.219 (49.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [50/391]	Time 0.049 (0.059)	Data 0.002 (0.009)	Loss 0.687 (0.812)	Prec@1 72.656 (71.982)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [100/391]	Time 0.034 (0.049)	Data 0.001 (0.005)	Loss 0.692 (0.746)	Prec@1 78.906 (74.211)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [150/391]	Time 0.035 (0.048)	Data 0.002 (0.004)	Loss 0.538 (0.708)	Prec@1 78.125 (75.414)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [200/391]	Time 0.038 (0.046)	Data 0.002 (0.004)	Loss 0.665 (0.693)	Prec@1 78.906 (76.077)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [250/391]	Time 0.047 (0.047)	Data 0.004 (0.003)	Loss 0.646 (0.678)	Prec@1 78.125 (76.534)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:50:Num bit 8	Num grad bit 8	
10-10-22 13:50:Iter: [300/391]	Time 0.051 (0.047)	Data 0.002 (0.003)	Loss 0.635 (0.668)	Prec@1 77.344 (76.866)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [350/391]	Time 0.033 (0.046)	Data 0.001 (0.003)	Loss 0.627 (0.661)	Prec@1 78.125 (77.066)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Test: [0/79]	Time: 0.3340(0.3340)	Loss: 0.684(0.684)	Prec@1: 77.344(77.344)	
10-10-22 13:51:Test: [50/79]	Time: 0.0897(0.0261)	Loss: 1.209(0.890)	Prec@1: 66.406(71.293)	
10-10-22 13:51:Test: [78/79]	Time: 0.0148(0.0359)	Loss: 0.534(0.892)	Prec@1: 75.000(71.300)	
10-10-22 13:51:Step 65 * Prec@1 71.300
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [0/391]	Time 0.346 (0.346)	Data 0.295 (0.295)	Loss 2.108 (2.108)	Prec@1 30.469 (30.469)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [50/391]	Time 0.034 (0.046)	Data 0.001 (0.007)	Loss 0.564 (0.824)	Prec@1 80.469 (71.630)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [100/391]	Time 0.038 (0.047)	Data 0.002 (0.005)	Loss 0.748 (0.732)	Prec@1 74.219 (74.745)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [150/391]	Time 0.039 (0.044)	Data 0.002 (0.004)	Loss 0.682 (0.701)	Prec@1 77.344 (75.854)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [200/391]	Time 0.039 (0.044)	Data 0.002 (0.003)	Loss 0.559 (0.684)	Prec@1 81.250 (76.399)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [250/391]	Time 0.052 (0.045)	Data 0.002 (0.003)	Loss 0.591 (0.674)	Prec@1 82.812 (76.774)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [300/391]	Time 0.035 (0.043)	Data 0.001 (0.003)	Loss 0.718 (0.666)	Prec@1 71.094 (76.954)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [350/391]	Time 0.034 (0.042)	Data 0.001 (0.003)	Loss 0.622 (0.660)	Prec@1 77.344 (77.012)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Test: [0/79]	Time: 0.3187(0.3187)	Loss: 0.754(0.754)	Prec@1: 74.219(74.219)	
10-10-22 13:51:Test: [50/79]	Time: 0.0187(0.0250)	Loss: 1.002(0.840)	Prec@1: 64.844(72.932)	
10-10-22 13:51:Test: [78/79]	Time: 0.0151(0.0220)	Loss: 0.502(0.836)	Prec@1: 81.250(73.010)	
10-10-22 13:51:Step 66 * Prec@1 73.010
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [0/391]	Time 0.342 (0.342)	Data 0.302 (0.302)	Loss 2.465 (2.465)	Prec@1 21.875 (21.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [50/391]	Time 0.039 (0.057)	Data 0.002 (0.008)	Loss 0.862 (0.914)	Prec@1 68.750 (67.693)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [100/391]	Time 0.036 (0.047)	Data 0.002 (0.005)	Loss 0.612 (0.804)	Prec@1 78.125 (71.372)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [150/391]	Time 0.036 (0.046)	Data 0.002 (0.004)	Loss 0.532 (0.756)	Prec@1 79.688 (73.262)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [200/391]	Time 0.036 (0.044)	Data 0.002 (0.003)	Loss 0.633 (0.729)	Prec@1 75.781 (74.230)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [250/391]	Time 0.042 (0.045)	Data 0.003 (0.003)	Loss 0.473 (0.718)	Prec@1 84.375 (74.664)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [300/391]	Time 0.034 (0.045)	Data 0.001 (0.003)	Loss 0.643 (0.707)	Prec@1 75.781 (75.088)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [350/391]	Time 0.043 (0.044)	Data 0.001 (0.003)	Loss 0.585 (0.704)	Prec@1 82.812 (75.249)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Test: [0/79]	Time: 0.2364(0.2364)	Loss: 0.671(0.671)	Prec@1: 76.562(76.562)	
10-10-22 13:51:Test: [50/79]	Time: 0.0200(0.0237)	Loss: 0.946(0.812)	Prec@1: 65.625(73.392)	
10-10-22 13:51:Test: [78/79]	Time: 0.0187(0.0223)	Loss: 0.558(0.804)	Prec@1: 68.750(73.210)	
10-10-22 13:51:Step 67 * Prec@1 73.210
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [0/391]	Time 0.650 (0.650)	Data 0.475 (0.475)	Loss 2.101 (2.101)	Prec@1 35.938 (35.938)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [50/391]	Time 0.046 (0.061)	Data 0.002 (0.011)	Loss 0.912 (0.887)	Prec@1 67.969 (69.225)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [100/391]	Time 0.057 (0.059)	Data 0.002 (0.007)	Loss 0.666 (0.794)	Prec@1 76.562 (72.594)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [150/391]	Time 0.050 (0.058)	Data 0.001 (0.005)	Loss 0.683 (0.756)	Prec@1 74.219 (73.613)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [200/391]	Time 0.049 (0.060)	Data 0.002 (0.005)	Loss 0.592 (0.734)	Prec@1 75.781 (74.324)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:51:Num bit 8	Num grad bit 8	
10-10-22 13:51:Iter: [250/391]	Time 0.034 (0.056)	Data 0.002 (0.004)	Loss 0.656 (0.717)	Prec@1 77.344 (74.847)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [300/391]	Time 0.039 (0.054)	Data 0.002 (0.004)	Loss 0.632 (0.710)	Prec@1 77.344 (75.169)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [350/391]	Time 0.043 (0.052)	Data 0.002 (0.003)	Loss 0.656 (0.705)	Prec@1 78.125 (75.416)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Test: [0/79]	Time: 0.4707(0.4707)	Loss: 0.728(0.728)	Prec@1: 75.781(75.781)	
10-10-22 13:52:Test: [50/79]	Time: 0.0161(0.0262)	Loss: 0.894(0.823)	Prec@1: 71.875(72.672)	
10-10-22 13:52:Test: [78/79]	Time: 0.0149(0.0227)	Loss: 0.552(0.817)	Prec@1: 81.250(72.520)	
10-10-22 13:52:Step 68 * Prec@1 72.520
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [0/391]	Time 0.306 (0.306)	Data 0.269 (0.269)	Loss 1.867 (1.867)	Prec@1 36.719 (36.719)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [50/391]	Time 0.034 (0.040)	Data 0.002 (0.007)	Loss 0.864 (0.920)	Prec@1 68.750 (68.673)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [100/391]	Time 0.033 (0.037)	Data 0.001 (0.004)	Loss 0.609 (0.819)	Prec@1 84.375 (71.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [150/391]	Time 0.039 (0.037)	Data 0.002 (0.003)	Loss 0.727 (0.780)	Prec@1 75.000 (73.210)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [200/391]	Time 0.036 (0.037)	Data 0.001 (0.003)	Loss 0.708 (0.755)	Prec@1 73.438 (74.090)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [250/391]	Time 0.037 (0.042)	Data 0.002 (0.003)	Loss 0.571 (0.737)	Prec@1 80.469 (74.658)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [300/391]	Time 0.036 (0.041)	Data 0.001 (0.003)	Loss 0.603 (0.728)	Prec@1 76.562 (74.904)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [350/391]	Time 0.042 (0.042)	Data 0.001 (0.002)	Loss 0.667 (0.725)	Prec@1 77.344 (74.987)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Test: [0/79]	Time: 0.3363(0.3363)	Loss: 0.949(0.949)	Prec@1: 67.188(67.188)	
10-10-22 13:52:Test: [50/79]	Time: 0.0189(0.0273)	Loss: 1.043(1.058)	Prec@1: 66.406(67.065)	
10-10-22 13:52:Test: [78/79]	Time: 0.0152(0.0251)	Loss: 0.896(1.055)	Prec@1: 62.500(66.860)	
10-10-22 13:52:Step 69 * Prec@1 66.860
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [0/391]	Time 0.428 (0.428)	Data 0.376 (0.376)	Loss 2.379 (2.379)	Prec@1 31.250 (31.250)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [50/391]	Time 0.036 (0.049)	Data 0.002 (0.009)	Loss 0.780 (0.950)	Prec@1 71.094 (68.520)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [100/391]	Time 0.031 (0.043)	Data 0.001 (0.005)	Loss 0.809 (0.856)	Prec@1 71.094 (70.924)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [150/391]	Time 0.051 (0.050)	Data 0.002 (0.005)	Loss 0.734 (0.829)	Prec@1 74.219 (71.559)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [200/391]	Time 0.035 (0.046)	Data 0.002 (0.004)	Loss 0.784 (0.812)	Prec@1 73.438 (72.081)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [250/391]	Time 0.034 (0.047)	Data 0.001 (0.003)	Loss 0.915 (0.806)	Prec@1 69.531 (72.189)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [300/391]	Time 0.043 (0.046)	Data 0.004 (0.003)	Loss 0.758 (0.797)	Prec@1 75.781 (72.526)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [350/391]	Time 0.065 (0.046)	Data 0.002 (0.003)	Loss 0.519 (0.791)	Prec@1 83.594 (72.716)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Test: [0/79]	Time: 0.3032(0.3032)	Loss: 1.046(1.046)	Prec@1: 67.188(67.188)	
10-10-22 13:52:Test: [50/79]	Time: 0.0161(0.0226)	Loss: 1.029(1.046)	Prec@1: 65.625(68.168)	
10-10-22 13:52:Test: [78/79]	Time: 0.0148(0.0204)	Loss: 1.212(1.057)	Prec@1: 43.750(67.770)	
10-10-22 13:52:Step 70 * Prec@1 67.770
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [0/391]	Time 0.649 (0.649)	Data 0.532 (0.532)	Loss 2.027 (2.027)	Prec@1 32.812 (32.812)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [50/391]	Time 0.047 (0.069)	Data 0.002 (0.012)	Loss 0.674 (1.018)	Prec@1 76.562 (66.054)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [100/391]	Time 0.038 (0.059)	Data 0.002 (0.007)	Loss 0.765 (0.921)	Prec@1 72.656 (68.626)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [150/391]	Time 0.038 (0.053)	Data 0.002 (0.005)	Loss 0.642 (0.868)	Prec@1 74.219 (70.235)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [200/391]	Time 0.035 (0.050)	Data 0.001 (0.005)	Loss 0.825 (0.838)	Prec@1 72.656 (71.195)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:52:Num bit 8	Num grad bit 8	
10-10-22 13:52:Iter: [250/391]	Time 0.046 (0.050)	Data 0.002 (0.004)	Loss 0.915 (0.813)	Prec@1 69.531 (71.943)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [300/391]	Time 0.048 (0.049)	Data 0.001 (0.004)	Loss 0.807 (0.800)	Prec@1 71.094 (72.262)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [350/391]	Time 0.031 (0.046)	Data 0.001 (0.003)	Loss 0.829 (0.788)	Prec@1 71.875 (72.665)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Test: [0/79]	Time: 0.3910(0.3910)	Loss: 0.718(0.718)	Prec@1: 78.906(78.906)	
10-10-22 13:53:Test: [50/79]	Time: 0.0473(0.0299)	Loss: 0.805(0.833)	Prec@1: 74.219(72.917)	
10-10-22 13:53:Test: [78/79]	Time: 0.0223(0.0352)	Loss: 0.767(0.833)	Prec@1: 62.500(72.570)	
10-10-22 13:53:Step 71 * Prec@1 72.570
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [0/391]	Time 0.388 (0.388)	Data 0.326 (0.326)	Loss 2.107 (2.107)	Prec@1 42.969 (42.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [50/391]	Time 0.034 (0.047)	Data 0.001 (0.008)	Loss 0.746 (0.995)	Prec@1 71.094 (65.518)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [100/391]	Time 0.047 (0.049)	Data 0.001 (0.005)	Loss 0.598 (0.883)	Prec@1 78.125 (69.090)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [150/391]	Time 0.048 (0.047)	Data 0.001 (0.004)	Loss 0.722 (0.843)	Prec@1 78.906 (70.400)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [200/391]	Time 0.049 (0.050)	Data 0.002 (0.004)	Loss 0.688 (0.820)	Prec@1 75.781 (71.323)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [250/391]	Time 0.037 (0.049)	Data 0.002 (0.003)	Loss 0.700 (0.800)	Prec@1 72.656 (71.975)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [300/391]	Time 0.046 (0.051)	Data 0.002 (0.003)	Loss 0.728 (0.790)	Prec@1 71.875 (72.397)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [350/391]	Time 0.044 (0.049)	Data 0.002 (0.003)	Loss 0.902 (0.785)	Prec@1 70.312 (72.630)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Test: [0/79]	Time: 0.6820(0.6820)	Loss: 0.862(0.862)	Prec@1: 67.969(67.969)	
10-10-22 13:53:Test: [50/79]	Time: 0.0261(0.0332)	Loss: 0.932(0.903)	Prec@1: 68.750(71.002)	
10-10-22 13:53:Test: [78/79]	Time: 0.0186(0.0282)	Loss: 0.770(0.898)	Prec@1: 75.000(70.820)	
10-10-22 13:53:Step 72 * Prec@1 70.820
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [0/391]	Time 0.420 (0.420)	Data 0.380 (0.380)	Loss 1.870 (1.870)	Prec@1 40.625 (40.625)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [50/391]	Time 0.043 (0.048)	Data 0.001 (0.009)	Loss 0.719 (0.996)	Prec@1 73.438 (66.468)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [100/391]	Time 0.057 (0.047)	Data 0.002 (0.006)	Loss 0.684 (0.871)	Prec@1 75.000 (70.282)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [150/391]	Time 0.042 (0.046)	Data 0.001 (0.004)	Loss 0.802 (0.828)	Prec@1 71.094 (71.435)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [200/391]	Time 0.073 (0.044)	Data 0.003 (0.004)	Loss 0.677 (0.801)	Prec@1 72.656 (72.287)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [250/391]	Time 0.040 (0.048)	Data 0.001 (0.004)	Loss 0.740 (0.792)	Prec@1 75.781 (72.644)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [300/391]	Time 0.064 (0.048)	Data 0.001 (0.003)	Loss 0.670 (0.780)	Prec@1 75.781 (72.955)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [350/391]	Time 0.036 (0.047)	Data 0.001 (0.003)	Loss 0.710 (0.776)	Prec@1 76.562 (73.026)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Test: [0/79]	Time: 0.6246(0.6246)	Loss: 0.726(0.726)	Prec@1: 72.656(72.656)	
10-10-22 13:53:Test: [50/79]	Time: 0.0162(0.0291)	Loss: 0.840(0.838)	Prec@1: 70.312(73.300)	
10-10-22 13:53:Test: [78/79]	Time: 0.0214(0.0247)	Loss: 0.730(0.840)	Prec@1: 68.750(72.910)	
10-10-22 13:53:Step 73 * Prec@1 72.910
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [0/391]	Time 0.375 (0.375)	Data 0.340 (0.340)	Loss 2.289 (2.289)	Prec@1 30.469 (30.469)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [50/391]	Time 0.031 (0.039)	Data 0.001 (0.008)	Loss 0.861 (1.075)	Prec@1 72.656 (63.909)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [100/391]	Time 0.054 (0.044)	Data 0.002 (0.005)	Loss 0.735 (0.943)	Prec@1 71.094 (68.178)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [150/391]	Time 0.031 (0.046)	Data 0.001 (0.004)	Loss 0.906 (0.896)	Prec@1 71.094 (69.542)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [200/391]	Time 0.040 (0.046)	Data 0.001 (0.004)	Loss 0.807 (0.862)	Prec@1 73.438 (70.538)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:53:Num bit 8	Num grad bit 8	
10-10-22 13:53:Iter: [250/391]	Time 0.062 (0.046)	Data 0.002 (0.003)	Loss 0.822 (0.842)	Prec@1 71.094 (71.187)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [300/391]	Time 0.057 (0.046)	Data 0.002 (0.003)	Loss 0.777 (0.829)	Prec@1 72.656 (71.699)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [350/391]	Time 0.035 (0.047)	Data 0.001 (0.003)	Loss 1.028 (0.823)	Prec@1 68.750 (71.935)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Test: [0/79]	Time: 0.8769(0.8769)	Loss: 0.832(0.832)	Prec@1: 70.312(70.312)	
10-10-22 13:54:Test: [50/79]	Time: 0.0160(0.0338)	Loss: 0.988(0.901)	Prec@1: 64.844(70.236)	
10-10-22 13:54:Test: [78/79]	Time: 0.0150(0.0278)	Loss: 0.709(0.906)	Prec@1: 75.000(69.890)	
10-10-22 13:54:Step 74 * Prec@1 69.890
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [0/391]	Time 0.413 (0.413)	Data 0.377 (0.377)	Loss 3.224 (3.224)	Prec@1 17.969 (17.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [50/391]	Time 0.034 (0.045)	Data 0.001 (0.009)	Loss 0.664 (1.166)	Prec@1 76.562 (63.419)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [100/391]	Time 0.054 (0.047)	Data 0.002 (0.005)	Loss 0.784 (0.982)	Prec@1 73.438 (67.837)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [150/391]	Time 0.033 (0.047)	Data 0.001 (0.004)	Loss 0.821 (0.916)	Prec@1 67.969 (69.433)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [200/391]	Time 0.038 (0.047)	Data 0.002 (0.004)	Loss 0.805 (0.874)	Prec@1 70.312 (70.635)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [250/391]	Time 0.040 (0.047)	Data 0.001 (0.003)	Loss 0.923 (0.857)	Prec@1 74.219 (71.091)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [300/391]	Time 0.042 (0.045)	Data 0.001 (0.003)	Loss 0.718 (0.845)	Prec@1 72.656 (71.283)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [350/391]	Time 0.042 (0.044)	Data 0.002 (0.003)	Loss 0.649 (0.838)	Prec@1 78.906 (71.546)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Test: [0/79]	Time: 0.3440(0.3440)	Loss: 0.904(0.904)	Prec@1: 69.531(69.531)	
10-10-22 13:54:Test: [50/79]	Time: 0.0160(0.0244)	Loss: 1.094(1.063)	Prec@1: 67.188(66.590)	
10-10-22 13:54:Test: [78/79]	Time: 0.0148(0.0227)	Loss: 0.709(1.052)	Prec@1: 62.500(66.910)	
10-10-22 13:54:Step 75 * Prec@1 66.910
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [0/391]	Time 0.773 (0.773)	Data 0.718 (0.718)	Loss 2.227 (2.227)	Prec@1 29.688 (29.688)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [50/391]	Time 0.038 (0.055)	Data 0.002 (0.016)	Loss 0.716 (1.087)	Prec@1 78.125 (64.614)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [100/391]	Time 0.053 (0.051)	Data 0.003 (0.009)	Loss 0.758 (0.939)	Prec@1 76.562 (68.758)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [150/391]	Time 0.031 (0.048)	Data 0.001 (0.007)	Loss 0.786 (0.884)	Prec@1 68.750 (70.235)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [200/391]	Time 0.035 (0.045)	Data 0.001 (0.005)	Loss 0.706 (0.854)	Prec@1 75.000 (71.098)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [250/391]	Time 0.037 (0.047)	Data 0.001 (0.005)	Loss 0.660 (0.842)	Prec@1 73.438 (71.402)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [300/391]	Time 0.044 (0.046)	Data 0.001 (0.004)	Loss 0.804 (0.831)	Prec@1 70.312 (71.683)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [350/391]	Time 0.126 (0.047)	Data 0.002 (0.004)	Loss 0.600 (0.819)	Prec@1 81.250 (72.046)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Test: [0/79]	Time: 0.3600(0.3600)	Loss: 0.830(0.830)	Prec@1: 67.969(67.969)	
10-10-22 13:54:Test: [50/79]	Time: 0.0214(0.0277)	Loss: 1.030(0.957)	Prec@1: 64.844(69.225)	
10-10-22 13:54:Test: [78/79]	Time: 0.0150(0.0241)	Loss: 0.821(0.948)	Prec@1: 75.000(69.060)	
10-10-22 13:54:Step 76 * Prec@1 69.060
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [0/391]	Time 0.336 (0.336)	Data 0.302 (0.302)	Loss 2.456 (2.456)	Prec@1 25.781 (25.781)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [50/391]	Time 0.031 (0.042)	Data 0.001 (0.007)	Loss 0.855 (1.113)	Prec@1 67.188 (63.955)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [100/391]	Time 0.045 (0.040)	Data 0.001 (0.004)	Loss 0.643 (0.950)	Prec@1 79.688 (68.417)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [150/391]	Time 0.118 (0.041)	Data 0.004 (0.003)	Loss 0.723 (0.886)	Prec@1 74.219 (70.230)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:54:Num bit 8	Num grad bit 8	
10-10-22 13:54:Iter: [200/391]	Time 0.037 (0.046)	Data 0.002 (0.003)	Loss 0.836 (0.855)	Prec@1 72.656 (71.094)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [250/391]	Time 0.035 (0.044)	Data 0.001 (0.003)	Loss 0.784 (0.840)	Prec@1 75.000 (71.620)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [300/391]	Time 0.034 (0.043)	Data 0.001 (0.003)	Loss 0.974 (0.828)	Prec@1 68.750 (71.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [350/391]	Time 0.043 (0.043)	Data 0.002 (0.003)	Loss 0.616 (0.818)	Prec@1 78.906 (72.171)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Test: [0/79]	Time: 0.4101(0.4101)	Loss: 0.710(0.710)	Prec@1: 78.125(78.125)	
10-10-22 13:55:Test: [50/79]	Time: 0.0165(0.0277)	Loss: 1.084(0.924)	Prec@1: 63.281(69.746)	
10-10-22 13:55:Test: [78/79]	Time: 0.0206(0.0262)	Loss: 1.033(0.921)	Prec@1: 56.250(69.660)	
10-10-22 13:55:Step 77 * Prec@1 69.660
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [0/391]	Time 0.605 (0.605)	Data 0.567 (0.567)	Loss 2.887 (2.887)	Prec@1 25.781 (25.781)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [50/391]	Time 0.038 (0.051)	Data 0.002 (0.013)	Loss 0.875 (1.228)	Prec@1 71.094 (63.251)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [100/391]	Time 0.035 (0.044)	Data 0.001 (0.007)	Loss 0.611 (1.000)	Prec@1 76.562 (68.386)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [150/391]	Time 0.037 (0.052)	Data 0.001 (0.006)	Loss 0.733 (0.933)	Prec@1 72.656 (69.697)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [200/391]	Time 0.057 (0.049)	Data 0.003 (0.005)	Loss 0.827 (0.905)	Prec@1 71.875 (70.173)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [250/391]	Time 0.036 (0.049)	Data 0.002 (0.004)	Loss 0.805 (0.888)	Prec@1 75.000 (70.540)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [300/391]	Time 0.039 (0.048)	Data 0.002 (0.004)	Loss 0.791 (0.876)	Prec@1 74.219 (70.780)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [350/391]	Time 0.035 (0.048)	Data 0.001 (0.003)	Loss 0.880 (0.866)	Prec@1 68.750 (70.987)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Test: [0/79]	Time: 0.3204(0.3204)	Loss: 1.001(1.001)	Prec@1: 65.625(65.625)	
10-10-22 13:55:Test: [50/79]	Time: 0.0165(0.0229)	Loss: 1.449(1.291)	Prec@1: 58.594(61.152)	
10-10-22 13:55:Test: [78/79]	Time: 0.0272(0.0242)	Loss: 0.761(1.273)	Prec@1: 68.750(61.510)	
10-10-22 13:55:Step 78 * Prec@1 61.510
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [0/391]	Time 0.943 (0.943)	Data 0.868 (0.868)	Loss 3.293 (3.293)	Prec@1 27.344 (27.344)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [50/391]	Time 0.034 (0.057)	Data 0.002 (0.019)	Loss 0.912 (1.230)	Prec@1 69.531 (63.297)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [100/391]	Time 0.034 (0.052)	Data 0.001 (0.010)	Loss 0.659 (1.015)	Prec@1 72.656 (67.474)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [150/391]	Time 0.099 (0.048)	Data 0.003 (0.007)	Loss 1.079 (0.939)	Prec@1 66.406 (69.298)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [200/391]	Time 0.050 (0.049)	Data 0.002 (0.006)	Loss 0.631 (0.899)	Prec@1 77.344 (70.138)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [250/391]	Time 0.036 (0.047)	Data 0.001 (0.005)	Loss 0.853 (0.878)	Prec@1 70.312 (70.527)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [300/391]	Time 0.033 (0.045)	Data 0.001 (0.005)	Loss 0.679 (0.859)	Prec@1 76.562 (70.959)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [350/391]	Time 0.061 (0.046)	Data 0.002 (0.004)	Loss 0.596 (0.851)	Prec@1 81.250 (71.223)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Test: [0/79]	Time: 0.4721(0.4721)	Loss: 1.000(1.000)	Prec@1: 67.188(67.188)	
10-10-22 13:55:Test: [50/79]	Time: 0.0163(0.0293)	Loss: 1.296(1.232)	Prec@1: 58.594(62.071)	
10-10-22 13:55:Test: [78/79]	Time: 0.0149(0.0247)	Loss: 1.254(1.225)	Prec@1: 68.750(61.940)	
10-10-22 13:55:Step 79 * Prec@1 61.940
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [0/391]	Time 0.371 (0.371)	Data 0.334 (0.334)	Loss 3.316 (3.316)	Prec@1 22.656 (22.656)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [50/391]	Time 0.044 (0.048)	Data 0.002 (0.008)	Loss 0.724 (1.175)	Prec@1 70.312 (64.828)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [100/391]	Time 0.049 (0.046)	Data 0.002 (0.005)	Loss 0.820 (0.981)	Prec@1 74.219 (68.704)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [150/391]	Time 0.039 (0.045)	Data 0.001 (0.004)	Loss 0.679 (0.905)	Prec@1 77.344 (70.571)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:55:Num bit 8	Num grad bit 8	
10-10-22 13:55:Iter: [200/391]	Time 0.034 (0.047)	Data 0.001 (0.003)	Loss 0.956 (0.870)	Prec@1 70.312 (71.346)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [250/391]	Time 0.037 (0.045)	Data 0.002 (0.003)	Loss 0.639 (0.848)	Prec@1 78.906 (71.878)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [300/391]	Time 0.037 (0.045)	Data 0.002 (0.003)	Loss 0.600 (0.839)	Prec@1 80.469 (71.997)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [350/391]	Time 0.037 (0.045)	Data 0.002 (0.003)	Loss 0.688 (0.839)	Prec@1 75.781 (71.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Test: [0/79]	Time: 0.4083(0.4083)	Loss: 0.792(0.792)	Prec@1: 73.438(73.438)	
10-10-22 13:56:Test: [50/79]	Time: 0.0231(0.0277)	Loss: 1.012(1.015)	Prec@1: 65.625(67.203)	
10-10-22 13:56:Test: [78/79]	Time: 0.0214(0.0261)	Loss: 0.936(1.009)	Prec@1: 68.750(67.400)	
10-10-22 13:56:Step 80 * Prec@1 67.400
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [0/391]	Time 0.400 (0.400)	Data 0.365 (0.365)	Loss 2.816 (2.816)	Prec@1 30.469 (30.469)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [50/391]	Time 0.040 (0.042)	Data 0.001 (0.008)	Loss 1.104 (1.195)	Prec@1 61.719 (63.465)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [100/391]	Time 0.042 (0.052)	Data 0.002 (0.006)	Loss 0.768 (1.011)	Prec@1 71.094 (67.350)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [150/391]	Time 0.061 (0.052)	Data 0.002 (0.004)	Loss 0.784 (0.937)	Prec@1 72.656 (69.071)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [200/391]	Time 0.037 (0.049)	Data 0.002 (0.004)	Loss 1.041 (0.905)	Prec@1 65.625 (69.869)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [250/391]	Time 0.048 (0.047)	Data 0.003 (0.003)	Loss 0.933 (0.883)	Prec@1 71.094 (70.409)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [300/391]	Time 0.037 (0.046)	Data 0.002 (0.003)	Loss 0.832 (0.879)	Prec@1 71.094 (70.466)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [350/391]	Time 0.033 (0.045)	Data 0.001 (0.003)	Loss 0.719 (0.872)	Prec@1 74.219 (70.680)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Test: [0/79]	Time: 0.3742(0.3742)	Loss: 1.218(1.218)	Prec@1: 57.031(57.031)	
10-10-22 13:56:Test: [50/79]	Time: 0.0159(0.0235)	Loss: 1.377(1.351)	Prec@1: 61.719(61.612)	
10-10-22 13:56:Test: [78/79]	Time: 0.0148(0.0210)	Loss: 0.774(1.347)	Prec@1: 68.750(61.430)	
10-10-22 13:56:Step 81 * Prec@1 61.430
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [0/391]	Time 0.378 (0.378)	Data 0.343 (0.343)	Loss 3.132 (3.132)	Prec@1 24.219 (24.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [50/391]	Time 0.043 (0.053)	Data 0.001 (0.009)	Loss 0.721 (1.405)	Prec@1 77.344 (60.846)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [100/391]	Time 0.036 (0.050)	Data 0.002 (0.005)	Loss 0.681 (1.147)	Prec@1 76.562 (65.671)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [150/391]	Time 0.036 (0.049)	Data 0.001 (0.004)	Loss 0.887 (1.043)	Prec@1 69.531 (67.503)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [200/391]	Time 0.035 (0.048)	Data 0.001 (0.004)	Loss 0.827 (0.984)	Prec@1 72.656 (68.645)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [250/391]	Time 0.035 (0.045)	Data 0.001 (0.003)	Loss 0.614 (0.946)	Prec@1 79.688 (69.466)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [300/391]	Time 0.035 (0.044)	Data 0.001 (0.003)	Loss 0.718 (0.925)	Prec@1 79.688 (69.985)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [350/391]	Time 0.035 (0.043)	Data 0.002 (0.003)	Loss 0.822 (0.908)	Prec@1 71.094 (70.252)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Test: [0/79]	Time: 0.3940(0.3940)	Loss: 0.983(0.983)	Prec@1: 67.188(67.188)	
10-10-22 13:56:Test: [50/79]	Time: 0.0162(0.0244)	Loss: 1.034(1.048)	Prec@1: 64.062(66.207)	
10-10-22 13:56:Test: [78/79]	Time: 0.0149(0.0226)	Loss: 0.920(1.041)	Prec@1: 75.000(66.270)	
10-10-22 13:56:Step 82 * Prec@1 66.270
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [0/391]	Time 0.401 (0.401)	Data 0.365 (0.365)	Loss 2.452 (2.452)	Prec@1 28.906 (28.906)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [50/391]	Time 0.050 (0.046)	Data 0.001 (0.009)	Loss 0.687 (1.279)	Prec@1 75.781 (63.710)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [100/391]	Time 0.040 (0.043)	Data 0.002 (0.005)	Loss 0.861 (1.032)	Prec@1 68.750 (68.301)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [150/391]	Time 0.041 (0.043)	Data 0.002 (0.004)	Loss 0.782 (0.952)	Prec@1 68.750 (69.821)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [200/391]	Time 0.040 (0.042)	Data 0.002 (0.004)	Loss 0.712 (0.916)	Prec@1 74.219 (70.449)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:56:Num bit 8	Num grad bit 8	
10-10-22 13:56:Iter: [250/391]	Time 0.038 (0.042)	Data 0.001 (0.003)	Loss 0.901 (0.893)	Prec@1 66.406 (70.817)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [300/391]	Time 0.035 (0.044)	Data 0.001 (0.003)	Loss 0.709 (0.880)	Prec@1 78.125 (70.938)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [350/391]	Time 0.036 (0.043)	Data 0.001 (0.003)	Loss 1.186 (0.880)	Prec@1 63.281 (70.816)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Test: [0/79]	Time: 0.3399(0.3399)	Loss: 1.425(1.425)	Prec@1: 61.719(61.719)	
10-10-22 13:57:Test: [50/79]	Time: 0.0169(0.0232)	Loss: 1.453(1.500)	Prec@1: 57.031(57.629)	
10-10-22 13:57:Test: [78/79]	Time: 0.0156(0.0221)	Loss: 1.867(1.493)	Prec@1: 62.500(57.250)	
10-10-22 13:57:Step 83 * Prec@1 57.250
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [0/391]	Time 0.399 (0.399)	Data 0.359 (0.359)	Loss 3.466 (3.466)	Prec@1 26.562 (26.562)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [50/391]	Time 0.058 (0.050)	Data 0.002 (0.009)	Loss 0.926 (1.586)	Prec@1 63.281 (58.425)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [100/391]	Time 0.036 (0.051)	Data 0.002 (0.005)	Loss 0.840 (1.259)	Prec@1 67.969 (62.972)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [150/391]	Time 0.035 (0.046)	Data 0.001 (0.004)	Loss 0.884 (1.124)	Prec@1 67.188 (65.382)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [200/391]	Time 0.169 (0.049)	Data 0.008 (0.004)	Loss 0.871 (1.048)	Prec@1 67.969 (66.772)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [250/391]	Time 0.035 (0.048)	Data 0.001 (0.003)	Loss 0.728 (1.002)	Prec@1 76.562 (67.667)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [300/391]	Time 0.037 (0.048)	Data 0.001 (0.003)	Loss 0.798 (0.979)	Prec@1 71.875 (68.060)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [350/391]	Time 0.034 (0.047)	Data 0.001 (0.003)	Loss 0.862 (0.960)	Prec@1 71.875 (68.474)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Test: [0/79]	Time: 0.3309(0.3309)	Loss: 1.089(1.089)	Prec@1: 65.625(65.625)	
10-10-22 13:57:Test: [50/79]	Time: 0.0161(0.0226)	Loss: 1.297(1.114)	Prec@1: 58.594(65.242)	
10-10-22 13:57:Test: [78/79]	Time: 0.0150(0.0204)	Loss: 1.195(1.108)	Prec@1: 62.500(65.360)	
10-10-22 13:57:Step 84 * Prec@1 65.360
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [0/391]	Time 0.405 (0.405)	Data 0.362 (0.362)	Loss 2.769 (2.769)	Prec@1 30.469 (30.469)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [50/391]	Time 0.041 (0.040)	Data 0.002 (0.008)	Loss 0.779 (1.239)	Prec@1 72.656 (61.443)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [100/391]	Time 0.040 (0.039)	Data 0.001 (0.005)	Loss 0.826 (1.039)	Prec@1 67.969 (66.166)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [150/391]	Time 0.040 (0.041)	Data 0.003 (0.004)	Loss 0.707 (0.966)	Prec@1 76.562 (67.948)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [200/391]	Time 0.035 (0.041)	Data 0.001 (0.003)	Loss 0.871 (0.931)	Prec@1 71.094 (68.808)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [250/391]	Time 0.034 (0.041)	Data 0.002 (0.003)	Loss 0.825 (0.908)	Prec@1 72.656 (69.373)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [300/391]	Time 0.058 (0.041)	Data 0.001 (0.003)	Loss 0.833 (0.899)	Prec@1 71.875 (69.555)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [350/391]	Time 0.056 (0.041)	Data 0.002 (0.003)	Loss 0.730 (0.885)	Prec@1 76.562 (70.094)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Test: [0/79]	Time: 0.3320(0.3320)	Loss: 1.027(1.027)	Prec@1: 65.625(65.625)	
10-10-22 13:57:Test: [50/79]	Time: 0.0160(0.0243)	Loss: 1.397(1.323)	Prec@1: 59.375(61.489)	
10-10-22 13:57:Test: [78/79]	Time: 0.0148(0.0214)	Loss: 1.202(1.300)	Prec@1: 62.500(61.900)	
10-10-22 13:57:Step 85 * Prec@1 61.900
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [0/391]	Time 0.389 (0.389)	Data 0.354 (0.354)	Loss 3.670 (3.670)	Prec@1 27.344 (27.344)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [50/391]	Time 0.046 (0.039)	Data 0.002 (0.008)	Loss 0.873 (1.465)	Prec@1 70.312 (60.018)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [100/391]	Time 0.042 (0.044)	Data 0.002 (0.005)	Loss 0.690 (1.179)	Prec@1 78.125 (64.774)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [150/391]	Time 0.035 (0.042)	Data 0.001 (0.004)	Loss 0.750 (1.065)	Prec@1 75.781 (66.799)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [200/391]	Time 0.037 (0.044)	Data 0.002 (0.003)	Loss 0.767 (1.008)	Prec@1 73.438 (67.806)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [250/391]	Time 0.066 (0.044)	Data 0.003 (0.003)	Loss 0.819 (0.975)	Prec@1 72.656 (68.429)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:57:Num bit 8	Num grad bit 8	
10-10-22 13:57:Iter: [300/391]	Time 0.035 (0.045)	Data 0.001 (0.003)	Loss 0.880 (0.957)	Prec@1 67.188 (68.742)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [350/391]	Time 0.035 (0.043)	Data 0.001 (0.003)	Loss 1.271 (0.950)	Prec@1 58.594 (68.855)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Test: [0/79]	Time: 0.3575(0.3575)	Loss: 1.293(1.293)	Prec@1: 61.719(61.719)	
10-10-22 13:58:Test: [50/79]	Time: 0.0161(0.0231)	Loss: 1.475(1.337)	Prec@1: 53.125(59.375)	
10-10-22 13:58:Test: [78/79]	Time: 0.0149(0.0206)	Loss: 1.361(1.324)	Prec@1: 37.500(59.600)	
10-10-22 13:58:Step 86 * Prec@1 59.600
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [0/391]	Time 0.385 (0.385)	Data 0.350 (0.350)	Loss 2.874 (2.874)	Prec@1 20.312 (20.312)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [50/391]	Time 0.032 (0.041)	Data 0.001 (0.008)	Loss 0.926 (1.481)	Prec@1 66.406 (61.397)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [100/391]	Time 0.038 (0.048)	Data 0.002 (0.005)	Loss 0.894 (1.199)	Prec@1 69.531 (65.006)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [150/391]	Time 0.035 (0.044)	Data 0.001 (0.004)	Loss 0.717 (1.100)	Prec@1 76.562 (66.179)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [200/391]	Time 0.038 (0.044)	Data 0.002 (0.003)	Loss 0.966 (1.050)	Prec@1 66.406 (66.873)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [250/391]	Time 0.036 (0.043)	Data 0.001 (0.003)	Loss 0.910 (1.009)	Prec@1 68.750 (67.567)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [300/391]	Time 0.043 (0.043)	Data 0.001 (0.003)	Loss 0.657 (0.978)	Prec@1 71.094 (68.083)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [350/391]	Time 0.058 (0.045)	Data 0.002 (0.003)	Loss 0.725 (0.968)	Prec@1 72.656 (68.142)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Test: [0/79]	Time: 0.8046(0.8046)	Loss: 1.103(1.103)	Prec@1: 61.719(61.719)	
10-10-22 13:58:Test: [50/79]	Time: 0.0218(0.0414)	Loss: 1.365(1.195)	Prec@1: 53.906(62.791)	
10-10-22 13:58:Test: [78/79]	Time: 0.0148(0.0335)	Loss: 1.195(1.199)	Prec@1: 50.000(62.570)	
10-10-22 13:58:Step 87 * Prec@1 62.570
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [0/391]	Time 0.409 (0.409)	Data 0.361 (0.361)	Loss 2.320 (2.320)	Prec@1 29.688 (29.688)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [50/391]	Time 0.036 (0.045)	Data 0.002 (0.008)	Loss 0.975 (1.293)	Prec@1 68.750 (61.673)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [100/391]	Time 0.037 (0.047)	Data 0.001 (0.005)	Loss 0.923 (1.087)	Prec@1 64.062 (65.648)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [150/391]	Time 0.045 (0.046)	Data 0.001 (0.004)	Loss 0.924 (1.006)	Prec@1 63.281 (67.275)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [200/391]	Time 0.037 (0.045)	Data 0.001 (0.004)	Loss 0.926 (0.974)	Prec@1 69.531 (67.806)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [250/391]	Time 0.035 (0.044)	Data 0.002 (0.003)	Loss 1.132 (0.953)	Prec@1 57.812 (68.196)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [300/391]	Time 0.157 (0.047)	Data 0.008 (0.003)	Loss 0.917 (0.948)	Prec@1 71.094 (68.179)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [350/391]	Time 0.036 (0.046)	Data 0.002 (0.003)	Loss 0.886 (0.955)	Prec@1 67.188 (68.073)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Test: [0/79]	Time: 0.4092(0.4092)	Loss: 1.637(1.637)	Prec@1: 52.344(52.344)	
10-10-22 13:58:Test: [50/79]	Time: 0.0164(0.0257)	Loss: 1.972(1.873)	Prec@1: 48.438(52.589)	
10-10-22 13:58:Test: [78/79]	Time: 0.0151(0.0224)	Loss: 1.553(1.895)	Prec@1: 56.250(51.890)	
10-10-22 13:58:Step 88 * Prec@1 51.890
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [0/391]	Time 0.494 (0.494)	Data 0.456 (0.456)	Loss 4.041 (4.041)	Prec@1 18.750 (18.750)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [50/391]	Time 0.066 (0.057)	Data 0.002 (0.011)	Loss 0.995 (1.750)	Prec@1 63.281 (58.885)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [100/391]	Time 0.031 (0.052)	Data 0.001 (0.006)	Loss 0.732 (1.321)	Prec@1 75.781 (64.271)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [150/391]	Time 0.066 (0.056)	Data 0.003 (0.005)	Loss 0.732 (1.180)	Prec@1 77.344 (66.034)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [200/391]	Time 0.036 (0.053)	Data 0.001 (0.004)	Loss 0.881 (1.123)	Prec@1 66.406 (66.406)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:58:Num bit 8	Num grad bit 8	
10-10-22 13:58:Iter: [250/391]	Time 0.031 (0.051)	Data 0.001 (0.004)	Loss 1.003 (1.083)	Prec@1 62.500 (66.761)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [300/391]	Time 0.038 (0.049)	Data 0.002 (0.003)	Loss 1.220 (1.072)	Prec@1 60.938 (66.689)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [350/391]	Time 0.041 (0.048)	Data 0.002 (0.003)	Loss 0.736 (1.051)	Prec@1 71.875 (66.960)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Test: [0/79]	Time: 0.3074(0.3074)	Loss: 1.201(1.201)	Prec@1: 64.844(64.844)	
10-10-22 13:59:Test: [50/79]	Time: 0.0171(0.0223)	Loss: 1.286(1.354)	Prec@1: 55.469(60.570)	
10-10-22 13:59:Test: [78/79]	Time: 0.0152(0.0201)	Loss: 0.945(1.374)	Prec@1: 62.500(60.070)	
10-10-22 13:59:Step 89 * Prec@1 60.070
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [0/391]	Time 0.403 (0.403)	Data 0.367 (0.367)	Loss 2.947 (2.947)	Prec@1 22.656 (22.656)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [50/391]	Time 0.031 (0.039)	Data 0.001 (0.008)	Loss 0.877 (1.767)	Prec@1 71.094 (58.150)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [100/391]	Time 0.041 (0.047)	Data 0.002 (0.005)	Loss 0.827 (1.354)	Prec@1 74.219 (63.111)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [150/391]	Time 0.113 (0.046)	Data 0.001 (0.004)	Loss 0.858 (1.207)	Prec@1 71.875 (65.118)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [200/391]	Time 0.041 (0.046)	Data 0.002 (0.004)	Loss 1.016 (1.133)	Prec@1 69.531 (66.185)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [250/391]	Time 0.034 (0.044)	Data 0.001 (0.003)	Loss 0.834 (1.097)	Prec@1 71.094 (66.409)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [300/391]	Time 0.040 (0.044)	Data 0.003 (0.003)	Loss 1.263 (1.084)	Prec@1 58.594 (66.385)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [350/391]	Time 0.038 (0.044)	Data 0.001 (0.003)	Loss 0.774 (1.078)	Prec@1 71.875 (66.333)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Test: [0/79]	Time: 0.4060(0.4060)	Loss: 1.546(1.546)	Prec@1: 54.688(54.688)	
10-10-22 13:59:Test: [50/79]	Time: 0.0259(0.0275)	Loss: 1.659(1.612)	Prec@1: 57.031(56.219)	
10-10-22 13:59:Test: [78/79]	Time: 0.0223(0.0269)	Loss: 0.900(1.627)	Prec@1: 62.500(55.800)	
10-10-22 13:59:Step 90 * Prec@1 55.800
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [0/391]	Time 0.748 (0.748)	Data 0.692 (0.692)	Loss 2.768 (2.768)	Prec@1 25.781 (25.781)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [50/391]	Time 0.037 (0.054)	Data 0.003 (0.015)	Loss 0.948 (1.638)	Prec@1 62.500 (54.596)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [100/391]	Time 0.122 (0.051)	Data 0.002 (0.009)	Loss 1.063 (1.320)	Prec@1 65.625 (60.210)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [150/391]	Time 0.036 (0.047)	Data 0.002 (0.006)	Loss 0.970 (1.203)	Prec@1 68.750 (62.319)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [200/391]	Time 0.057 (0.047)	Data 0.003 (0.005)	Loss 1.258 (1.133)	Prec@1 57.812 (64.000)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [250/391]	Time 0.034 (0.045)	Data 0.001 (0.005)	Loss 0.946 (1.093)	Prec@1 67.969 (64.769)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [300/391]	Time 0.105 (0.044)	Data 0.005 (0.004)	Loss 1.028 (1.064)	Prec@1 64.062 (65.391)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [350/391]	Time 0.033 (0.046)	Data 0.001 (0.004)	Loss 1.210 (1.055)	Prec@1 60.938 (65.505)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Test: [0/79]	Time: 0.4165(0.4165)	Loss: 1.606(1.606)	Prec@1: 55.469(55.469)	
10-10-22 13:59:Test: [50/79]	Time: 0.0161(0.0246)	Loss: 1.724(1.798)	Prec@1: 46.875(53.324)	
10-10-22 13:59:Test: [78/79]	Time: 0.0151(0.0223)	Loss: 1.544(1.823)	Prec@1: 50.000(52.930)	
10-10-22 13:59:Step 91 * Prec@1 52.930
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [0/391]	Time 0.394 (0.394)	Data 0.355 (0.355)	Loss 4.240 (4.240)	Prec@1 17.969 (17.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [50/391]	Time 0.047 (0.050)	Data 0.001 (0.009)	Loss 0.960 (2.196)	Prec@1 67.969 (55.070)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [100/391]	Time 0.044 (0.049)	Data 0.001 (0.005)	Loss 0.803 (1.590)	Prec@1 71.875 (60.729)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [150/391]	Time 0.033 (0.045)	Data 0.001 (0.004)	Loss 1.148 (1.398)	Prec@1 63.281 (62.241)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [200/391]	Time 0.033 (0.042)	Data 0.001 (0.003)	Loss 1.005 (1.295)	Prec@1 65.625 (63.305)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [250/391]	Time 0.033 (0.040)	Data 0.001 (0.003)	Loss 0.983 (1.230)	Prec@1 66.406 (63.947)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 13:59:Num bit 8	Num grad bit 8	
10-10-22 13:59:Iter: [300/391]	Time 0.126 (0.042)	Data 0.005 (0.003)	Loss 1.157 (1.190)	Prec@1 64.844 (64.405)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [350/391]	Time 0.038 (0.042)	Data 0.003 (0.003)	Loss 1.158 (1.178)	Prec@1 62.500 (64.252)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Test: [0/79]	Time: 0.3571(0.3571)	Loss: 2.448(2.448)	Prec@1: 43.750(43.750)	
10-10-22 14:00:Test: [50/79]	Time: 0.0175(0.0315)	Loss: 2.539(2.484)	Prec@1: 43.750(44.516)	
10-10-22 14:00:Test: [78/79]	Time: 0.0150(0.0263)	Loss: 1.254(2.513)	Prec@1: 68.750(44.170)	
10-10-22 14:00:Step 92 * Prec@1 44.170
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [0/391]	Time 0.452 (0.452)	Data 0.413 (0.413)	Loss 3.621 (3.621)	Prec@1 22.656 (22.656)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [50/391]	Time 0.051 (0.048)	Data 0.003 (0.010)	Loss 1.193 (2.408)	Prec@1 56.250 (49.066)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [100/391]	Time 0.033 (0.046)	Data 0.002 (0.006)	Loss 1.271 (1.762)	Prec@1 54.688 (55.554)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [150/391]	Time 0.031 (0.043)	Data 0.001 (0.005)	Loss 1.226 (1.523)	Prec@1 60.156 (58.418)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [200/391]	Time 0.035 (0.041)	Data 0.001 (0.004)	Loss 1.049 (1.401)	Prec@1 64.844 (60.009)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [250/391]	Time 0.040 (0.044)	Data 0.002 (0.003)	Loss 0.931 (1.310)	Prec@1 70.312 (61.529)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [300/391]	Time 0.036 (0.045)	Data 0.002 (0.003)	Loss 1.087 (1.255)	Prec@1 61.719 (62.365)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [350/391]	Time 0.035 (0.045)	Data 0.001 (0.003)	Loss 1.291 (1.225)	Prec@1 61.719 (62.740)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Test: [0/79]	Time: 0.3504(0.3504)	Loss: 1.679(1.679)	Prec@1: 55.469(55.469)	
10-10-22 14:00:Test: [50/79]	Time: 0.0185(0.0256)	Loss: 1.661(1.909)	Prec@1: 53.125(49.816)	
10-10-22 14:00:Test: [78/79]	Time: 0.0299(0.0246)	Loss: 1.589(1.917)	Prec@1: 56.250(49.580)	
10-10-22 14:00:Step 93 * Prec@1 49.580
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [0/391]	Time 0.509 (0.509)	Data 0.461 (0.461)	Loss 4.217 (4.217)	Prec@1 21.094 (21.094)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [50/391]	Time 0.047 (0.057)	Data 0.002 (0.011)	Loss 1.361 (2.095)	Prec@1 54.688 (48.269)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [100/391]	Time 0.035 (0.048)	Data 0.001 (0.006)	Loss 1.127 (1.683)	Prec@1 61.719 (52.537)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [150/391]	Time 0.117 (0.046)	Data 0.005 (0.005)	Loss 1.199 (1.501)	Prec@1 61.719 (55.365)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [200/391]	Time 0.035 (0.049)	Data 0.001 (0.004)	Loss 1.095 (1.404)	Prec@1 60.938 (56.919)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [250/391]	Time 0.062 (0.048)	Data 0.001 (0.004)	Loss 1.025 (1.340)	Prec@1 60.938 (58.062)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [300/391]	Time 0.033 (0.046)	Data 0.001 (0.003)	Loss 1.146 (1.298)	Prec@1 63.281 (58.838)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [350/391]	Time 0.036 (0.046)	Data 0.002 (0.003)	Loss 1.068 (1.265)	Prec@1 60.156 (59.482)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Test: [0/79]	Time: 0.3652(0.3652)	Loss: 2.300(2.300)	Prec@1: 39.844(39.844)	
10-10-22 14:00:Test: [50/79]	Time: 0.0162(0.0259)	Loss: 2.557(2.340)	Prec@1: 39.062(46.599)	
10-10-22 14:00:Test: [78/79]	Time: 0.0148(0.0226)	Loss: 2.497(2.361)	Prec@1: 50.000(46.330)	
10-10-22 14:00:Step 94 * Prec@1 46.330
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [0/391]	Time 0.382 (0.382)	Data 0.347 (0.347)	Loss 4.535 (4.535)	Prec@1 21.875 (21.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [50/391]	Time 0.045 (0.052)	Data 0.002 (0.009)	Loss 1.426 (2.145)	Prec@1 54.688 (50.904)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [100/391]	Time 0.084 (0.053)	Data 0.003 (0.005)	Loss 1.102 (1.723)	Prec@1 58.594 (53.519)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [150/391]	Time 0.036 (0.054)	Data 0.001 (0.004)	Loss 1.248 (1.565)	Prec@1 56.250 (54.900)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [200/391]	Time 0.060 (0.051)	Data 0.003 (0.004)	Loss 1.443 (1.490)	Prec@1 51.562 (55.453)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [250/391]	Time 0.031 (0.049)	Data 0.001 (0.003)	Loss 1.374 (1.454)	Prec@1 52.344 (55.494)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [300/391]	Time 0.035 (0.046)	Data 0.001 (0.003)	Loss 1.349 (1.437)	Prec@1 54.688 (55.430)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:00:Num bit 8	Num grad bit 8	
10-10-22 14:00:Iter: [350/391]	Time 0.061 (0.045)	Data 0.003 (0.003)	Loss 1.154 (1.437)	Prec@1 60.156 (55.050)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Test: [0/79]	Time: 0.3652(0.3652)	Loss: 2.328(2.328)	Prec@1: 35.938(35.938)	
10-10-22 14:01:Test: [50/79]	Time: 0.0163(0.0234)	Loss: 2.190(2.240)	Prec@1: 29.688(40.640)	
10-10-22 14:01:Test: [78/79]	Time: 0.0153(0.0214)	Loss: 1.500(2.272)	Prec@1: 50.000(40.030)	
10-10-22 14:01:Step 95 * Prec@1 40.030
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [0/391]	Time 0.790 (0.790)	Data 0.754 (0.754)	Loss 3.858 (3.858)	Prec@1 24.219 (24.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [50/391]	Time 0.037 (0.054)	Data 0.002 (0.016)	Loss 1.314 (1.943)	Prec@1 51.562 (46.538)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [100/391]	Time 0.062 (0.048)	Data 0.003 (0.009)	Loss 1.132 (1.642)	Prec@1 64.062 (50.456)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [150/391]	Time 0.041 (0.047)	Data 0.002 (0.007)	Loss 1.047 (1.525)	Prec@1 55.469 (51.992)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [200/391]	Time 0.035 (0.046)	Data 0.001 (0.006)	Loss 1.412 (1.470)	Prec@1 59.375 (52.868)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [250/391]	Time 0.094 (0.046)	Data 0.005 (0.005)	Loss 1.013 (1.428)	Prec@1 62.500 (53.660)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [300/391]	Time 0.045 (0.048)	Data 0.001 (0.004)	Loss 1.236 (1.414)	Prec@1 55.469 (53.883)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [350/391]	Time 0.039 (0.048)	Data 0.002 (0.004)	Loss 1.841 (1.409)	Prec@1 48.438 (54.038)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Test: [0/79]	Time: 0.4058(0.4058)	Loss: 2.502(2.502)	Prec@1: 41.406(41.406)	
10-10-22 14:01:Test: [50/79]	Time: 0.0164(0.0258)	Loss: 2.723(2.513)	Prec@1: 35.938(42.096)	
10-10-22 14:01:Test: [78/79]	Time: 0.0150(0.0224)	Loss: 2.196(2.533)	Prec@1: 50.000(41.800)	
10-10-22 14:01:Step 96 * Prec@1 41.800
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [0/391]	Time 0.569 (0.569)	Data 0.532 (0.532)	Loss 3.315 (3.315)	Prec@1 20.312 (20.312)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [50/391]	Time 0.048 (0.056)	Data 0.001 (0.012)	Loss 1.158 (2.487)	Prec@1 56.250 (45.052)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [100/391]	Time 0.042 (0.049)	Data 0.001 (0.007)	Loss 1.125 (1.883)	Prec@1 57.812 (50.549)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [150/391]	Time 0.033 (0.044)	Data 0.001 (0.005)	Loss 1.400 (1.680)	Prec@1 53.125 (52.468)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [200/391]	Time 0.046 (0.047)	Data 0.002 (0.005)	Loss 1.266 (1.589)	Prec@1 55.469 (53.242)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [250/391]	Time 0.035 (0.045)	Data 0.001 (0.004)	Loss 1.413 (1.528)	Prec@1 49.219 (53.822)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [300/391]	Time 0.037 (0.045)	Data 0.002 (0.004)	Loss 0.844 (1.514)	Prec@1 68.750 (53.701)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [350/391]	Time 0.040 (0.045)	Data 0.002 (0.003)	Loss 1.210 (1.514)	Prec@1 57.031 (53.635)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Test: [0/79]	Time: 0.3842(0.3842)	Loss: 3.194(3.194)	Prec@1: 35.156(35.156)	
10-10-22 14:01:Test: [50/79]	Time: 0.0160(0.0251)	Loss: 2.996(3.131)	Prec@1: 29.688(35.662)	
10-10-22 14:01:Test: [78/79]	Time: 0.0150(0.0219)	Loss: 2.890(3.150)	Prec@1: 56.250(35.290)	
10-10-22 14:01:Step 97 * Prec@1 35.290
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [0/391]	Time 0.387 (0.387)	Data 0.344 (0.344)	Loss 4.643 (4.643)	Prec@1 21.875 (21.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [50/391]	Time 0.048 (0.053)	Data 0.002 (0.008)	Loss 1.352 (2.852)	Prec@1 56.250 (42.065)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [100/391]	Time 0.032 (0.048)	Data 0.001 (0.005)	Loss 1.560 (2.129)	Prec@1 50.000 (47.146)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [150/391]	Time 0.038 (0.047)	Data 0.002 (0.004)	Loss 1.192 (1.861)	Prec@1 61.719 (49.503)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [200/391]	Time 0.047 (0.045)	Data 0.001 (0.004)	Loss 1.292 (1.719)	Prec@1 54.688 (50.995)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [250/391]	Time 0.049 (0.045)	Data 0.001 (0.003)	Loss 1.127 (1.643)	Prec@1 62.500 (51.737)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [300/391]	Time 0.055 (0.046)	Data 0.002 (0.003)	Loss 1.272 (1.599)	Prec@1 53.125 (52.074)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:01:Num bit 8	Num grad bit 8	
10-10-22 14:01:Iter: [350/391]	Time 0.036 (0.045)	Data 0.001 (0.003)	Loss 1.681 (1.575)	Prec@1 57.031 (52.270)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Test: [0/79]	Time: 0.4141(0.4141)	Loss: 3.356(3.356)	Prec@1: 32.812(32.812)	
10-10-22 14:02:Test: [50/79]	Time: 0.0209(0.0269)	Loss: 3.182(3.363)	Prec@1: 28.125(32.782)	
10-10-22 14:02:Test: [78/79]	Time: 0.0149(0.0236)	Loss: 1.827(3.368)	Prec@1: 37.500(32.510)	
10-10-22 14:02:Step 98 * Prec@1 32.510
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [0/391]	Time 0.523 (0.523)	Data 0.361 (0.361)	Loss 5.852 (5.852)	Prec@1 15.625 (15.625)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [50/391]	Time 0.037 (0.066)	Data 0.004 (0.010)	Loss 1.380 (2.541)	Prec@1 47.656 (42.479)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [100/391]	Time 0.035 (0.051)	Data 0.002 (0.006)	Loss 1.365 (1.935)	Prec@1 54.688 (48.043)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [150/391]	Time 0.037 (0.048)	Data 0.002 (0.005)	Loss 1.071 (1.747)	Prec@1 62.500 (49.943)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [200/391]	Time 0.034 (0.046)	Data 0.002 (0.004)	Loss 1.426 (1.663)	Prec@1 53.906 (50.447)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [250/391]	Time 0.032 (0.045)	Data 0.001 (0.003)	Loss 1.348 (1.615)	Prec@1 50.000 (50.868)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [300/391]	Time 0.032 (0.045)	Data 0.001 (0.003)	Loss 1.701 (1.588)	Prec@1 44.531 (51.088)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [350/391]	Time 0.034 (0.044)	Data 0.001 (0.003)	Loss 1.460 (1.589)	Prec@1 52.344 (51.015)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Test: [0/79]	Time: 0.3163(0.3163)	Loss: 4.387(4.387)	Prec@1: 17.188(17.188)	
10-10-22 14:02:Test: [50/79]	Time: 0.0162(0.0220)	Loss: 4.367(4.381)	Prec@1: 25.000(25.061)	
10-10-22 14:02:Test: [78/79]	Time: 0.0149(0.0210)	Loss: 4.458(4.384)	Prec@1: 31.250(25.250)	
10-10-22 14:02:Step 99 * Prec@1 25.250
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [0/391]	Time 0.379 (0.379)	Data 0.327 (0.327)	Loss 5.472 (5.472)	Prec@1 17.969 (17.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [50/391]	Time 0.097 (0.045)	Data 0.004 (0.008)	Loss 1.327 (1.697)	Prec@1 52.344 (44.301)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [100/391]	Time 0.051 (0.050)	Data 0.003 (0.005)	Loss 1.222 (1.516)	Prec@1 56.250 (48.538)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [150/391]	Time 0.034 (0.045)	Data 0.001 (0.004)	Loss 1.467 (1.444)	Prec@1 49.219 (50.419)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [200/391]	Time 0.036 (0.046)	Data 0.001 (0.004)	Loss 1.294 (1.399)	Prec@1 53.125 (51.656)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [250/391]	Time 0.039 (0.044)	Data 0.002 (0.003)	Loss 1.160 (1.367)	Prec@1 57.031 (52.615)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [300/391]	Time 0.052 (0.044)	Data 0.003 (0.003)	Loss 1.358 (1.346)	Prec@1 55.469 (53.182)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [350/391]	Time 0.036 (0.045)	Data 0.002 (0.003)	Loss 1.143 (1.329)	Prec@1 55.469 (53.690)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Test: [0/79]	Time: 0.3143(0.3143)	Loss: 3.503(3.503)	Prec@1: 25.000(25.000)	
10-10-22 14:02:Test: [50/79]	Time: 0.0161(0.0221)	Loss: 3.196(3.188)	Prec@1: 25.781(29.596)	
10-10-22 14:02:Test: [78/79]	Time: 0.0205(0.0210)	Loss: 3.218(3.196)	Prec@1: 31.250(29.480)	
10-10-22 14:02:Step 100 * Prec@1 29.480
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [0/391]	Time 0.493 (0.493)	Data 0.393 (0.393)	Loss 2.846 (2.846)	Prec@1 21.875 (21.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [50/391]	Time 0.043 (0.067)	Data 0.002 (0.010)	Loss 1.388 (1.551)	Prec@1 50.000 (46.722)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [100/391]	Time 0.037 (0.058)	Data 0.002 (0.006)	Loss 1.354 (1.512)	Prec@1 51.562 (46.860)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [150/391]	Time 0.034 (0.052)	Data 0.001 (0.005)	Loss 1.267 (1.473)	Prec@1 55.469 (47.910)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [200/391]	Time 0.051 (0.051)	Data 0.002 (0.004)	Loss 1.374 (1.449)	Prec@1 53.125 (48.698)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [250/391]	Time 0.035 (0.048)	Data 0.001 (0.003)	Loss 1.251 (1.424)	Prec@1 54.688 (49.645)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [300/391]	Time 0.036 (0.046)	Data 0.001 (0.003)	Loss 1.259 (1.408)	Prec@1 58.594 (50.202)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Num bit 8	Num grad bit 8	
10-10-22 14:02:Iter: [350/391]	Time 0.056 (0.048)	Data 0.002 (0.003)	Loss 1.190 (1.394)	Prec@1 55.469 (50.775)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:02:Test: [0/79]	Time: 0.3361(0.3361)	Loss: 2.994(2.994)	Prec@1: 26.562(26.562)	
10-10-22 14:03:Test: [50/79]	Time: 0.0166(0.0330)	Loss: 2.923(2.986)	Prec@1: 20.312(30.637)	
10-10-22 14:03:Test: [78/79]	Time: 0.0150(0.0280)	Loss: 2.494(2.983)	Prec@1: 25.000(30.230)	
10-10-22 14:03:Step 101 * Prec@1 30.230
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [0/391]	Time 0.348 (0.348)	Data 0.312 (0.312)	Loss 2.555 (2.555)	Prec@1 27.344 (27.344)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [50/391]	Time 0.037 (0.053)	Data 0.002 (0.008)	Loss 1.265 (1.550)	Prec@1 57.031 (46.676)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [100/391]	Time 0.061 (0.053)	Data 0.003 (0.005)	Loss 1.209 (1.439)	Prec@1 57.812 (49.776)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [150/391]	Time 0.036 (0.050)	Data 0.001 (0.004)	Loss 1.370 (1.401)	Prec@1 56.250 (50.797)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [200/391]	Time 0.041 (0.054)	Data 0.001 (0.004)	Loss 1.371 (1.369)	Prec@1 50.781 (51.850)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [250/391]	Time 0.043 (0.053)	Data 0.003 (0.003)	Loss 1.215 (1.343)	Prec@1 53.906 (52.773)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [300/391]	Time 0.045 (0.052)	Data 0.002 (0.003)	Loss 1.291 (1.325)	Prec@1 54.688 (53.496)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [350/391]	Time 0.037 (0.051)	Data 0.002 (0.003)	Loss 1.217 (1.313)	Prec@1 59.375 (53.835)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Test: [0/79]	Time: 0.2894(0.2894)	Loss: 3.143(3.143)	Prec@1: 28.125(28.125)	
10-10-22 14:03:Test: [50/79]	Time: 0.0160(0.0217)	Loss: 3.060(3.044)	Prec@1: 25.781(31.740)	
10-10-22 14:03:Test: [78/79]	Time: 0.0148(0.0197)	Loss: 2.373(3.052)	Prec@1: 25.000(31.410)	
10-10-22 14:03:Step 102 * Prec@1 31.410
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [0/391]	Time 0.368 (0.368)	Data 0.331 (0.331)	Loss 2.626 (2.626)	Prec@1 24.219 (24.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [50/391]	Time 0.082 (0.050)	Data 0.002 (0.008)	Loss 1.295 (1.504)	Prec@1 49.219 (47.365)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [100/391]	Time 0.036 (0.048)	Data 0.001 (0.005)	Loss 1.512 (1.420)	Prec@1 50.000 (50.015)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [150/391]	Time 0.037 (0.047)	Data 0.002 (0.004)	Loss 1.249 (1.387)	Prec@1 59.375 (51.081)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [200/391]	Time 0.094 (0.046)	Data 0.002 (0.003)	Loss 1.362 (1.366)	Prec@1 49.219 (51.640)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [250/391]	Time 0.034 (0.046)	Data 0.002 (0.003)	Loss 1.411 (1.350)	Prec@1 51.562 (52.179)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [300/391]	Time 0.033 (0.044)	Data 0.001 (0.003)	Loss 1.237 (1.335)	Prec@1 51.562 (52.689)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [350/391]	Time 0.034 (0.043)	Data 0.001 (0.003)	Loss 1.356 (1.325)	Prec@1 51.562 (52.976)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Test: [0/79]	Time: 0.2677(0.2677)	Loss: 2.916(2.916)	Prec@1: 29.688(29.688)	
10-10-22 14:03:Test: [50/79]	Time: 0.0162(0.0212)	Loss: 2.888(2.839)	Prec@1: 25.000(31.556)	
10-10-22 14:03:Test: [78/79]	Time: 0.0148(0.0195)	Loss: 2.106(2.858)	Prec@1: 31.250(31.040)	
10-10-22 14:03:Step 103 * Prec@1 31.040
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [0/391]	Time 0.383 (0.383)	Data 0.346 (0.346)	Loss 2.294 (2.294)	Prec@1 28.906 (28.906)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [50/391]	Time 0.052 (0.054)	Data 0.003 (0.009)	Loss 1.223 (1.475)	Prec@1 61.719 (48.744)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [100/391]	Time 0.055 (0.052)	Data 0.003 (0.005)	Loss 1.195 (1.390)	Prec@1 56.250 (51.145)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [150/391]	Time 0.043 (0.054)	Data 0.002 (0.004)	Loss 1.388 (1.351)	Prec@1 48.438 (52.427)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [200/391]	Time 0.038 (0.050)	Data 0.001 (0.004)	Loss 1.197 (1.334)	Prec@1 58.594 (52.717)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [250/391]	Time 0.045 (0.049)	Data 0.001 (0.003)	Loss 1.264 (1.318)	Prec@1 57.031 (53.383)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:03:Num bit 8	Num grad bit 8	
10-10-22 14:03:Iter: [300/391]	Time 0.059 (0.051)	Data 0.002 (0.003)	Loss 1.283 (1.309)	Prec@1 50.781 (53.561)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [350/391]	Time 0.034 (0.049)	Data 0.001 (0.003)	Loss 1.327 (1.302)	Prec@1 47.656 (53.786)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Test: [0/79]	Time: 0.3032(0.3032)	Loss: 3.442(3.442)	Prec@1: 28.125(28.125)	
10-10-22 14:04:Test: [50/79]	Time: 0.0164(0.0265)	Loss: 3.232(3.279)	Prec@1: 24.219(30.729)	
10-10-22 14:04:Test: [78/79]	Time: 0.0153(0.0245)	Loss: 2.222(3.289)	Prec@1: 37.500(30.250)	
10-10-22 14:04:Step 104 * Prec@1 30.250
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [0/391]	Time 0.362 (0.362)	Data 0.310 (0.310)	Loss 2.565 (2.565)	Prec@1 23.438 (23.438)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [50/391]	Time 0.034 (0.052)	Data 0.001 (0.008)	Loss 1.453 (1.460)	Prec@1 53.906 (48.162)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [100/391]	Time 0.035 (0.048)	Data 0.002 (0.005)	Loss 1.342 (1.382)	Prec@1 53.125 (51.021)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [150/391]	Time 0.033 (0.043)	Data 0.001 (0.004)	Loss 1.203 (1.342)	Prec@1 60.938 (52.442)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [200/391]	Time 0.090 (0.045)	Data 0.001 (0.003)	Loss 1.252 (1.330)	Prec@1 57.812 (52.958)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [250/391]	Time 0.037 (0.045)	Data 0.002 (0.003)	Loss 1.124 (1.314)	Prec@1 60.938 (53.405)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [300/391]	Time 0.046 (0.045)	Data 0.001 (0.003)	Loss 1.304 (1.304)	Prec@1 50.000 (53.678)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [350/391]	Time 0.035 (0.045)	Data 0.001 (0.003)	Loss 1.236 (1.298)	Prec@1 53.906 (53.813)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Test: [0/79]	Time: 0.3215(0.3215)	Loss: 3.251(3.251)	Prec@1: 25.000(25.000)	
10-10-22 14:04:Test: [50/79]	Time: 0.0280(0.0286)	Loss: 3.199(3.200)	Prec@1: 24.219(29.948)	
10-10-22 14:04:Test: [78/79]	Time: 0.0221(0.0254)	Loss: 2.153(3.220)	Prec@1: 31.250(29.610)	
10-10-22 14:04:Step 105 * Prec@1 29.610
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [0/391]	Time 0.394 (0.394)	Data 0.355 (0.355)	Loss 2.629 (2.629)	Prec@1 22.656 (22.656)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [50/391]	Time 0.034 (0.044)	Data 0.001 (0.008)	Loss 1.362 (1.480)	Prec@1 46.875 (48.024)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [100/391]	Time 0.031 (0.039)	Data 0.001 (0.005)	Loss 1.212 (1.366)	Prec@1 55.469 (51.795)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [150/391]	Time 0.039 (0.039)	Data 0.002 (0.004)	Loss 1.306 (1.324)	Prec@1 49.219 (53.166)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [200/391]	Time 0.038 (0.045)	Data 0.002 (0.004)	Loss 1.548 (1.308)	Prec@1 48.438 (53.541)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [250/391]	Time 0.036 (0.045)	Data 0.002 (0.003)	Loss 1.209 (1.299)	Prec@1 57.812 (54.018)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [300/391]	Time 0.035 (0.043)	Data 0.001 (0.003)	Loss 1.199 (1.288)	Prec@1 59.375 (54.319)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [350/391]	Time 0.047 (0.044)	Data 0.001 (0.003)	Loss 1.110 (1.283)	Prec@1 57.031 (54.498)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Test: [0/79]	Time: 0.4130(0.4130)	Loss: 3.357(3.357)	Prec@1: 28.906(28.906)	
10-10-22 14:04:Test: [50/79]	Time: 0.0167(0.0268)	Loss: 3.158(3.080)	Prec@1: 25.000(32.996)	
10-10-22 14:04:Test: [78/79]	Time: 0.0182(0.0241)	Loss: 2.207(3.098)	Prec@1: 31.250(32.450)	
10-10-22 14:04:Step 106 * Prec@1 32.450
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [0/391]	Time 0.395 (0.395)	Data 0.349 (0.349)	Loss 2.438 (2.438)	Prec@1 25.000 (25.000)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [50/391]	Time 0.034 (0.052)	Data 0.001 (0.009)	Loss 1.376 (1.426)	Prec@1 51.562 (50.092)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [100/391]	Time 0.064 (0.044)	Data 0.003 (0.005)	Loss 1.380 (1.346)	Prec@1 52.344 (52.800)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [150/391]	Time 0.054 (0.052)	Data 0.002 (0.004)	Loss 1.349 (1.317)	Prec@1 51.562 (53.689)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [200/391]	Time 0.045 (0.051)	Data 0.002 (0.004)	Loss 1.177 (1.293)	Prec@1 60.156 (54.509)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [250/391]	Time 0.037 (0.051)	Data 0.001 (0.003)	Loss 1.203 (1.278)	Prec@1 64.062 (54.980)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:04:Num bit 8	Num grad bit 8	
10-10-22 14:04:Iter: [300/391]	Time 0.037 (0.049)	Data 0.002 (0.003)	Loss 1.177 (1.270)	Prec@1 53.906 (55.230)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [350/391]	Time 0.036 (0.049)	Data 0.002 (0.003)	Loss 1.249 (1.267)	Prec@1 57.812 (55.340)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Test: [0/79]	Time: 0.3135(0.3135)	Loss: 2.769(2.769)	Prec@1: 32.812(32.812)	
10-10-22 14:05:Test: [50/79]	Time: 0.0160(0.0225)	Loss: 2.747(2.633)	Prec@1: 30.469(37.377)	
10-10-22 14:05:Test: [78/79]	Time: 0.0147(0.0202)	Loss: 1.510(2.638)	Prec@1: 50.000(36.810)	
10-10-22 14:05:Step 107 * Prec@1 36.810
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [0/391]	Time 0.310 (0.310)	Data 0.268 (0.268)	Loss 2.592 (2.592)	Prec@1 26.562 (26.562)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [50/391]	Time 0.066 (0.054)	Data 0.002 (0.007)	Loss 1.304 (1.434)	Prec@1 56.250 (50.368)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [100/391]	Time 0.050 (0.056)	Data 0.004 (0.005)	Loss 0.993 (1.363)	Prec@1 64.844 (52.823)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [150/391]	Time 0.034 (0.052)	Data 0.001 (0.004)	Loss 1.057 (1.323)	Prec@1 63.281 (54.005)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [200/391]	Time 0.046 (0.050)	Data 0.002 (0.003)	Loss 1.244 (1.313)	Prec@1 57.031 (54.233)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [250/391]	Time 0.053 (0.050)	Data 0.002 (0.003)	Loss 1.314 (1.305)	Prec@1 50.781 (54.258)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [300/391]	Time 0.038 (0.049)	Data 0.002 (0.003)	Loss 1.207 (1.293)	Prec@1 57.031 (54.719)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [350/391]	Time 0.038 (0.047)	Data 0.002 (0.003)	Loss 1.202 (1.293)	Prec@1 57.812 (54.623)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Test: [0/79]	Time: 0.3001(0.3001)	Loss: 3.254(3.254)	Prec@1: 35.156(35.156)	
10-10-22 14:05:Test: [50/79]	Time: 0.0162(0.0230)	Loss: 2.961(3.026)	Prec@1: 26.562(32.690)	
10-10-22 14:05:Test: [78/79]	Time: 0.0147(0.0206)	Loss: 2.080(3.027)	Prec@1: 31.250(32.580)	
10-10-22 14:05:Step 108 * Prec@1 32.580
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [0/391]	Time 0.317 (0.317)	Data 0.282 (0.282)	Loss 2.256 (2.256)	Prec@1 24.219 (24.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [50/391]	Time 0.056 (0.056)	Data 0.002 (0.008)	Loss 1.228 (1.451)	Prec@1 51.562 (48.376)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [100/391]	Time 0.031 (0.058)	Data 0.001 (0.005)	Loss 1.318 (1.352)	Prec@1 53.125 (52.205)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [150/391]	Time 0.052 (0.056)	Data 0.002 (0.004)	Loss 1.032 (1.318)	Prec@1 65.625 (53.404)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [200/391]	Time 0.045 (0.054)	Data 0.001 (0.003)	Loss 1.248 (1.305)	Prec@1 56.250 (53.914)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [250/391]	Time 0.061 (0.053)	Data 0.002 (0.003)	Loss 1.445 (1.294)	Prec@1 46.094 (54.361)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [300/391]	Time 0.037 (0.051)	Data 0.001 (0.003)	Loss 1.288 (1.286)	Prec@1 53.906 (54.537)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [350/391]	Time 0.045 (0.050)	Data 0.003 (0.003)	Loss 1.256 (1.278)	Prec@1 53.906 (54.768)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Test: [0/79]	Time: 0.3243(0.3243)	Loss: 4.444(4.444)	Prec@1: 25.781(25.781)	
10-10-22 14:05:Test: [50/79]	Time: 0.0201(0.0268)	Loss: 4.191(4.192)	Prec@1: 18.750(25.643)	
10-10-22 14:05:Test: [78/79]	Time: 0.0248(0.0245)	Loss: 2.855(4.206)	Prec@1: 43.750(25.210)	
10-10-22 14:05:Step 109 * Prec@1 25.210
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [0/391]	Time 0.405 (0.405)	Data 0.347 (0.347)	Loss 2.409 (2.409)	Prec@1 27.344 (27.344)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [50/391]	Time 0.041 (0.053)	Data 0.002 (0.009)	Loss 1.273 (1.427)	Prec@1 57.031 (50.934)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [100/391]	Time 0.041 (0.052)	Data 0.002 (0.005)	Loss 1.157 (1.351)	Prec@1 58.594 (52.576)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [150/391]	Time 0.037 (0.049)	Data 0.002 (0.004)	Loss 1.217 (1.314)	Prec@1 53.125 (53.865)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [200/391]	Time 0.042 (0.048)	Data 0.002 (0.003)	Loss 1.450 (1.309)	Prec@1 48.438 (54.116)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [250/391]	Time 0.066 (0.048)	Data 0.003 (0.003)	Loss 1.513 (1.296)	Prec@1 51.562 (54.582)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:05:Num bit 8	Num grad bit 8	
10-10-22 14:05:Iter: [300/391]	Time 0.031 (0.049)	Data 0.001 (0.003)	Loss 1.048 (1.288)	Prec@1 60.156 (54.802)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [350/391]	Time 0.032 (0.048)	Data 0.001 (0.003)	Loss 1.221 (1.278)	Prec@1 56.250 (55.106)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Test: [0/79]	Time: 0.2902(0.2902)	Loss: 4.686(4.686)	Prec@1: 22.656(22.656)	
10-10-22 14:06:Test: [50/79]	Time: 0.0164(0.0229)	Loss: 4.404(4.509)	Prec@1: 27.344(24.525)	
10-10-22 14:06:Test: [78/79]	Time: 0.0181(0.0210)	Loss: 3.895(4.503)	Prec@1: 12.500(24.250)	
10-10-22 14:06:Step 110 * Prec@1 24.250
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [0/391]	Time 0.391 (0.391)	Data 0.330 (0.330)	Loss 2.507 (2.507)	Prec@1 24.219 (24.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [50/391]	Time 0.043 (0.049)	Data 0.002 (0.008)	Loss 1.328 (1.537)	Prec@1 51.562 (47.396)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [100/391]	Time 0.048 (0.046)	Data 0.002 (0.005)	Loss 1.625 (1.458)	Prec@1 41.406 (49.528)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [150/391]	Time 0.051 (0.045)	Data 0.002 (0.004)	Loss 1.526 (1.419)	Prec@1 53.906 (50.730)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [200/391]	Time 0.044 (0.046)	Data 0.001 (0.003)	Loss 1.141 (1.388)	Prec@1 58.594 (51.664)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [250/391]	Time 0.039 (0.046)	Data 0.003 (0.003)	Loss 1.305 (1.379)	Prec@1 56.250 (51.980)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [300/391]	Time 0.045 (0.046)	Data 0.001 (0.003)	Loss 1.247 (1.369)	Prec@1 59.375 (52.302)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [350/391]	Time 0.033 (0.047)	Data 0.001 (0.003)	Loss 1.246 (1.365)	Prec@1 51.562 (52.557)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Test: [0/79]	Time: 0.2968(0.2968)	Loss: 4.262(4.262)	Prec@1: 28.125(28.125)	
10-10-22 14:06:Test: [50/79]	Time: 0.0161(0.0219)	Loss: 3.901(4.110)	Prec@1: 30.469(30.484)	
10-10-22 14:06:Test: [78/79]	Time: 0.0149(0.0199)	Loss: 2.196(4.105)	Prec@1: 31.250(30.220)	
10-10-22 14:06:Step 111 * Prec@1 30.220
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [0/391]	Time 0.359 (0.359)	Data 0.324 (0.324)	Loss 2.582 (2.582)	Prec@1 26.562 (26.562)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [50/391]	Time 0.037 (0.054)	Data 0.001 (0.008)	Loss 1.360 (1.459)	Prec@1 53.125 (48.989)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [100/391]	Time 0.036 (0.049)	Data 0.001 (0.005)	Loss 1.265 (1.362)	Prec@1 52.344 (52.027)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [150/391]	Time 0.041 (0.047)	Data 0.001 (0.004)	Loss 1.072 (1.328)	Prec@1 64.062 (53.461)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [200/391]	Time 0.063 (0.046)	Data 0.002 (0.003)	Loss 1.203 (1.325)	Prec@1 60.156 (53.848)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [250/391]	Time 0.039 (0.045)	Data 0.001 (0.003)	Loss 1.553 (1.321)	Prec@1 53.906 (54.087)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [300/391]	Time 0.035 (0.044)	Data 0.001 (0.003)	Loss 1.325 (1.320)	Prec@1 54.688 (54.145)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [350/391]	Time 0.033 (0.043)	Data 0.001 (0.003)	Loss 1.263 (1.319)	Prec@1 62.500 (54.311)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Test: [0/79]	Time: 0.3006(0.3006)	Loss: 5.334(5.334)	Prec@1: 21.094(21.094)	
10-10-22 14:06:Test: [50/79]	Time: 0.0163(0.0224)	Loss: 5.035(5.434)	Prec@1: 18.750(23.912)	
10-10-22 14:06:Test: [78/79]	Time: 0.0147(0.0202)	Loss: 3.461(5.420)	Prec@1: 12.500(23.960)	
10-10-22 14:06:Step 112 * Prec@1 23.960
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [0/391]	Time 0.366 (0.366)	Data 0.332 (0.332)	Loss 2.697 (2.697)	Prec@1 21.875 (21.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [50/391]	Time 0.053 (0.048)	Data 0.002 (0.008)	Loss 1.273 (1.505)	Prec@1 58.594 (49.280)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [100/391]	Time 0.035 (0.048)	Data 0.002 (0.005)	Loss 1.510 (1.433)	Prec@1 56.250 (51.137)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [150/391]	Time 0.055 (0.051)	Data 0.002 (0.004)	Loss 1.398 (1.412)	Prec@1 57.812 (51.925)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [200/391]	Time 0.032 (0.048)	Data 0.002 (0.004)	Loss 1.278 (1.403)	Prec@1 58.594 (52.149)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [250/391]	Time 0.036 (0.046)	Data 0.002 (0.003)	Loss 1.282 (1.396)	Prec@1 55.469 (52.384)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:06:Num bit 8	Num grad bit 8	
10-10-22 14:06:Iter: [300/391]	Time 0.036 (0.044)	Data 0.001 (0.003)	Loss 1.036 (1.380)	Prec@1 58.594 (52.793)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [350/391]	Time 0.053 (0.047)	Data 0.002 (0.003)	Loss 1.394 (1.376)	Prec@1 54.688 (52.976)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Test: [0/79]	Time: 0.3036(0.3036)	Loss: 4.240(4.240)	Prec@1: 22.656(22.656)	
10-10-22 14:07:Test: [50/79]	Time: 0.0260(0.0263)	Loss: 4.117(4.300)	Prec@1: 22.656(27.589)	
10-10-22 14:07:Test: [78/79]	Time: 0.0151(0.0242)	Loss: 2.268(4.285)	Prec@1: 43.750(27.650)	
10-10-22 14:07:Step 113 * Prec@1 27.650
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [0/391]	Time 0.357 (0.357)	Data 0.317 (0.317)	Loss 2.738 (2.738)	Prec@1 20.312 (20.312)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [50/391]	Time 0.048 (0.050)	Data 0.001 (0.008)	Loss 1.376 (1.491)	Prec@1 52.344 (49.326)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [100/391]	Time 0.038 (0.045)	Data 0.002 (0.005)	Loss 1.892 (1.423)	Prec@1 38.281 (51.346)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [150/391]	Time 0.044 (0.045)	Data 0.003 (0.004)	Loss 1.082 (1.392)	Prec@1 60.156 (52.520)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [200/391]	Time 0.043 (0.044)	Data 0.001 (0.003)	Loss 1.446 (1.378)	Prec@1 50.000 (53.063)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [250/391]	Time 0.034 (0.043)	Data 0.001 (0.003)	Loss 1.172 (1.372)	Prec@1 56.250 (53.228)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [300/391]	Time 0.057 (0.046)	Data 0.003 (0.003)	Loss 1.366 (1.368)	Prec@1 53.125 (53.468)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [350/391]	Time 0.035 (0.045)	Data 0.001 (0.003)	Loss 1.476 (1.357)	Prec@1 53.125 (53.793)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Test: [0/79]	Time: 0.3375(0.3375)	Loss: 4.048(4.048)	Prec@1: 36.719(36.719)	
10-10-22 14:07:Test: [50/79]	Time: 0.0372(0.0257)	Loss: 3.836(4.144)	Prec@1: 28.906(32.721)	
10-10-22 14:07:Test: [78/79]	Time: 0.0152(0.0232)	Loss: 2.268(4.107)	Prec@1: 50.000(33.010)	
10-10-22 14:07:Step 114 * Prec@1 33.010
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [0/391]	Time 0.357 (0.357)	Data 0.317 (0.317)	Loss 2.681 (2.681)	Prec@1 14.062 (14.062)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [50/391]	Time 0.057 (0.051)	Data 0.002 (0.008)	Loss 1.631 (1.470)	Prec@1 46.094 (49.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [100/391]	Time 0.045 (0.053)	Data 0.002 (0.005)	Loss 1.060 (1.416)	Prec@1 59.375 (51.624)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [150/391]	Time 0.040 (0.049)	Data 0.001 (0.004)	Loss 1.152 (1.388)	Prec@1 59.375 (52.706)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [200/391]	Time 0.032 (0.045)	Data 0.001 (0.003)	Loss 1.401 (1.376)	Prec@1 48.438 (53.164)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [250/391]	Time 0.035 (0.043)	Data 0.001 (0.003)	Loss 1.585 (1.371)	Prec@1 53.125 (53.499)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [300/391]	Time 0.033 (0.044)	Data 0.000 (0.003)	Loss 1.455 (1.368)	Prec@1 48.438 (53.582)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [350/391]	Time 0.036 (0.043)	Data 0.002 (0.003)	Loss 1.568 (1.364)	Prec@1 51.562 (53.612)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Test: [0/79]	Time: 0.3363(0.3363)	Loss: 3.833(3.833)	Prec@1: 35.156(35.156)	
10-10-22 14:07:Test: [50/79]	Time: 0.0263(0.0263)	Loss: 3.649(3.988)	Prec@1: 32.812(34.513)	
10-10-22 14:07:Test: [78/79]	Time: 0.0152(0.0243)	Loss: 2.328(3.958)	Prec@1: 43.750(34.810)	
10-10-22 14:07:Step 115 * Prec@1 34.810
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [0/391]	Time 0.384 (0.384)	Data 0.345 (0.345)	Loss 2.469 (2.469)	Prec@1 25.781 (25.781)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [50/391]	Time 0.036 (0.045)	Data 0.002 (0.008)	Loss 1.245 (1.536)	Prec@1 54.688 (48.882)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [100/391]	Time 0.060 (0.045)	Data 0.004 (0.005)	Loss 1.215 (1.437)	Prec@1 58.594 (52.375)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [150/391]	Time 0.033 (0.042)	Data 0.002 (0.004)	Loss 1.189 (1.393)	Prec@1 57.812 (53.591)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [200/391]	Time 0.034 (0.041)	Data 0.001 (0.003)	Loss 1.476 (1.390)	Prec@1 52.344 (53.914)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [250/391]	Time 0.066 (0.043)	Data 0.003 (0.003)	Loss 1.256 (1.396)	Prec@1 57.812 (53.878)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [300/391]	Time 0.031 (0.045)	Data 0.001 (0.003)	Loss 1.082 (1.391)	Prec@1 66.406 (54.257)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:07:Num bit 8	Num grad bit 8	
10-10-22 14:07:Iter: [350/391]	Time 0.042 (0.044)	Data 0.001 (0.003)	Loss 1.473 (1.390)	Prec@1 49.219 (54.320)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Test: [0/79]	Time: 0.3726(0.3726)	Loss: 3.051(3.051)	Prec@1: 40.625(40.625)	
10-10-22 14:08:Test: [50/79]	Time: 0.0176(0.0255)	Loss: 2.894(3.213)	Prec@1: 39.844(38.297)	
10-10-22 14:08:Test: [78/79]	Time: 0.0151(0.0228)	Loss: 1.479(3.200)	Prec@1: 62.500(38.630)	
10-10-22 14:08:Step 116 * Prec@1 38.630
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [0/391]	Time 0.372 (0.372)	Data 0.322 (0.322)	Loss 3.061 (3.061)	Prec@1 16.406 (16.406)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [50/391]	Time 0.035 (0.049)	Data 0.001 (0.008)	Loss 1.319 (1.625)	Prec@1 52.344 (46.768)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [100/391]	Time 0.040 (0.045)	Data 0.001 (0.005)	Loss 1.436 (1.508)	Prec@1 52.344 (49.876)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [150/391]	Time 0.066 (0.046)	Data 0.003 (0.004)	Loss 1.302 (1.463)	Prec@1 59.375 (51.490)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [200/391]	Time 0.041 (0.047)	Data 0.002 (0.003)	Loss 1.826 (1.452)	Prec@1 46.094 (51.838)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [250/391]	Time 0.067 (0.046)	Data 0.002 (0.003)	Loss 1.561 (1.452)	Prec@1 48.438 (52.176)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [300/391]	Time 0.045 (0.045)	Data 0.001 (0.003)	Loss 1.268 (1.438)	Prec@1 62.500 (52.546)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [350/391]	Time 0.039 (0.045)	Data 0.001 (0.003)	Loss 2.042 (1.437)	Prec@1 48.438 (52.638)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Test: [0/79]	Time: 0.2861(0.2861)	Loss: 3.639(3.639)	Prec@1: 28.125(28.125)	
10-10-22 14:08:Test: [50/79]	Time: 0.0161(0.0217)	Loss: 3.389(3.524)	Prec@1: 27.344(33.165)	
10-10-22 14:08:Test: [78/79]	Time: 0.0149(0.0198)	Loss: 2.180(3.515)	Prec@1: 43.750(32.990)	
10-10-22 14:08:Step 117 * Prec@1 32.990
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [0/391]	Time 0.349 (0.349)	Data 0.312 (0.312)	Loss 2.906 (2.906)	Prec@1 21.094 (21.094)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [50/391]	Time 0.034 (0.039)	Data 0.001 (0.007)	Loss 1.560 (1.894)	Prec@1 48.438 (42.387)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [100/391]	Time 0.036 (0.038)	Data 0.001 (0.004)	Loss 2.019 (1.812)	Prec@1 42.188 (43.758)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [150/391]	Time 0.032 (0.044)	Data 0.001 (0.004)	Loss 1.831 (1.762)	Prec@1 46.875 (45.023)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [200/391]	Time 0.035 (0.042)	Data 0.001 (0.003)	Loss 1.512 (1.731)	Prec@1 50.000 (45.767)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [250/391]	Time 0.037 (0.041)	Data 0.002 (0.003)	Loss 1.285 (1.692)	Prec@1 50.781 (46.501)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [300/391]	Time 0.036 (0.041)	Data 0.001 (0.003)	Loss 1.499 (1.687)	Prec@1 50.781 (46.665)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [350/391]	Time 0.038 (0.041)	Data 0.002 (0.003)	Loss 1.552 (1.674)	Prec@1 42.969 (46.884)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Test: [0/79]	Time: 0.3093(0.3093)	Loss: 3.023(3.023)	Prec@1: 29.688(29.688)	
10-10-22 14:08:Test: [50/79]	Time: 0.0172(0.0220)	Loss: 2.745(3.083)	Prec@1: 32.031(32.966)	
10-10-22 14:08:Test: [78/79]	Time: 0.0149(0.0200)	Loss: 1.888(3.087)	Prec@1: 25.000(32.580)	
10-10-22 14:08:Step 118 * Prec@1 32.580
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [0/391]	Time 0.303 (0.303)	Data 0.269 (0.269)	Loss 3.073 (3.073)	Prec@1 23.438 (23.438)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [50/391]	Time 0.031 (0.036)	Data 0.001 (0.006)	Loss 1.546 (1.752)	Prec@1 50.000 (43.949)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [100/391]	Time 0.034 (0.036)	Data 0.001 (0.004)	Loss 1.539 (1.646)	Prec@1 50.000 (46.798)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [150/391]	Time 0.033 (0.035)	Data 0.001 (0.003)	Loss 1.490 (1.591)	Prec@1 50.781 (48.168)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [200/391]	Time 0.038 (0.035)	Data 0.002 (0.003)	Loss 1.442 (1.564)	Prec@1 46.875 (48.861)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [250/391]	Time 0.040 (0.036)	Data 0.002 (0.003)	Loss 1.640 (1.557)	Prec@1 44.531 (49.290)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [300/391]	Time 0.038 (0.037)	Data 0.002 (0.002)	Loss 1.336 (1.548)	Prec@1 53.125 (49.663)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [350/391]	Time 0.039 (0.038)	Data 0.002 (0.002)	Loss 1.153 (1.538)	Prec@1 61.719 (49.884)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Test: [0/79]	Time: 0.2940(0.2940)	Loss: 2.496(2.496)	Prec@1: 39.062(39.062)	
10-10-22 14:08:Test: [50/79]	Time: 0.0166(0.0225)	Loss: 2.368(2.593)	Prec@1: 34.375(38.526)	
10-10-22 14:08:Test: [78/79]	Time: 0.0167(0.0213)	Loss: 1.877(2.601)	Prec@1: 43.750(38.630)	
10-10-22 14:08:Step 119 * Prec@1 38.630
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [0/391]	Time 0.360 (0.360)	Data 0.305 (0.305)	Loss 3.320 (3.320)	Prec@1 16.406 (16.406)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:08:Num bit 8	Num grad bit 8	
10-10-22 14:08:Iter: [50/391]	Time 0.061 (0.057)	Data 0.002 (0.008)	Loss 1.692 (1.758)	Prec@1 46.875 (44.899)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [100/391]	Time 0.032 (0.049)	Data 0.001 (0.005)	Loss 1.406 (1.615)	Prec@1 58.594 (48.407)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [150/391]	Time 0.051 (0.046)	Data 0.002 (0.004)	Loss 1.591 (1.545)	Prec@1 53.125 (49.907)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [200/391]	Time 0.037 (0.047)	Data 0.001 (0.003)	Loss 1.516 (1.523)	Prec@1 50.000 (50.719)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [250/391]	Time 0.044 (0.046)	Data 0.001 (0.003)	Loss 1.447 (1.511)	Prec@1 51.562 (50.931)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [300/391]	Time 0.035 (0.045)	Data 0.001 (0.003)	Loss 1.689 (1.502)	Prec@1 46.875 (51.259)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [350/391]	Time 0.058 (0.045)	Data 0.003 (0.003)	Loss 1.611 (1.496)	Prec@1 50.000 (51.458)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Test: [0/79]	Time: 0.3174(0.3174)	Loss: 2.743(2.743)	Prec@1: 41.406(41.406)	
10-10-22 14:09:Test: [50/79]	Time: 0.0197(0.0266)	Loss: 2.878(2.886)	Prec@1: 36.719(37.990)	
10-10-22 14:09:Test: [78/79]	Time: 0.0192(0.0244)	Loss: 2.337(2.882)	Prec@1: 37.500(38.030)	
10-10-22 14:09:Step 120 * Prec@1 38.030
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [0/391]	Time 0.319 (0.319)	Data 0.276 (0.276)	Loss 2.753 (2.753)	Prec@1 24.219 (24.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [50/391]	Time 0.050 (0.050)	Data 0.002 (0.007)	Loss 1.195 (1.716)	Prec@1 56.250 (45.588)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [100/391]	Time 0.064 (0.054)	Data 0.002 (0.005)	Loss 1.437 (1.614)	Prec@1 53.125 (48.461)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [150/391]	Time 0.049 (0.053)	Data 0.002 (0.004)	Loss 1.707 (1.582)	Prec@1 47.656 (49.457)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [200/391]	Time 0.056 (0.051)	Data 0.003 (0.003)	Loss 1.483 (1.558)	Prec@1 50.781 (50.148)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [250/391]	Time 0.044 (0.050)	Data 0.002 (0.003)	Loss 1.287 (1.547)	Prec@1 61.719 (50.545)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [300/391]	Time 0.034 (0.047)	Data 0.001 (0.003)	Loss 1.361 (1.528)	Prec@1 56.250 (51.088)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [350/391]	Time 0.041 (0.046)	Data 0.002 (0.003)	Loss 1.264 (1.520)	Prec@1 58.594 (51.380)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Test: [0/79]	Time: 0.2857(0.2857)	Loss: 3.694(3.694)	Prec@1: 30.469(30.469)	
10-10-22 14:09:Test: [50/79]	Time: 0.0163(0.0219)	Loss: 3.606(3.641)	Prec@1: 27.344(35.172)	
10-10-22 14:09:Test: [78/79]	Time: 0.0148(0.0199)	Loss: 3.116(3.633)	Prec@1: 31.250(34.990)	
10-10-22 14:09:Step 121 * Prec@1 34.990
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [0/391]	Time 0.346 (0.346)	Data 0.310 (0.310)	Loss 3.120 (3.120)	Prec@1 25.000 (25.000)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [50/391]	Time 0.051 (0.045)	Data 0.002 (0.008)	Loss 1.406 (1.791)	Prec@1 53.906 (47.610)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [100/391]	Time 0.045 (0.053)	Data 0.002 (0.005)	Loss 1.410 (1.616)	Prec@1 53.125 (50.874)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [150/391]	Time 0.051 (0.055)	Data 0.002 (0.004)	Loss 1.299 (1.560)	Prec@1 57.031 (51.537)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [200/391]	Time 0.040 (0.051)	Data 0.001 (0.004)	Loss 1.644 (1.533)	Prec@1 45.312 (52.044)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [250/391]	Time 0.045 (0.053)	Data 0.002 (0.003)	Loss 1.615 (1.530)	Prec@1 49.219 (52.048)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [300/391]	Time 0.036 (0.051)	Data 0.001 (0.003)	Loss 0.901 (1.519)	Prec@1 65.625 (52.445)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [350/391]	Time 0.037 (0.049)	Data 0.002 (0.003)	Loss 1.251 (1.520)	Prec@1 53.906 (52.720)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Test: [0/79]	Time: 0.3454(0.3454)	Loss: 3.342(3.342)	Prec@1: 33.594(33.594)	
10-10-22 14:09:Test: [50/79]	Time: 0.0174(0.0277)	Loss: 3.390(3.318)	Prec@1: 34.375(35.524)	
10-10-22 14:09:Test: [78/79]	Time: 0.0154(0.0246)	Loss: 2.456(3.330)	Prec@1: 18.750(35.640)	
10-10-22 14:09:Step 122 * Prec@1 35.640
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [0/391]	Time 0.386 (0.386)	Data 0.345 (0.345)	Loss 4.122 (4.122)	Prec@1 14.062 (14.062)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:09:Num bit 8	Num grad bit 8	
10-10-22 14:09:Iter: [50/391]	Time 0.043 (0.051)	Data 0.002 (0.009)	Loss 1.133 (1.882)	Prec@1 60.156 (46.354)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [100/391]	Time 0.032 (0.050)	Data 0.002 (0.005)	Loss 1.448 (1.677)	Prec@1 53.906 (49.923)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [150/391]	Time 0.039 (0.046)	Data 0.001 (0.004)	Loss 1.597 (1.613)	Prec@1 54.688 (50.952)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [200/391]	Time 0.047 (0.052)	Data 0.002 (0.004)	Loss 1.373 (1.574)	Prec@1 53.125 (51.652)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [250/391]	Time 0.041 (0.050)	Data 0.002 (0.003)	Loss 1.707 (1.573)	Prec@1 52.344 (51.678)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [300/391]	Time 0.041 (0.048)	Data 0.002 (0.003)	Loss 1.540 (1.576)	Prec@1 56.250 (51.848)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [350/391]	Time 0.046 (0.047)	Data 0.002 (0.003)	Loss 1.687 (1.568)	Prec@1 47.656 (52.106)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Test: [0/79]	Time: 0.3758(0.3758)	Loss: 2.518(2.518)	Prec@1: 41.406(41.406)	
10-10-22 14:10:Test: [50/79]	Time: 0.0182(0.0340)	Loss: 2.635(2.520)	Prec@1: 36.719(41.575)	
10-10-22 14:10:Test: [78/79]	Time: 0.0151(0.0307)	Loss: 1.578(2.532)	Prec@1: 37.500(41.530)	
10-10-22 14:10:Step 123 * Prec@1 41.530
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [0/391]	Time 0.376 (0.376)	Data 0.329 (0.329)	Loss 3.251 (3.251)	Prec@1 17.969 (17.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [50/391]	Time 0.031 (0.046)	Data 0.001 (0.008)	Loss 1.173 (1.848)	Prec@1 56.250 (46.140)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [100/391]	Time 0.093 (0.045)	Data 0.007 (0.005)	Loss 1.553 (1.673)	Prec@1 49.219 (50.054)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [150/391]	Time 0.047 (0.052)	Data 0.002 (0.004)	Loss 1.185 (1.621)	Prec@1 58.594 (51.081)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [200/391]	Time 0.045 (0.050)	Data 0.003 (0.004)	Loss 1.871 (1.594)	Prec@1 46.875 (51.800)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [250/391]	Time 0.045 (0.048)	Data 0.002 (0.003)	Loss 1.575 (1.615)	Prec@1 51.562 (51.671)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [300/391]	Time 0.047 (0.048)	Data 0.002 (0.003)	Loss 1.241 (1.608)	Prec@1 65.625 (52.066)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [350/391]	Time 0.048 (0.048)	Data 0.002 (0.003)	Loss 1.756 (1.586)	Prec@1 49.219 (52.522)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Test: [0/79]	Time: 0.3179(0.3179)	Loss: 3.360(3.360)	Prec@1: 36.719(36.719)	
10-10-22 14:10:Test: [50/79]	Time: 0.0161(0.0227)	Loss: 3.788(3.428)	Prec@1: 31.250(39.675)	
10-10-22 14:10:Test: [78/79]	Time: 0.0149(0.0204)	Loss: 2.100(3.411)	Prec@1: 43.750(39.390)	
10-10-22 14:10:Step 124 * Prec@1 39.390
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [0/391]	Time 0.373 (0.373)	Data 0.334 (0.334)	Loss 3.530 (3.530)	Prec@1 18.750 (18.750)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [50/391]	Time 0.082 (0.055)	Data 0.004 (0.009)	Loss 1.414 (1.840)	Prec@1 53.906 (48.085)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [100/391]	Time 0.040 (0.051)	Data 0.002 (0.005)	Loss 1.961 (1.639)	Prec@1 46.094 (51.222)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [150/391]	Time 0.035 (0.046)	Data 0.001 (0.004)	Loss 1.688 (1.654)	Prec@1 50.000 (51.278)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [200/391]	Time 0.037 (0.046)	Data 0.001 (0.003)	Loss 1.995 (1.650)	Prec@1 41.406 (51.446)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [250/391]	Time 0.037 (0.045)	Data 0.001 (0.003)	Loss 1.780 (1.641)	Prec@1 47.656 (51.731)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [300/391]	Time 0.035 (0.046)	Data 0.001 (0.003)	Loss 1.450 (1.629)	Prec@1 48.438 (51.970)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [350/391]	Time 0.039 (0.046)	Data 0.001 (0.003)	Loss 1.323 (1.633)	Prec@1 60.938 (51.876)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Test: [0/79]	Time: 0.2946(0.2946)	Loss: 3.320(3.320)	Prec@1: 32.031(32.031)	
10-10-22 14:10:Test: [50/79]	Time: 0.0169(0.0234)	Loss: 2.930(3.119)	Prec@1: 36.719(37.822)	
10-10-22 14:10:Test: [78/79]	Time: 0.0152(0.0212)	Loss: 1.760(3.122)	Prec@1: 43.750(37.510)	
10-10-22 14:10:Step 125 * Prec@1 37.510
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [0/391]	Time 0.325 (0.325)	Data 0.288 (0.288)	Loss 3.733 (3.733)	Prec@1 16.406 (16.406)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:10:Num bit 8	Num grad bit 8	
10-10-22 14:10:Iter: [50/391]	Time 0.052 (0.055)	Data 0.002 (0.007)	Loss 1.951 (2.174)	Prec@1 46.875 (42.264)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [100/391]	Time 0.035 (0.052)	Data 0.001 (0.005)	Loss 1.370 (1.915)	Prec@1 57.812 (46.403)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [150/391]	Time 0.045 (0.049)	Data 0.002 (0.004)	Loss 1.893 (1.848)	Prec@1 57.812 (47.651)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [200/391]	Time 0.044 (0.049)	Data 0.002 (0.003)	Loss 1.399 (1.805)	Prec@1 50.000 (48.399)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [250/391]	Time 0.042 (0.048)	Data 0.002 (0.003)	Loss 1.888 (1.786)	Prec@1 46.094 (48.823)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [300/391]	Time 0.039 (0.048)	Data 0.003 (0.003)	Loss 1.230 (1.754)	Prec@1 57.031 (49.268)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [350/391]	Time 0.042 (0.047)	Data 0.002 (0.003)	Loss 1.279 (1.748)	Prec@1 60.156 (49.584)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Test: [0/79]	Time: 0.3071(0.3071)	Loss: 2.307(2.307)	Prec@1: 40.625(40.625)	
10-10-22 14:11:Test: [50/79]	Time: 0.0163(0.0227)	Loss: 2.299(2.471)	Prec@1: 40.625(40.916)	
10-10-22 14:11:Test: [78/79]	Time: 0.0154(0.0209)	Loss: 1.754(2.477)	Prec@1: 37.500(41.170)	
10-10-22 14:11:Step 126 * Prec@1 41.170
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [0/391]	Time 0.362 (0.362)	Data 0.327 (0.327)	Loss 2.607 (2.607)	Prec@1 26.562 (26.562)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [50/391]	Time 0.044 (0.045)	Data 0.002 (0.008)	Loss 2.229 (2.001)	Prec@1 50.000 (45.680)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [100/391]	Time 0.054 (0.046)	Data 0.001 (0.005)	Loss 1.475 (1.755)	Prec@1 53.906 (49.838)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [150/391]	Time 0.040 (0.045)	Data 0.002 (0.004)	Loss 1.244 (1.704)	Prec@1 57.812 (50.517)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [200/391]	Time 0.061 (0.043)	Data 0.003 (0.003)	Loss 1.424 (1.655)	Prec@1 53.125 (51.224)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [250/391]	Time 0.047 (0.045)	Data 0.003 (0.003)	Loss 1.757 (1.622)	Prec@1 48.438 (51.671)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [300/391]	Time 0.034 (0.044)	Data 0.001 (0.003)	Loss 1.885 (1.612)	Prec@1 48.438 (51.897)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [350/391]	Time 0.033 (0.045)	Data 0.001 (0.003)	Loss 1.621 (1.626)	Prec@1 52.344 (51.754)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Test: [0/79]	Time: 0.3403(0.3403)	Loss: 2.788(2.788)	Prec@1: 50.781(50.781)	
10-10-22 14:11:Test: [50/79]	Time: 0.0175(0.0261)	Loss: 2.510(2.590)	Prec@1: 37.500(42.938)	
10-10-22 14:11:Test: [78/79]	Time: 0.0168(0.0233)	Loss: 1.409(2.596)	Prec@1: 43.750(43.060)	
10-10-22 14:11:Step 127 * Prec@1 43.060
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [0/391]	Time 0.356 (0.356)	Data 0.313 (0.313)	Loss 3.593 (3.593)	Prec@1 17.188 (17.188)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [50/391]	Time 0.039 (0.052)	Data 0.002 (0.008)	Loss 2.117 (2.001)	Prec@1 49.219 (44.991)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [100/391]	Time 0.036 (0.045)	Data 0.001 (0.005)	Loss 1.346 (1.823)	Prec@1 60.938 (48.407)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [150/391]	Time 0.041 (0.042)	Data 0.001 (0.003)	Loss 1.481 (1.791)	Prec@1 57.812 (49.384)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [200/391]	Time 0.052 (0.041)	Data 0.004 (0.003)	Loss 1.459 (1.774)	Prec@1 53.125 (49.930)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [250/391]	Time 0.046 (0.042)	Data 0.002 (0.003)	Loss 1.731 (1.755)	Prec@1 50.000 (50.199)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [300/391]	Time 0.036 (0.041)	Data 0.001 (0.003)	Loss 1.403 (1.725)	Prec@1 53.906 (50.693)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [350/391]	Time 0.055 (0.042)	Data 0.001 (0.003)	Loss 1.528 (1.713)	Prec@1 53.906 (50.930)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Test: [0/79]	Time: 0.3198(0.3198)	Loss: 2.329(2.329)	Prec@1: 47.656(47.656)	
10-10-22 14:11:Test: [50/79]	Time: 0.0165(0.0229)	Loss: 2.325(2.470)	Prec@1: 37.500(40.303)	
10-10-22 14:11:Test: [78/79]	Time: 0.0154(0.0211)	Loss: 1.351(2.471)	Prec@1: 43.750(40.150)	
10-10-22 14:11:Step 128 * Prec@1 40.150
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [0/391]	Time 0.364 (0.364)	Data 0.329 (0.329)	Loss 2.937 (2.937)	Prec@1 21.094 (21.094)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [50/391]	Time 0.040 (0.050)	Data 0.001 (0.008)	Loss 1.841 (2.502)	Prec@1 42.188 (40.748)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:11:Num bit 8	Num grad bit 8	
10-10-22 14:11:Iter: [100/391]	Time 0.045 (0.047)	Data 0.002 (0.005)	Loss 1.953 (2.278)	Prec@1 45.312 (43.000)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [150/391]	Time 0.044 (0.046)	Data 0.002 (0.004)	Loss 1.271 (2.091)	Prec@1 57.812 (45.623)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [200/391]	Time 0.054 (0.048)	Data 0.002 (0.003)	Loss 1.387 (1.966)	Prec@1 55.469 (47.287)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [250/391]	Time 0.047 (0.048)	Data 0.002 (0.003)	Loss 1.473 (1.895)	Prec@1 52.344 (48.319)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [300/391]	Time 0.047 (0.048)	Data 0.002 (0.003)	Loss 1.711 (1.862)	Prec@1 46.875 (48.876)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [350/391]	Time 0.068 (0.049)	Data 0.004 (0.003)	Loss 2.276 (1.824)	Prec@1 41.406 (49.562)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Test: [0/79]	Time: 0.3290(0.3290)	Loss: 2.565(2.565)	Prec@1: 43.750(43.750)	
10-10-22 14:12:Test: [50/79]	Time: 0.0198(0.0265)	Loss: 2.421(2.709)	Prec@1: 36.719(39.323)	
10-10-22 14:12:Test: [78/79]	Time: 0.0213(0.0245)	Loss: 1.716(2.700)	Prec@1: 56.250(39.390)	
10-10-22 14:12:Step 129 * Prec@1 39.390
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [0/391]	Time 0.367 (0.367)	Data 0.331 (0.331)	Loss 3.132 (3.132)	Prec@1 18.750 (18.750)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [50/391]	Time 0.062 (0.060)	Data 0.002 (0.009)	Loss 1.120 (1.956)	Prec@1 58.594 (46.477)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [100/391]	Time 0.042 (0.054)	Data 0.001 (0.005)	Loss 2.071 (1.804)	Prec@1 46.875 (49.273)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [150/391]	Time 0.045 (0.050)	Data 0.002 (0.004)	Loss 1.636 (1.769)	Prec@1 52.344 (50.145)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [200/391]	Time 0.036 (0.049)	Data 0.002 (0.004)	Loss 1.595 (1.727)	Prec@1 52.344 (50.871)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [250/391]	Time 0.060 (0.049)	Data 0.002 (0.003)	Loss 3.031 (1.728)	Prec@1 32.812 (50.987)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [300/391]	Time 0.056 (0.051)	Data 0.002 (0.003)	Loss 2.346 (1.866)	Prec@1 44.531 (49.548)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [350/391]	Time 0.068 (0.050)	Data 0.006 (0.003)	Loss 1.945 (1.972)	Prec@1 44.531 (48.295)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Test: [0/79]	Time: 0.3167(0.3167)	Loss: 2.795(2.795)	Prec@1: 33.594(33.594)	
10-10-22 14:12:Test: [50/79]	Time: 0.0192(0.0243)	Loss: 2.625(2.885)	Prec@1: 39.062(35.187)	
10-10-22 14:12:Test: [78/79]	Time: 0.0214(0.0223)	Loss: 2.420(2.873)	Prec@1: 43.750(34.900)	
10-10-22 14:12:Step 130 * Prec@1 34.900
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [0/391]	Time 0.338 (0.338)	Data 0.291 (0.291)	Loss 3.593 (3.593)	Prec@1 17.969 (17.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [50/391]	Time 0.034 (0.043)	Data 0.001 (0.007)	Loss 1.931 (2.417)	Prec@1 42.188 (39.537)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [100/391]	Time 0.039 (0.044)	Data 0.002 (0.005)	Loss 2.517 (2.179)	Prec@1 43.750 (42.992)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [150/391]	Time 0.036 (0.042)	Data 0.001 (0.004)	Loss 1.586 (2.156)	Prec@1 54.688 (43.817)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [200/391]	Time 0.036 (0.041)	Data 0.002 (0.003)	Loss 1.796 (2.129)	Prec@1 43.750 (44.387)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [250/391]	Time 0.031 (0.040)	Data 0.001 (0.003)	Loss 2.049 (2.094)	Prec@1 39.062 (44.983)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [300/391]	Time 0.044 (0.040)	Data 0.003 (0.003)	Loss 1.881 (2.069)	Prec@1 47.656 (45.507)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [350/391]	Time 0.044 (0.040)	Data 0.002 (0.003)	Loss 2.059 (2.037)	Prec@1 47.656 (45.844)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Test: [0/79]	Time: 0.4134(0.4134)	Loss: 2.666(2.666)	Prec@1: 32.812(32.812)	
10-10-22 14:12:Test: [50/79]	Time: 0.0168(0.0293)	Loss: 2.602(2.717)	Prec@1: 38.281(36.397)	
10-10-22 14:12:Test: [78/79]	Time: 0.0149(0.0248)	Loss: 1.734(2.713)	Prec@1: 43.750(36.110)	
10-10-22 14:12:Step 131 * Prec@1 36.110
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [0/391]	Time 0.386 (0.386)	Data 0.347 (0.347)	Loss 2.389 (2.389)	Prec@1 24.219 (24.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:12:Num bit 8	Num grad bit 8	
10-10-22 14:12:Iter: [50/391]	Time 0.036 (0.045)	Data 0.001 (0.008)	Loss 2.087 (2.690)	Prec@1 56.250 (38.848)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [100/391]	Time 0.048 (0.046)	Data 0.001 (0.005)	Loss 1.625 (2.487)	Prec@1 53.125 (40.393)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [150/391]	Time 0.052 (0.049)	Data 0.002 (0.004)	Loss 1.788 (2.459)	Prec@1 40.625 (40.604)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [200/391]	Time 0.072 (0.052)	Data 0.003 (0.004)	Loss 2.395 (2.342)	Prec@1 42.969 (42.114)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [250/391]	Time 0.051 (0.051)	Data 0.002 (0.003)	Loss 2.129 (2.292)	Prec@1 43.750 (42.819)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [300/391]	Time 0.037 (0.052)	Data 0.001 (0.003)	Loss 1.913 (2.216)	Prec@1 46.875 (43.854)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [350/391]	Time 0.038 (0.050)	Data 0.002 (0.003)	Loss 1.528 (2.145)	Prec@1 54.688 (44.660)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Test: [0/79]	Time: 0.3113(0.3113)	Loss: 2.311(2.311)	Prec@1: 37.500(37.500)	
10-10-22 14:13:Test: [50/79]	Time: 0.0164(0.0281)	Loss: 2.380(2.476)	Prec@1: 32.812(37.776)	
10-10-22 14:13:Test: [78/79]	Time: 0.0148(0.0250)	Loss: 2.458(2.493)	Prec@1: 37.500(37.600)	
10-10-22 14:13:Step 132 * Prec@1 37.600
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [0/391]	Time 0.325 (0.325)	Data 0.290 (0.290)	Loss 3.009 (3.009)	Prec@1 28.125 (28.125)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [50/391]	Time 0.053 (0.056)	Data 0.002 (0.007)	Loss 2.144 (2.201)	Prec@1 39.844 (43.704)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [100/391]	Time 0.042 (0.050)	Data 0.002 (0.005)	Loss 1.404 (1.975)	Prec@1 54.688 (46.821)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [150/391]	Time 0.040 (0.046)	Data 0.003 (0.004)	Loss 1.849 (1.869)	Prec@1 45.312 (48.474)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [200/391]	Time 0.046 (0.045)	Data 0.002 (0.003)	Loss 2.088 (1.823)	Prec@1 45.312 (49.386)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [250/391]	Time 0.045 (0.045)	Data 0.001 (0.003)	Loss 2.074 (1.789)	Prec@1 41.406 (49.847)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [300/391]	Time 0.063 (0.045)	Data 0.007 (0.003)	Loss 1.430 (1.801)	Prec@1 57.031 (49.707)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [350/391]	Time 0.055 (0.046)	Data 0.002 (0.003)	Loss 1.760 (1.777)	Prec@1 57.812 (49.971)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Test: [0/79]	Time: 0.2739(0.2739)	Loss: 2.296(2.296)	Prec@1: 35.156(35.156)	
10-10-22 14:13:Test: [50/79]	Time: 0.0163(0.0214)	Loss: 2.427(2.478)	Prec@1: 35.156(40.732)	
10-10-22 14:13:Test: [78/79]	Time: 0.0158(0.0200)	Loss: 2.536(2.512)	Prec@1: 43.750(40.520)	
10-10-22 14:13:Step 133 * Prec@1 40.520
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [0/391]	Time 0.444 (0.444)	Data 0.356 (0.356)	Loss 2.892 (2.892)	Prec@1 25.000 (25.000)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [50/391]	Time 0.048 (0.053)	Data 0.001 (0.009)	Loss 1.844 (1.960)	Prec@1 46.875 (47.258)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [100/391]	Time 0.061 (0.046)	Data 0.002 (0.005)	Loss 1.519 (1.874)	Prec@1 50.781 (49.242)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [150/391]	Time 0.033 (0.046)	Data 0.002 (0.004)	Loss 1.420 (1.836)	Prec@1 55.469 (49.969)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [200/391]	Time 0.058 (0.045)	Data 0.002 (0.004)	Loss 1.170 (1.770)	Prec@1 60.156 (51.193)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [250/391]	Time 0.047 (0.046)	Data 0.002 (0.003)	Loss 1.871 (1.729)	Prec@1 54.688 (51.855)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [300/391]	Time 0.041 (0.044)	Data 0.001 (0.003)	Loss 1.288 (1.694)	Prec@1 60.156 (52.188)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [350/391]	Time 0.042 (0.044)	Data 0.003 (0.003)	Loss 3.119 (1.698)	Prec@1 49.219 (52.264)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Test: [0/79]	Time: 0.2899(0.2899)	Loss: 2.677(2.677)	Prec@1: 40.625(40.625)	
10-10-22 14:13:Test: [50/79]	Time: 0.0166(0.0219)	Loss: 2.536(2.642)	Prec@1: 36.719(41.942)	
10-10-22 14:13:Test: [78/79]	Time: 0.0150(0.0199)	Loss: 2.152(2.644)	Prec@1: 43.750(42.140)	
10-10-22 14:13:Step 134 * Prec@1 42.140
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [0/391]	Time 0.338 (0.338)	Data 0.302 (0.302)	Loss 3.034 (3.034)	Prec@1 21.875 (21.875)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [50/391]	Time 0.045 (0.050)	Data 0.001 (0.008)	Loss 1.481 (2.165)	Prec@1 55.469 (47.319)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:13:Num bit 8	Num grad bit 8	
10-10-22 14:13:Iter: [100/391]	Time 0.048 (0.047)	Data 0.002 (0.005)	Loss 1.238 (1.913)	Prec@1 64.844 (50.774)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [150/391]	Time 0.037 (0.047)	Data 0.002 (0.004)	Loss 1.539 (1.800)	Prec@1 50.000 (51.676)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [200/391]	Time 0.052 (0.049)	Data 0.002 (0.003)	Loss 1.436 (1.726)	Prec@1 60.156 (52.849)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [250/391]	Time 0.045 (0.048)	Data 0.002 (0.003)	Loss 1.959 (1.683)	Prec@1 46.875 (53.427)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [300/391]	Time 0.031 (0.047)	Data 0.001 (0.003)	Loss 1.349 (1.705)	Prec@1 57.031 (53.094)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [350/391]	Time 0.057 (0.047)	Data 0.003 (0.003)	Loss 1.145 (1.684)	Prec@1 64.062 (53.232)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Test: [0/79]	Time: 0.2942(0.2942)	Loss: 3.192(3.192)	Prec@1: 39.844(39.844)	
10-10-22 14:14:Test: [50/79]	Time: 0.0179(0.0236)	Loss: 2.778(3.244)	Prec@1: 30.469(35.600)	
10-10-22 14:14:Test: [78/79]	Time: 0.0159(0.0214)	Loss: 2.426(3.249)	Prec@1: 50.000(35.800)	
10-10-22 14:14:Step 135 * Prec@1 35.800
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [0/391]	Time 0.318 (0.318)	Data 0.284 (0.284)	Loss 2.624 (2.624)	Prec@1 24.219 (24.219)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [50/391]	Time 0.032 (0.044)	Data 0.001 (0.007)	Loss 1.467 (2.379)	Prec@1 51.562 (45.159)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [100/391]	Time 0.039 (0.041)	Data 0.001 (0.004)	Loss 1.904 (2.230)	Prec@1 53.125 (47.130)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [150/391]	Time 0.038 (0.039)	Data 0.001 (0.003)	Loss 1.678 (2.012)	Prec@1 61.719 (49.762)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [200/391]	Time 0.035 (0.039)	Data 0.002 (0.003)	Loss 2.305 (1.883)	Prec@1 46.094 (51.341)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [250/391]	Time 0.048 (0.042)	Data 0.003 (0.003)	Loss 1.648 (1.848)	Prec@1 47.656 (51.612)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [300/391]	Time 0.042 (0.042)	Data 0.002 (0.002)	Loss 1.900 (1.805)	Prec@1 55.469 (51.970)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [350/391]	Time 0.043 (0.042)	Data 0.002 (0.002)	Loss 1.466 (1.773)	Prec@1 53.906 (52.150)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Test: [0/79]	Time: 0.3213(0.3213)	Loss: 3.672(3.672)	Prec@1: 44.531(44.531)	
10-10-22 14:14:Test: [50/79]	Time: 0.0458(0.0317)	Loss: 3.511(3.839)	Prec@1: 34.375(39.522)	
10-10-22 14:14:Test: [78/79]	Time: 0.0186(0.0312)	Loss: 2.165(3.818)	Prec@1: 37.500(39.580)	
10-10-22 14:14:Step 136 * Prec@1 39.580
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [0/391]	Time 0.406 (0.406)	Data 0.369 (0.369)	Loss 5.015 (5.015)	Prec@1 17.188 (17.188)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [50/391]	Time 0.044 (0.052)	Data 0.001 (0.009)	Loss 1.531 (2.182)	Prec@1 58.594 (48.912)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [100/391]	Time 0.036 (0.048)	Data 0.001 (0.005)	Loss 1.425 (1.885)	Prec@1 53.906 (51.655)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [150/391]	Time 0.041 (0.048)	Data 0.002 (0.004)	Loss 1.944 (1.795)	Prec@1 50.781 (52.370)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [200/391]	Time 0.031 (0.046)	Data 0.001 (0.004)	Loss 1.324 (1.732)	Prec@1 56.250 (52.966)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [250/391]	Time 0.044 (0.046)	Data 0.003 (0.003)	Loss 1.616 (1.723)	Prec@1 61.719 (53.106)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [300/391]	Time 0.045 (0.045)	Data 0.003 (0.003)	Loss 2.107 (1.694)	Prec@1 44.531 (53.527)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [350/391]	Time 0.045 (0.045)	Data 0.002 (0.003)	Loss 2.146 (1.691)	Prec@1 50.000 (53.526)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Test: [0/79]	Time: 0.3041(0.3041)	Loss: 2.567(2.567)	Prec@1: 41.406(41.406)	
10-10-22 14:14:Test: [50/79]	Time: 0.0185(0.0244)	Loss: 2.150(2.640)	Prec@1: 43.750(41.054)	
10-10-22 14:14:Test: [78/79]	Time: 0.0148(0.0219)	Loss: 1.950(2.630)	Prec@1: 62.500(41.400)	
10-10-22 14:14:Step 137 * Prec@1 41.400
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [0/391]	Time 0.494 (0.494)	Data 0.454 (0.454)	Loss 2.994 (2.994)	Prec@1 26.562 (26.562)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [50/391]	Time 0.036 (0.045)	Data 0.001 (0.010)	Loss 1.532 (2.173)	Prec@1 50.781 (48.698)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:14:Num bit 8	Num grad bit 8	
10-10-22 14:14:Iter: [100/391]	Time 0.037 (0.053)	Data 0.002 (0.007)	Loss 2.186 (2.053)	Prec@1 42.969 (49.343)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [150/391]	Time 0.042 (0.048)	Data 0.001 (0.005)	Loss 1.936 (1.996)	Prec@1 53.125 (49.907)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [200/391]	Time 0.037 (0.047)	Data 0.002 (0.004)	Loss 1.649 (1.982)	Prec@1 53.906 (50.400)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [250/391]	Time 0.033 (0.046)	Data 0.001 (0.004)	Loss 3.244 (1.968)	Prec@1 50.000 (50.598)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [300/391]	Time 0.046 (0.045)	Data 0.003 (0.003)	Loss 1.605 (1.945)	Prec@1 51.562 (51.108)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [350/391]	Time 0.039 (0.046)	Data 0.001 (0.003)	Loss 2.189 (1.933)	Prec@1 51.562 (51.300)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Test: [0/79]	Time: 0.2891(0.2891)	Loss: 4.132(4.132)	Prec@1: 29.688(29.688)	
10-10-22 14:15:Test: [50/79]	Time: 0.0196(0.0255)	Loss: 3.799(4.284)	Prec@1: 35.156(31.710)	
10-10-22 14:15:Test: [78/79]	Time: 0.0265(0.0263)	Loss: 2.116(4.248)	Prec@1: 43.750(31.880)	
10-10-22 14:15:Step 138 * Prec@1 31.880
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [0/391]	Time 0.354 (0.354)	Data 0.310 (0.310)	Loss 3.349 (3.349)	Prec@1 20.312 (20.312)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [50/391]	Time 0.041 (0.049)	Data 0.002 (0.008)	Loss 1.274 (2.312)	Prec@1 57.031 (46.737)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [100/391]	Time 0.049 (0.052)	Data 0.003 (0.005)	Loss 2.085 (2.147)	Prec@1 46.094 (49.049)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [150/391]	Time 0.042 (0.052)	Data 0.002 (0.004)	Loss 2.859 (2.156)	Prec@1 38.281 (49.022)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [200/391]	Time 0.036 (0.051)	Data 0.001 (0.004)	Loss 2.759 (2.108)	Prec@1 44.531 (49.355)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [250/391]	Time 0.033 (0.048)	Data 0.001 (0.003)	Loss 1.488 (2.064)	Prec@1 60.156 (49.807)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [300/391]	Time 0.060 (0.050)	Data 0.005 (0.003)	Loss 1.816 (2.041)	Prec@1 48.438 (50.003)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [350/391]	Time 0.034 (0.049)	Data 0.001 (0.003)	Loss 2.752 (2.039)	Prec@1 45.312 (49.931)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Test: [0/79]	Time: 0.3242(0.3242)	Loss: 5.717(5.717)	Prec@1: 22.656(22.656)	
10-10-22 14:15:Test: [50/79]	Time: 0.0166(0.0240)	Loss: 4.967(5.585)	Prec@1: 30.469(26.317)	
10-10-22 14:15:Test: [78/79]	Time: 0.0171(0.0222)	Loss: 3.749(5.544)	Prec@1: 31.250(26.410)	
10-10-22 14:15:Step 139 * Prec@1 26.410
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [0/391]	Time 0.571 (0.571)	Data 0.508 (0.508)	Loss 3.786 (3.786)	Prec@1 25.000 (25.000)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [50/391]	Time 0.040 (0.060)	Data 0.001 (0.012)	Loss 2.968 (3.593)	Prec@1 45.312 (36.811)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [100/391]	Time 0.035 (0.050)	Data 0.001 (0.007)	Loss 5.327 (3.567)	Prec@1 28.906 (35.999)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [150/391]	Time 0.037 (0.045)	Data 0.001 (0.005)	Loss 5.152 (4.049)	Prec@1 23.438 (34.184)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [200/391]	Time 0.043 (0.043)	Data 0.001 (0.004)	Loss 4.028 (4.116)	Prec@1 30.469 (33.578)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [250/391]	Time 0.052 (0.046)	Data 0.005 (0.004)	Loss 3.645 (4.135)	Prec@1 42.969 (33.304)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [300/391]	Time 0.035 (0.045)	Data 0.001 (0.003)	Loss 4.277 (4.093)	Prec@1 38.281 (33.518)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [350/391]	Time 0.110 (0.044)	Data 0.008 (0.003)	Loss 4.542 (4.064)	Prec@1 25.000 (33.772)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Test: [0/79]	Time: 0.4452(0.4452)	Loss: 6.888(6.888)	Prec@1: 28.125(28.125)	
10-10-22 14:15:Test: [50/79]	Time: 0.0282(0.0280)	Loss: 6.545(7.021)	Prec@1: 19.531(26.333)	
10-10-22 14:15:Test: [78/79]	Time: 0.0153(0.0241)	Loss: 6.521(7.032)	Prec@1: 18.750(26.350)	
10-10-22 14:15:Step 140 * Prec@1 26.350
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [0/391]	Time 0.446 (0.446)	Data 0.405 (0.405)	Loss 10.615 (10.615)	Prec@1 5.469 (5.469)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [50/391]	Time 0.032 (0.055)	Data 0.001 (0.010)	Loss 5.018 (4.893)	Prec@1 39.062 (33.364)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:15:Num bit 8	Num grad bit 8	
10-10-22 14:15:Iter: [100/391]	Time 0.036 (0.045)	Data 0.001 (0.006)	Loss 4.028 (4.118)	Prec@1 34.375 (36.649)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [150/391]	Time 0.037 (0.042)	Data 0.001 (0.004)	Loss 2.817 (4.038)	Prec@1 42.188 (36.693)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [200/391]	Time 0.031 (0.041)	Data 0.001 (0.004)	Loss 3.599 (4.046)	Prec@1 35.156 (36.968)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [250/391]	Time 0.035 (0.040)	Data 0.001 (0.003)	Loss 6.214 (4.025)	Prec@1 42.969 (37.254)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [300/391]	Time 0.036 (0.039)	Data 0.001 (0.003)	Loss 6.178 (4.128)	Prec@1 32.812 (37.131)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [350/391]	Time 0.044 (0.040)	Data 0.001 (0.003)	Loss 5.986 (4.259)	Prec@1 28.125 (36.554)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Test: [0/79]	Time: 0.3626(0.3626)	Loss: 6.919(6.919)	Prec@1: 25.781(25.781)	
10-10-22 14:16:Test: [50/79]	Time: 0.0165(0.0240)	Loss: 7.001(6.818)	Prec@1: 24.219(29.841)	
10-10-22 14:16:Test: [78/79]	Time: 0.0150(0.0213)	Loss: 10.040(6.816)	Prec@1: 12.500(29.530)	
10-10-22 14:16:Step 141 * Prec@1 29.530
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [0/391]	Time 0.606 (0.606)	Data 0.539 (0.539)	Loss 9.864 (9.864)	Prec@1 12.500 (12.500)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [50/391]	Time 0.044 (0.050)	Data 0.004 (0.012)	Loss 5.805 (6.326)	Prec@1 25.781 (30.423)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [100/391]	Time 0.040 (0.044)	Data 0.002 (0.007)	Loss 3.365 (5.717)	Prec@1 43.750 (32.410)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [150/391]	Time 0.036 (0.045)	Data 0.002 (0.005)	Loss 8.284 (5.353)	Prec@1 31.250 (33.708)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [200/391]	Time 0.032 (0.047)	Data 0.001 (0.005)	Loss 14.274 (6.215)	Prec@1 14.062 (32.610)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [250/391]	Time 0.067 (0.047)	Data 0.002 (0.004)	Loss 18.093 (7.403)	Prec@1 25.000 (31.502)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [300/391]	Time 0.035 (0.047)	Data 0.001 (0.004)	Loss 9.250 (8.481)	Prec@1 25.781 (30.804)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [350/391]	Time 0.048 (0.046)	Data 0.001 (0.003)	Loss 18.158 (10.541)	Prec@1 21.875 (29.503)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Test: [0/79]	Time: 0.3451(0.3451)	Loss: 27.866(27.866)	Prec@1: 17.188(17.188)	
10-10-22 14:16:Test: [50/79]	Time: 0.0167(0.0240)	Loss: 28.850(31.755)	Prec@1: 20.312(21.860)	
10-10-22 14:16:Test: [78/79]	Time: 0.0149(0.0215)	Loss: 23.343(31.514)	Prec@1: 37.500(22.000)	
10-10-22 14:16:Step 142 * Prec@1 22.000
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [0/391]	Time 0.372 (0.372)	Data 0.326 (0.326)	Loss 35.888 (35.888)	Prec@1 15.625 (15.625)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [50/391]	Time 0.071 (0.067)	Data 0.002 (0.009)	Loss 22.373 (21.815)	Prec@1 14.062 (21.293)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [100/391]	Time 0.043 (0.061)	Data 0.002 (0.005)	Loss 16.061 (21.567)	Prec@1 17.188 (22.672)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [150/391]	Time 0.040 (0.056)	Data 0.001 (0.004)	Loss 28.584 (22.311)	Prec@1 17.969 (22.667)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [200/391]	Time 0.035 (0.053)	Data 0.001 (0.004)	Loss 26.046 (22.366)	Prec@1 18.750 (22.373)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [250/391]	Time 0.054 (0.052)	Data 0.003 (0.003)	Loss 31.440 (21.468)	Prec@1 20.312 (22.681)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [300/391]	Time 0.041 (0.052)	Data 0.002 (0.003)	Loss 10.512 (20.591)	Prec@1 21.875 (22.913)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [350/391]	Time 0.039 (0.051)	Data 0.002 (0.003)	Loss 15.938 (19.652)	Prec@1 26.562 (23.110)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Test: [0/79]	Time: 0.2824(0.2824)	Loss: 16.297(16.297)	Prec@1: 17.188(17.188)	
10-10-22 14:16:Test: [50/79]	Time: 0.0192(0.0219)	Loss: 14.218(14.230)	Prec@1: 16.406(23.560)	
10-10-22 14:16:Test: [78/79]	Time: 0.0171(0.0209)	Loss: 13.445(14.383)	Prec@1: 6.250(23.480)	
10-10-22 14:16:Step 143 * Prec@1 23.480
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [0/391]	Time 0.402 (0.402)	Data 0.327 (0.327)	Loss 23.582 (23.582)	Prec@1 9.375 (9.375)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [50/391]	Time 0.055 (0.054)	Data 0.002 (0.009)	Loss 14.690 (15.636)	Prec@1 26.562 (25.092)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:16:Num bit 8	Num grad bit 8	
10-10-22 14:16:Iter: [100/391]	Time 0.033 (0.045)	Data 0.001 (0.005)	Loss 13.904 (14.398)	Prec@1 21.875 (25.681)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [150/391]	Time 0.036 (0.047)	Data 0.002 (0.004)	Loss 29.499 (16.654)	Prec@1 26.562 (24.493)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [200/391]	Time 0.044 (0.045)	Data 0.002 (0.003)	Loss 19.308 (18.576)	Prec@1 21.094 (23.504)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [250/391]	Time 0.034 (0.044)	Data 0.001 (0.003)	Loss 18.475 (19.352)	Prec@1 25.000 (23.329)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [300/391]	Time 0.037 (0.044)	Data 0.001 (0.003)	Loss 20.179 (19.318)	Prec@1 27.344 (23.466)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [350/391]	Time 0.034 (0.043)	Data 0.002 (0.003)	Loss 18.155 (19.153)	Prec@1 23.438 (23.518)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Test: [0/79]	Time: 0.2953(0.2953)	Loss: 13.345(13.345)	Prec@1: 25.000(25.000)	
10-10-22 14:17:Test: [50/79]	Time: 0.0162(0.0220)	Loss: 10.803(11.974)	Prec@1: 21.875(28.048)	
10-10-22 14:17:Test: [78/79]	Time: 0.0151(0.0200)	Loss: 12.406(12.060)	Prec@1: 6.250(27.440)	
10-10-22 14:17:Step 144 * Prec@1 27.440
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [0/391]	Time 0.314 (0.314)	Data 0.279 (0.279)	Loss 21.248 (21.248)	Prec@1 12.500 (12.500)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [50/391]	Time 0.057 (0.052)	Data 0.003 (0.007)	Loss 10.675 (20.991)	Prec@1 26.562 (23.499)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [100/391]	Time 0.059 (0.050)	Data 0.003 (0.005)	Loss 15.248 (19.629)	Prec@1 28.125 (24.250)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [150/391]	Time 0.044 (0.050)	Data 0.002 (0.004)	Loss 57.510 (24.646)	Prec@1 19.531 (23.696)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [200/391]	Time 0.051 (0.050)	Data 0.002 (0.003)	Loss 45.712 (28.197)	Prec@1 26.562 (23.041)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [250/391]	Time 0.033 (0.049)	Data 0.001 (0.003)	Loss 31.366 (29.645)	Prec@1 15.625 (22.532)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [300/391]	Time 0.038 (0.048)	Data 0.001 (0.003)	Loss 55.097 (30.130)	Prec@1 11.719 (22.368)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [350/391]	Time 0.046 (0.047)	Data 0.002 (0.003)	Loss 71.908 (34.504)	Prec@1 15.625 (21.908)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Test: [0/79]	Time: 0.3296(0.3296)	Loss: 97.446(97.446)	Prec@1: 12.500(12.500)	
10-10-22 14:17:Test: [50/79]	Time: 0.0206(0.0272)	Loss: 98.753(92.775)	Prec@1: 17.188(14.859)	
10-10-22 14:17:Test: [78/79]	Time: 0.0178(0.0244)	Loss: 125.778(92.737)	Prec@1: 12.500(15.090)	
10-10-22 14:17:Step 145 * Prec@1 15.090
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [0/391]	Time 0.401 (0.401)	Data 0.355 (0.355)	Loss 98.083 (98.083)	Prec@1 6.250 (6.250)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [50/391]	Time 0.054 (0.054)	Data 0.001 (0.009)	Loss 85.352 (82.034)	Prec@1 20.312 (16.100)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [100/391]	Time 0.039 (0.050)	Data 0.002 (0.006)	Loss 115.086 (92.437)	Prec@1 10.156 (16.762)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [150/391]	Time 0.045 (0.047)	Data 0.001 (0.004)	Loss 748.989 (177.672)	Prec@1 11.719 (16.474)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [200/391]	Time 0.031 (0.046)	Data 0.001 (0.004)	Loss 727.935 (348.034)	Prec@1 10.156 (15.473)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [250/391]	Time 0.040 (0.046)	Data 0.001 (0.003)	Loss 797.156 (420.435)	Prec@1 6.250 (14.918)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [300/391]	Time 0.042 (0.045)	Data 0.001 (0.003)	Loss 572.884 (471.107)	Prec@1 16.406 (14.680)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [350/391]	Time 0.085 (0.044)	Data 0.005 (0.003)	Loss 663.927 (512.784)	Prec@1 12.500 (14.646)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Test: [0/79]	Time: 0.2887(0.2887)	Loss: 1012.817(1012.817)	Prec@1: 12.500(12.500)	
10-10-22 14:17:Test: [50/79]	Time: 0.0164(0.0216)	Loss: 1013.750(1128.350)	Prec@1: 8.594(12.010)	
10-10-22 14:17:Test: [78/79]	Time: 0.0150(0.0198)	Loss: 681.214(1120.360)	Prec@1: 18.750(12.060)	
10-10-22 14:17:Step 146 * Prec@1 12.060
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [0/391]	Time 0.301 (0.301)	Data 0.267 (0.267)	Loss 734.747 (734.747)	Prec@1 5.469 (5.469)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [50/391]	Time 0.039 (0.039)	Data 0.001 (0.006)	Loss 763.855 (928.813)	Prec@1 20.312 (13.450)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [100/391]	Time 0.063 (0.049)	Data 0.002 (0.004)	Loss 1038.513 (925.019)	Prec@1 11.719 (13.320)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:17:Num bit 8	Num grad bit 8	
10-10-22 14:17:Iter: [150/391]	Time 0.085 (0.047)	Data 0.007 (0.003)	Loss 1336.160 (889.876)	Prec@1 12.500 (13.850)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [200/391]	Time 0.058 (0.047)	Data 0.001 (0.003)	Loss 487.659 (875.696)	Prec@1 12.500 (13.973)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [250/391]	Time 0.036 (0.046)	Data 0.001 (0.003)	Loss 827.701 (852.211)	Prec@1 12.500 (14.340)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [300/391]	Time 0.055 (0.047)	Data 0.002 (0.003)	Loss 811.675 (831.934)	Prec@1 9.375 (14.561)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [350/391]	Time 0.031 (0.046)	Data 0.001 (0.003)	Loss 392.681 (817.328)	Prec@1 14.844 (14.739)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Test: [0/79]	Time: 0.2984(0.2984)	Loss: 518.622(518.622)	Prec@1: 14.062(14.062)	
10-10-22 14:18:Test: [50/79]	Time: 0.0203(0.0232)	Loss: 526.059(576.280)	Prec@1: 14.062(14.859)	
10-10-22 14:18:Test: [78/79]	Time: 0.0148(0.0207)	Loss: 708.073(573.158)	Prec@1: 12.500(15.020)	
10-10-22 14:18:Step 147 * Prec@1 15.020
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [0/391]	Time 0.355 (0.355)	Data 0.316 (0.316)	Loss 574.570 (574.570)	Prec@1 3.906 (3.906)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [50/391]	Time 0.032 (0.052)	Data 0.001 (0.008)	Loss 651.140 (693.823)	Prec@1 14.844 (15.656)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [100/391]	Time 0.036 (0.047)	Data 0.001 (0.005)	Loss 510.630 (620.857)	Prec@1 20.312 (17.025)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [150/391]	Time 0.038 (0.045)	Data 0.002 (0.004)	Loss 499.738 (606.722)	Prec@1 9.375 (17.255)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [200/391]	Time 0.045 (0.045)	Data 0.001 (0.003)	Loss 587.322 (612.737)	Prec@1 25.000 (17.199)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [250/391]	Time 0.041 (0.044)	Data 0.002 (0.003)	Loss 2199.531 (679.399)	Prec@1 18.750 (16.618)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [300/391]	Time 0.046 (0.044)	Data 0.002 (0.003)	Loss 104942.938 (8884.140)	Prec@1 12.500 (16.027)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [350/391]	Time 0.032 (0.043)	Data 0.001 (0.003)	Loss 1361539.875 (63326.086)	Prec@1 14.844 (15.442)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Test: [0/79]	Time: 0.3343(0.3343)	Loss: 32244822.000(32244822.000)	Prec@1: 8.594(8.594)	
10-10-22 14:18:Test: [50/79]	Time: 0.0163(0.0277)	Loss: 28071790.000(27891616.118)	Prec@1: 6.250(9.957)	
10-10-22 14:18:Test: [78/79]	Time: 0.0178(0.0241)	Loss: 33068662.000(28053455.549)	Prec@1: 0.000(10.000)	
10-10-22 14:18:Step 148 * Prec@1 10.000
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [0/391]	Time 0.350 (0.350)	Data 0.296 (0.296)	Loss 36204432.000 (36204432.000)	Prec@1 11.719 (11.719)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [50/391]	Time 0.060 (0.052)	Data 0.002 (0.008)	Loss 25272186.000 (31055315.000)	Prec@1 9.375 (9.743)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [100/391]	Time 0.041 (0.050)	Data 0.002 (0.005)	Loss 8196686.500 (21921624.010)	Prec@1 11.719 (10.257)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [150/391]	Time 0.035 (0.045)	Data 0.002 (0.004)	Loss 8547632.000 (17890922.404)	Prec@1 12.500 (10.493)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [200/391]	Time 0.036 (0.043)	Data 0.001 (0.003)	Loss 15994896.000 (16190688.876)	Prec@1 11.719 (10.697)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [250/391]	Time 0.037 (0.043)	Data 0.001 (0.003)	Loss 15915426.000 (16153648.773)	Prec@1 13.281 (10.598)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [300/391]	Time 0.034 (0.043)	Data 0.001 (0.003)	Loss 82143144.000 (21604464.698)	Prec@1 11.719 (10.678)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [350/391]	Time 0.033 (0.041)	Data 0.001 (0.002)	Loss 335540704.000 (49217266.194)	Prec@1 12.500 (10.702)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Test: [0/79]	Time: 0.3258(0.3258)	Loss: 301945394823168.000(301945394823168.000)	Prec@1: 8.594(8.594)	
10-10-22 14:18:Test: [50/79]	Time: 0.0282(0.0290)	Loss: 341608276951040.000(296383527641569.875)	Prec@1: 6.250(9.957)	
10-10-22 14:18:Test: [78/79]	Time: 0.0150(0.0258)	Loss: 236259255517184.000(296084111177967.188)	Prec@1: 0.000(10.000)	
10-10-22 14:18:Step 149 * Prec@1 10.000
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [0/391]	Time 0.358 (0.358)	Data 0.323 (0.323)	Loss 901287360.000 (901287360.000)	Prec@1 16.406 (16.406)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [50/391]	Time 0.037 (0.052)	Data 0.002 (0.008)	Loss 119238840.000 (314232628.078)	Prec@1 7.812 (9.697)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [100/391]	Time 0.038 (0.045)	Data 0.001 (0.005)	Loss 85621808.000 (209720838.495)	Prec@1 12.500 (9.545)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [150/391]	Time 0.039 (0.044)	Data 0.001 (0.004)	Loss 103249928.000 (178545024.106)	Prec@1 14.844 (9.706)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:18:Num bit 8	Num grad bit 8	
10-10-22 14:18:Iter: [200/391]	Time 0.066 (0.047)	Data 0.002 (0.003)	Loss 152206896.000 (163894612.816)	Prec@1 9.375 (9.503)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [250/391]	Time 0.037 (0.047)	Data 0.002 (0.003)	Loss 132580728.000 (156146290.327)	Prec@1 10.938 (9.599)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [300/391]	Time 0.038 (0.047)	Data 0.002 (0.003)	Loss 119762592.000 (151386850.870)	Prec@1 7.031 (9.570)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [350/391]	Time 0.050 (0.047)	Data 0.002 (0.003)	Loss 124835232.000 (145770667.373)	Prec@1 7.031 (9.542)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Test: [0/79]	Time: 0.2940(0.2940)	Loss: 51659791138816.000(51659791138816.000)	Prec@1: 10.156(10.156)	
10-10-22 14:19:Test: [50/79]	Time: 0.0160(0.0218)	Loss: 52174562263040.000(50250179401687.844)	Prec@1: 10.938(9.957)	
10-10-22 14:19:Test: [78/79]	Time: 0.0147(0.0198)	Loss: 33774838480896.000(50026037112314.266)	Prec@1: 12.500(10.000)	
10-10-22 14:19:Step 150 * Prec@1 10.000
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [0/391]	Time 0.364 (0.364)	Data 0.316 (0.316)	Loss 119742776.000 (119742776.000)	Prec@1 7.031 (7.031)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [50/391]	Time 0.052 (0.049)	Data 0.002 (0.008)	Loss 103762328.000 (102616754.824)	Prec@1 10.938 (9.574)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [100/391]	Time 0.032 (0.042)	Data 0.001 (0.005)	Loss 103649256.000 (104153922.851)	Prec@1 11.719 (9.630)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [150/391]	Time 0.037 (0.043)	Data 0.002 (0.004)	Loss 114107864.000 (104307517.430)	Prec@1 9.375 (9.825)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [200/391]	Time 0.038 (0.042)	Data 0.001 (0.003)	Loss 93479560.000 (102808585.393)	Prec@1 11.719 (9.977)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [250/391]	Time 0.086 (0.042)	Data 0.006 (0.003)	Loss 88044320.000 (100854562.582)	Prec@1 8.594 (10.035)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [300/391]	Time 0.064 (0.043)	Data 0.001 (0.003)	Loss 57134540.000 (99564908.625)	Prec@1 14.062 (10.068)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [350/391]	Time 0.069 (0.043)	Data 0.003 (0.003)	Loss 128945912.000 (99544919.966)	Prec@1 9.375 (10.081)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Test: [0/79]	Time: 0.2939(0.2939)	Loss: 734627692544.000(734627692544.000)	Prec@1: 10.156(10.156)	
10-10-22 14:19:Test: [50/79]	Time: 0.0175(0.0219)	Loss: 663308468224.000(755937729114.353)	Prec@1: 10.938(9.957)	
10-10-22 14:19:Test: [78/79]	Time: 0.0149(0.0202)	Loss: 776188919808.000(754570861268.173)	Prec@1: 12.500(10.000)	
10-10-22 14:19:Step 151 * Prec@1 10.000
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [0/391]	Time 0.344 (0.344)	Data 0.297 (0.297)	Loss 107910800.000 (107910800.000)	Prec@1 8.594 (8.594)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [50/391]	Time 0.045 (0.045)	Data 0.003 (0.007)	Loss 85055280.000 (102828172.863)	Prec@1 14.844 (10.187)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [100/391]	Time 0.056 (0.050)	Data 0.002 (0.005)	Loss 142121040.000 (104912699.327)	Prec@1 12.500 (10.210)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [150/391]	Time 0.045 (0.052)	Data 0.001 (0.004)	Loss 155659632.000 (105426092.000)	Prec@1 6.250 (10.141)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [200/391]	Time 0.048 (0.050)	Data 0.002 (0.003)	Loss 108602848.000 (109668395.443)	Prec@1 7.812 (9.954)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [250/391]	Time 0.050 (0.049)	Data 0.002 (0.003)	Loss 162226112.000 (113698949.689)	Prec@1 11.719 (10.131)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [300/391]	Time 0.043 (0.047)	Data 0.001 (0.003)	Loss 134819840.000 (118743977.475)	Prec@1 7.812 (10.182)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [350/391]	Time 0.127 (0.047)	Data 0.007 (0.003)	Loss 198405424.000 (128405321.037)	Prec@1 12.500 (10.150)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Test: [0/79]	Time: 0.2761(0.2761)	Loss: 42765173915648.000(42765173915648.000)	Prec@1: 7.031(7.031)	
10-10-22 14:19:Test: [50/79]	Time: 0.0162(0.0218)	Loss: 46122584244224.000(40001328659516.234)	Prec@1: 4.688(9.972)	
10-10-22 14:19:Test: [78/79]	Time: 0.0167(0.0199)	Loss: 38719692210176.000(39992951263828.375)	Prec@1: 18.750(10.000)	
10-10-22 14:19:Step 152 * Prec@1 10.000
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [0/391]	Time 0.739 (0.739)	Data 0.694 (0.694)	Loss 247906768.000 (247906768.000)	Prec@1 6.250 (6.250)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [50/391]	Time 0.036 (0.053)	Data 0.001 (0.015)	Loss 287661632.000 (271856447.843)	Prec@1 9.375 (9.084)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [100/391]	Time 0.046 (0.052)	Data 0.003 (0.009)	Loss 278309120.000 (271183544.396)	Prec@1 11.719 (9.901)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [150/391]	Time 0.052 (0.050)	Data 0.002 (0.006)	Loss 241440272.000 (275033767.417)	Prec@1 13.281 (10.187)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:19:Num bit 8	Num grad bit 8	
10-10-22 14:19:Iter: [200/391]	Time 0.039 (0.047)	Data 0.001 (0.005)	Loss 275155872.000 (276383706.905)	Prec@1 16.406 (10.339)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [250/391]	Time 0.045 (0.049)	Data 0.002 (0.005)	Loss 247492864.000 (279344738.040)	Prec@1 17.188 (10.461)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [300/391]	Time 0.035 (0.047)	Data 0.001 (0.004)	Loss 294326048.000 (287994604.757)	Prec@1 12.500 (10.392)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [350/391]	Time 0.053 (0.048)	Data 0.002 (0.004)	Loss 541876608.000 (306211383.430)	Prec@1 5.469 (10.317)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Test: [0/79]	Time: 0.3187(0.3187)	Loss: 42522873167872.000(42522873167872.000)	Prec@1: 14.844(14.844)	
10-10-22 14:20:Test: [50/79]	Time: 0.0175(0.0241)	Loss: 45662867554304.000(41656170495678.742)	Prec@1: 10.938(9.942)	
10-10-22 14:20:Test: [78/79]	Time: 0.0170(0.0220)	Loss: 45702868631552.000(41800767899697.148)	Prec@1: 0.000(10.000)	
10-10-22 14:20:Step 153 * Prec@1 10.000
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [0/391]	Time 0.395 (0.395)	Data 0.324 (0.324)	Loss 576541824.000 (576541824.000)	Prec@1 7.812 (7.812)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [50/391]	Time 0.032 (0.057)	Data 0.001 (0.008)	Loss 487380576.000 (450406924.549)	Prec@1 7.031 (10.080)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [100/391]	Time 0.031 (0.044)	Data 0.001 (0.005)	Loss 549304768.000 (456686960.158)	Prec@1 7.812 (10.063)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [150/391]	Time 0.051 (0.045)	Data 0.002 (0.004)	Loss 384903616.000 (458693706.172)	Prec@1 15.625 (10.017)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [200/391]	Time 0.037 (0.046)	Data 0.001 (0.003)	Loss 449094688.000 (467878366.567)	Prec@1 7.812 (9.966)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [250/391]	Time 0.046 (0.046)	Data 0.003 (0.003)	Loss 343869248.000 (472406821.482)	Prec@1 14.062 (10.001)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [300/391]	Time 0.031 (0.044)	Data 0.001 (0.003)	Loss 317739744.000 (474431255.920)	Prec@1 8.594 (10.016)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [350/391]	Time 0.069 (0.044)	Data 0.002 (0.003)	Loss 478150592.000 (478889185.368)	Prec@1 11.719 (10.014)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Test: [0/79]	Time: 0.2816(0.2816)	Loss: 90734690566144.000(90734690566144.000)	Prec@1: 8.594(8.594)	
10-10-22 14:20:Test: [50/79]	Time: 0.0201(0.0252)	Loss: 97174440378368.000(91381782523823.688)	Prec@1: 6.250(9.957)	
10-10-22 14:20:Test: [78/79]	Time: 0.0185(0.0234)	Loss: 97520638230528.000(91087826052133.688)	Prec@1: 0.000(10.000)	
10-10-22 14:20:Step 154 * Prec@1 10.000
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [0/391]	Time 0.345 (0.345)	Data 0.311 (0.311)	Loss 599786112.000 (599786112.000)	Prec@1 6.250 (6.250)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [50/391]	Time 0.041 (0.040)	Data 0.002 (0.007)	Loss 441552320.000 (562661611.922)	Prec@1 7.812 (10.064)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [100/391]	Time 0.045 (0.047)	Data 0.002 (0.005)	Loss 653628928.000 (567941435.564)	Prec@1 9.375 (10.048)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [150/391]	Time 0.043 (0.049)	Data 0.002 (0.004)	Loss 847488640.000 (580466171.126)	Prec@1 6.250 (10.161)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [200/391]	Time 0.039 (0.047)	Data 0.001 (0.003)	Loss 697616448.000 (581573748.219)	Prec@1 7.812 (10.121)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [250/391]	Time 0.034 (0.047)	Data 0.001 (0.003)	Loss 691014144.000 (584838234.390)	Prec@1 10.938 (10.134)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [300/391]	Time 0.037 (0.045)	Data 0.001 (0.003)	Loss 792853248.000 (594334665.249)	Prec@1 11.719 (10.058)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [350/391]	Time 0.049 (0.045)	Data 0.001 (0.003)	Loss 706068544.000 (603230552.342)	Prec@1 14.844 (10.096)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Test: [0/79]	Time: 0.2749(0.2749)	Loss: 81117906468864.000(81117906468864.000)	Prec@1: 10.156(10.156)	
10-10-22 14:20:Test: [50/79]	Time: 0.0165(0.0222)	Loss: 90280304836608.000(78526621376351.375)	Prec@1: 10.938(9.957)	
10-10-22 14:20:Test: [78/79]	Time: 0.0149(0.0201)	Loss: 64803049570304.000(78587492200506.984)	Prec@1: 12.500(10.000)	
10-10-22 14:20:Step 155 * Prec@1 10.000
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [0/391]	Time 0.337 (0.337)	Data 0.302 (0.302)	Loss 712221376.000 (712221376.000)	Prec@1 9.375 (9.375)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [50/391]	Time 0.046 (0.042)	Data 0.002 (0.008)	Loss 826177984.000 (713474063.686)	Prec@1 8.594 (9.161)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [100/391]	Time 0.055 (0.043)	Data 0.003 (0.005)	Loss 715967936.000 (717805044.911)	Prec@1 5.469 (9.476)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [150/391]	Time 0.068 (0.045)	Data 0.003 (0.004)	Loss 906367616.000 (723621630.940)	Prec@1 14.062 (9.499)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:20:Num bit 8	Num grad bit 8	
10-10-22 14:20:Iter: [200/391]	Time 0.055 (0.046)	Data 0.002 (0.003)	Loss 809753088.000 (728886441.871)	Prec@1 1.562 (9.756)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [250/391]	Time 0.042 (0.047)	Data 0.001 (0.003)	Loss 779824512.000 (737587046.502)	Prec@1 8.594 (9.730)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [300/391]	Time 0.034 (0.045)	Data 0.001 (0.003)	Loss 641090752.000 (750640341.900)	Prec@1 10.938 (9.676)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [350/391]	Time 0.035 (0.044)	Data 0.001 (0.003)	Loss 1088036480.000 (763609833.846)	Prec@1 8.594 (9.698)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Test: [0/79]	Time: 0.3134(0.3134)	Loss: 72774479511552.000(72774479511552.000)	Prec@1: 8.594(8.594)	
10-10-22 14:21:Test: [50/79]	Time: 0.0190(0.0238)	Loss: 76139267620864.000(74145459406085.016)	Prec@1: 6.250(9.957)	
10-10-22 14:21:Test: [78/79]	Time: 0.0149(0.0214)	Loss: 68146413174784.000(74082318360248.312)	Prec@1: 0.000(10.000)	
10-10-22 14:21:Step 156 * Prec@1 10.000
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [0/391]	Time 0.373 (0.373)	Data 0.338 (0.338)	Loss 612407680.000 (612407680.000)	Prec@1 9.375 (9.375)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [50/391]	Time 0.048 (0.050)	Data 0.002 (0.009)	Loss 1217256192.000 (867312476.863)	Prec@1 6.250 (9.804)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [100/391]	Time 0.049 (0.044)	Data 0.002 (0.005)	Loss 1021666624.000 (864296394.139)	Prec@1 10.156 (10.326)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [150/391]	Time 0.037 (0.042)	Data 0.002 (0.004)	Loss 867938048.000 (867293782.040)	Prec@1 6.250 (10.368)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [200/391]	Time 0.050 (0.044)	Data 0.002 (0.003)	Loss 663023616.000 (868941307.542)	Prec@1 9.375 (10.448)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [250/391]	Time 0.040 (0.043)	Data 0.001 (0.003)	Loss 699669120.000 (873140063.618)	Prec@1 8.594 (10.452)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [300/391]	Time 0.040 (0.042)	Data 0.001 (0.003)	Loss 927073984.000 (893195442.392)	Prec@1 5.469 (10.361)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [350/391]	Time 0.031 (0.041)	Data 0.001 (0.003)	Loss 1271749760.000 (931621611.943)	Prec@1 7.812 (10.339)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Test: [0/79]	Time: 0.3915(0.3915)	Loss: 76565576679424.000(76565576679424.000)	Prec@1: 7.812(7.812)	
10-10-22 14:21:Test: [50/79]	Time: 0.0214(0.0365)	Loss: 69554545885184.000(78899067347425.875)	Prec@1: 13.281(10.386)	
10-10-22 14:21:Test: [78/79]	Time: 0.0149(0.0302)	Loss: 101921385873408.000(78853275010885.219)	Prec@1: 0.000(10.000)	
10-10-22 14:21:Step 157 * Prec@1 10.000
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [0/391]	Time 0.351 (0.351)	Data 0.311 (0.311)	Loss 1635178368.000 (1635178368.000)	Prec@1 10.156 (10.156)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [50/391]	Time 0.041 (0.045)	Data 0.002 (0.007)	Loss 1204578688.000 (1385676902.902)	Prec@1 5.469 (9.819)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [100/391]	Time 0.033 (0.041)	Data 0.002 (0.005)	Loss 1806194048.000 (1458191898.614)	Prec@1 8.594 (9.955)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [150/391]	Time 0.041 (0.042)	Data 0.002 (0.004)	Loss 960003584.000 (1509922348.503)	Prec@1 9.375 (9.887)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [200/391]	Time 0.045 (0.042)	Data 0.001 (0.003)	Loss 2464057344.000 (1644376607.522)	Prec@1 13.281 (10.028)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [250/391]	Time 0.034 (0.043)	Data 0.001 (0.003)	Loss 3439543552.000 (1830699927.713)	Prec@1 5.469 (10.054)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [300/391]	Time 0.051 (0.045)	Data 0.001 (0.003)	Loss 3831318272.000 (2027640478.405)	Prec@1 12.500 (10.055)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [350/391]	Time 0.038 (0.043)	Data 0.002 (0.003)	Loss 4288890880.000 (2230855684.194)	Prec@1 13.281 (10.027)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Test: [0/79]	Time: 0.3143(0.3143)	Loss: 263774376296448.000(263774376296448.000)	Prec@1: 13.281(13.281)	
10-10-22 14:21:Test: [50/79]	Time: 0.0162(0.0236)	Loss: 255697136648192.000(269191778489685.344)	Prec@1: 15.625(9.773)	
10-10-22 14:21:Test: [78/79]	Time: 0.0148(0.0210)	Loss: 218907772190720.000(267386824702296.062)	Prec@1: 18.750(10.000)	
10-10-22 14:21:Step 158 * Prec@1 10.000
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [0/391]	Time 0.372 (0.372)	Data 0.331 (0.331)	Loss 4896760832.000 (4896760832.000)	Prec@1 7.031 (7.031)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [50/391]	Time 0.040 (0.054)	Data 0.002 (0.008)	Loss 4704581120.000 (3993166456.471)	Prec@1 8.594 (9.926)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [100/391]	Time 0.037 (0.046)	Data 0.002 (0.005)	Loss 4260688128.000 (4005469802.455)	Prec@1 10.156 (9.530)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [150/391]	Time 0.035 (0.045)	Data 0.002 (0.004)	Loss 5031535616.000 (4113944062.305)	Prec@1 13.281 (9.722)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [200/391]	Time 0.040 (0.044)	Data 0.002 (0.003)	Loss 5335413248.000 (4239233181.930)	Prec@1 11.719 (9.756)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [250/391]	Time 0.038 (0.042)	Data 0.001 (0.003)	Loss 4494787584.000 (4360380353.785)	Prec@1 7.031 (9.805)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:21:Num bit 8	Num grad bit 8	
10-10-22 14:21:Iter: [300/391]	Time 0.032 (0.042)	Data 0.001 (0.003)	Loss 4022027264.000 (4477225818.153)	Prec@1 14.844 (9.941)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [350/391]	Time 0.044 (0.043)	Data 0.002 (0.003)	Loss 4553255936.000 (4620178342.291)	Prec@1 6.250 (9.882)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Test: [0/79]	Time: 0.3142(0.3142)	Loss: 5673565290496.000(5673565290496.000)	Prec@1: 10.156(10.156)	
10-10-22 14:22:Test: [50/79]	Time: 0.0164(0.0223)	Loss: 5889764360192.000(5637335879439.059)	Prec@1: 10.938(9.957)	
10-10-22 14:22:Test: [78/79]	Time: 0.0169(0.0206)	Loss: 5291463147520.000(5642209437155.328)	Prec@1: 12.500(10.000)	
10-10-22 14:22:Step 159 * Prec@1 10.000
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [0/391]	Time 0.671 (0.671)	Data 0.495 (0.495)	Loss 3179505152.000 (3179505152.000)	Prec@1 13.281 (13.281)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [50/391]	Time 0.059 (0.052)	Data 0.002 (0.011)	Loss 5710201856.000 (5226845023.373)	Prec@1 8.594 (10.478)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [100/391]	Time 0.060 (0.047)	Data 0.002 (0.007)	Loss 6693240832.000 (5343653817.030)	Prec@1 4.688 (10.265)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [150/391]	Time 0.035 (0.047)	Data 0.002 (0.005)	Loss 5423975936.000 (5382200547.179)	Prec@1 13.281 (10.089)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [200/391]	Time 0.036 (0.045)	Data 0.001 (0.004)	Loss 5176018944.000 (5380086522.905)	Prec@1 6.250 (10.246)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [250/391]	Time 0.059 (0.043)	Data 0.003 (0.004)	Loss 7688467456.000 (5442172700.558)	Prec@1 7.812 (10.147)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [300/391]	Time 0.037 (0.046)	Data 0.002 (0.004)	Loss 5768567296.000 (5471896556.439)	Prec@1 10.156 (10.278)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [350/391]	Time 0.062 (0.046)	Data 0.001 (0.003)	Loss 5394553856.000 (5541441371.897)	Prec@1 14.062 (10.301)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Test: [0/79]	Time: 0.3225(0.3225)	Loss: 12924761407488.000(12924761407488.000)	Prec@1: 10.156(10.156)	
10-10-22 14:22:Test: [50/79]	Time: 0.0205(0.0250)	Loss: 12195063660544.000(13042193572000.627)	Prec@1: 10.938(9.957)	
10-10-22 14:22:Test: [78/79]	Time: 0.0231(0.0236)	Loss: 11124462321664.000(13002593202601.984)	Prec@1: 12.500(10.000)	
10-10-22 14:22:Step 160 * Prec@1 10.000
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [0/391]	Time 0.535 (0.535)	Data 0.463 (0.463)	Loss 6703787520.000 (6703787520.000)	Prec@1 8.594 (8.594)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [50/391]	Time 0.046 (0.063)	Data 0.002 (0.011)	Loss 7825991680.000 (7053577125.647)	Prec@1 8.594 (10.386)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [100/391]	Time 0.039 (0.056)	Data 0.001 (0.006)	Loss 7850293248.000 (7186344772.436)	Prec@1 10.938 (10.009)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [150/391]	Time 0.032 (0.049)	Data 0.001 (0.005)	Loss 7492525056.000 (7125757551.894)	Prec@1 7.812 (10.006)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [200/391]	Time 0.034 (0.046)	Data 0.001 (0.004)	Loss 6738163200.000 (7227466306.229)	Prec@1 9.375 (9.958)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [250/391]	Time 0.041 (0.044)	Data 0.002 (0.003)	Loss 5890342400.000 (7293371010.550)	Prec@1 10.938 (10.069)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [300/391]	Time 0.055 (0.046)	Data 0.001 (0.003)	Loss 5665672192.000 (7360848343.176)	Prec@1 7.812 (10.068)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [350/391]	Time 0.033 (0.045)	Data 0.001 (0.003)	Loss 9209241600.000 (7386259111.020)	Prec@1 7.031 (10.085)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Test: [0/79]	Time: 0.3471(0.3471)	Loss: 12798815895552.000(12798815895552.000)	Prec@1: 10.156(10.156)	
10-10-22 14:22:Test: [50/79]	Time: 0.0163(0.0251)	Loss: 12575463964672.000(13367550153748.078)	Prec@1: 10.938(9.957)	
10-10-22 14:22:Test: [78/79]	Time: 0.0151(0.0221)	Loss: 11556508139520.000(13336655945111.961)	Prec@1: 12.500(10.000)	
10-10-22 14:22:Step 161 * Prec@1 10.000
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [0/391]	Time 0.373 (0.373)	Data 0.339 (0.339)	Loss 8328224768.000 (8328224768.000)	Prec@1 8.594 (8.594)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [50/391]	Time 0.043 (0.043)	Data 0.002 (0.008)	Loss 7906165248.000 (8564386032.941)	Prec@1 7.812 (9.452)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [100/391]	Time 0.061 (0.044)	Data 0.002 (0.005)	Loss 7772564992.000 (8570439816.871)	Prec@1 5.469 (9.793)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [150/391]	Time 0.035 (0.045)	Data 0.001 (0.004)	Loss 12119076864.000 (8566234383.258)	Prec@1 13.281 (10.027)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [200/391]	Time 0.040 (0.043)	Data 0.001 (0.003)	Loss 7450982400.000 (8519435600.239)	Prec@1 7.031 (10.063)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [250/391]	Time 0.032 (0.043)	Data 0.001 (0.003)	Loss 6464338944.000 (8566670001.466)	Prec@1 6.250 (10.007)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [300/391]	Time 0.045 (0.042)	Data 0.001 (0.003)	Loss 8619539456.000 (8571988275.880)	Prec@1 6.250 (9.977)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:22:Num bit 8	Num grad bit 8	
10-10-22 14:22:Iter: [350/391]	Time 0.038 (0.043)	Data 0.002 (0.003)	Loss 9374787584.000 (8617658238.177)	Prec@1 6.250 (9.945)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Test: [0/79]	Time: 0.2800(0.2800)	Loss: 4137508405248.000(4137508405248.000)	Prec@1: 8.594(8.594)	
10-10-22 14:23:Test: [50/79]	Time: 0.0171(0.0224)	Loss: 3770351353856.000(3691817442203.608)	Prec@1: 6.250(9.957)	
10-10-22 14:23:Test: [78/79]	Time: 0.0195(0.0212)	Loss: 3811394191360.000(3692958787095.757)	Prec@1: 0.000(10.000)	
10-10-22 14:23:Step 162 * Prec@1 10.000
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [0/391]	Time 0.410 (0.410)	Data 0.361 (0.361)	Loss 7401872896.000 (7401872896.000)	Prec@1 10.938 (10.938)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [50/391]	Time 0.046 (0.054)	Data 0.002 (0.009)	Loss 10492481536.000 (9232351352.471)	Prec@1 13.281 (9.911)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [100/391]	Time 0.056 (0.052)	Data 0.002 (0.005)	Loss 12800451584.000 (9255667265.901)	Prec@1 7.031 (9.870)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [150/391]	Time 0.039 (0.048)	Data 0.001 (0.004)	Loss 9530984448.000 (9278404502.887)	Prec@1 10.938 (9.779)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [200/391]	Time 0.033 (0.045)	Data 0.001 (0.003)	Loss 10847345664.000 (9269590596.776)	Prec@1 8.594 (9.787)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [250/391]	Time 0.090 (0.044)	Data 0.004 (0.003)	Loss 8567135232.000 (9315608449.530)	Prec@1 7.812 (9.783)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [300/391]	Time 0.054 (0.045)	Data 0.002 (0.003)	Loss 9344507904.000 (9287447631.947)	Prec@1 14.844 (9.868)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [350/391]	Time 0.035 (0.044)	Data 0.001 (0.003)	Loss 7730486784.000 (9244336273.869)	Prec@1 10.156 (9.940)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Test: [0/79]	Time: 0.3168(0.3168)	Loss: 9205615427584.000(9205615427584.000)	Prec@1: 7.812(7.812)	
10-10-22 14:23:Test: [50/79]	Time: 0.0188(0.0236)	Loss: 8783144157184.000(8639196620518.902)	Prec@1: 13.281(10.386)	
10-10-22 14:23:Test: [78/79]	Time: 0.0154(0.0211)	Loss: 9595480178688.000(8687571565123.993)	Prec@1: 0.000(10.000)	
10-10-22 14:23:Step 163 * Prec@1 10.000
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [0/391]	Time 0.374 (0.374)	Data 0.335 (0.335)	Loss 7912854528.000 (7912854528.000)	Prec@1 14.844 (14.844)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [50/391]	Time 0.036 (0.057)	Data 0.001 (0.009)	Loss 4665762816.000 (9107825523.451)	Prec@1 11.719 (10.248)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [100/391]	Time 0.039 (0.048)	Data 0.001 (0.005)	Loss 7693796864.000 (9060087747.168)	Prec@1 7.031 (9.955)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [150/391]	Time 0.033 (0.043)	Data 0.001 (0.004)	Loss 12502665216.000 (9159894938.278)	Prec@1 10.156 (10.084)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [200/391]	Time 0.047 (0.043)	Data 0.002 (0.003)	Loss 10699937792.000 (9141440962.866)	Prec@1 9.375 (9.923)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [250/391]	Time 0.034 (0.041)	Data 0.001 (0.003)	Loss 8724897792.000 (9222946946.550)	Prec@1 7.812 (9.885)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [300/391]	Time 0.038 (0.041)	Data 0.002 (0.003)	Loss 7426948608.000 (9230964531.880)	Prec@1 10.938 (10.032)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [350/391]	Time 0.031 (0.041)	Data 0.001 (0.003)	Loss 10615246848.000 (9332160649.117)	Prec@1 14.062 (10.210)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Test: [0/79]	Time: 0.4916(0.4916)	Loss: 13204325400576.000(13204325400576.000)	Prec@1: 14.844(14.844)	
10-10-22 14:23:Test: [50/79]	Time: 0.0175(0.0277)	Loss: 13389760823296.000(13618919069013.334)	Prec@1: 10.938(9.942)	
10-10-22 14:23:Test: [78/79]	Time: 0.0148(0.0238)	Loss: 15674857488384.000(13617031175313.818)	Prec@1: 0.000(10.000)	
10-10-22 14:23:Step 164 * Prec@1 10.000
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [0/391]	Time 0.412 (0.412)	Data 0.373 (0.373)	Loss 17144379392.000 (17144379392.000)	Prec@1 9.375 (9.375)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [50/391]	Time 0.037 (0.045)	Data 0.001 (0.009)	Loss 10281882624.000 (13391367268.392)	Prec@1 12.500 (10.156)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [100/391]	Time 0.043 (0.044)	Data 0.002 (0.005)	Loss 14078764032.000 (13996622432.317)	Prec@1 10.156 (10.071)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [150/391]	Time 0.032 (0.042)	Data 0.001 (0.004)	Loss 21898098688.000 (15378131201.695)	Prec@1 13.281 (9.949)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [200/391]	Time 0.045 (0.041)	Data 0.001 (0.003)	Loss 20657248256.000 (17322778231.721)	Prec@1 18.750 (10.036)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [250/391]	Time 0.035 (0.042)	Data 0.002 (0.003)	Loss 56714907648.000 (21534512797.068)	Prec@1 6.250 (9.929)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [300/391]	Time 0.044 (0.042)	Data 0.001 (0.003)	Loss 87317602304.000 (29062497599.787)	Prec@1 12.500 (9.876)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [350/391]	Time 0.047 (0.042)	Data 0.002 (0.003)	Loss 89818546176.000 (41936932557.675)	Prec@1 12.500 (9.936)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Test: [0/79]	Time: 0.2884(0.2884)	Loss: 63156730724352.000(63156730724352.000)	Prec@1: 7.812(7.812)	
10-10-22 14:23:Test: [50/79]	Time: 0.0819(0.0253)	Loss: 58709447278592.000(57255552079831.844)	Prec@1: 13.281(10.386)	
10-10-22 14:23:Test: [78/79]	Time: 0.0154(0.0235)	Loss: 76521528098816.000(57504315714253.617)	Prec@1: 0.000(10.000)	
10-10-22 14:23:Step 165 * Prec@1 10.000
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [0/391]	Time 0.339 (0.339)	Data 0.296 (0.296)	Loss 197068587008.000 (197068587008.000)	Prec@1 12.500 (12.500)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:23:Num bit 8	Num grad bit 8	
10-10-22 14:23:Iter: [50/391]	Time 0.047 (0.045)	Data 0.002 (0.007)	Loss 187530002432.000 (232505964664.471)	Prec@1 10.156 (9.743)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [100/391]	Time 0.031 (0.044)	Data 0.001 (0.005)	Loss 229190582272.000 (237683191199.683)	Prec@1 12.500 (9.646)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [150/391]	Time 0.071 (0.045)	Data 0.003 (0.004)	Loss 162055618560.000 (241636218812.185)	Prec@1 10.156 (9.603)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [200/391]	Time 0.034 (0.048)	Data 0.001 (0.003)	Loss 214363832320.000 (251629627993.154)	Prec@1 5.469 (9.678)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [250/391]	Time 0.035 (0.047)	Data 0.001 (0.003)	Loss 223920570368.000 (264211553357.514)	Prec@1 10.156 (9.646)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [300/391]	Time 0.033 (0.046)	Data 0.001 (0.003)	Loss 278315892736.000 (272287425791.150)	Prec@1 5.469 (9.694)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [350/391]	Time 0.046 (0.046)	Data 0.002 (0.003)	Loss 261025775616.000 (277883492649.573)	Prec@1 10.156 (9.684)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Test: [0/79]	Time: 0.3094(0.3094)	Loss: 211501335969792.000(211501335969792.000)	Prec@1: 14.844(14.844)	
10-10-22 14:24:Test: [50/79]	Time: 0.0160(0.0224)	Loss: 235976962080768.000(221912715991963.594)	Prec@1: 10.938(9.942)	
10-10-22 14:24:Test: [78/79]	Time: 0.0147(0.0201)	Loss: 207710037475328.000(222193578374751.844)	Prec@1: 0.000(10.000)	
10-10-22 14:24:Step 166 * Prec@1 10.000
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [0/391]	Time 0.377 (0.377)	Data 0.340 (0.340)	Loss 204159647744.000 (204159647744.000)	Prec@1 8.594 (8.594)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [50/391]	Time 0.050 (0.053)	Data 0.004 (0.009)	Loss 254929174528.000 (285736928195.765)	Prec@1 9.375 (9.498)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [100/391]	Time 0.042 (0.049)	Data 0.003 (0.005)	Loss 292179771392.000 (285246382809.980)	Prec@1 9.375 (9.916)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [150/391]	Time 0.037 (0.048)	Data 0.002 (0.004)	Loss 277981528064.000 (281559098917.298)	Prec@1 9.375 (9.851)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [200/391]	Time 0.062 (0.047)	Data 0.002 (0.004)	Loss 186043449344.000 (276986459273.552)	Prec@1 7.031 (9.954)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [250/391]	Time 0.047 (0.046)	Data 0.001 (0.003)	Loss 291092594688.000 (274944149610.072)	Prec@1 10.156 (9.795)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [300/391]	Time 0.056 (0.047)	Data 0.002 (0.003)	Loss 199978221568.000 (270991173189.741)	Prec@1 11.719 (9.764)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [350/391]	Time 0.033 (0.047)	Data 0.001 (0.003)	Loss 236132777984.000 (264099890202.256)	Prec@1 12.500 (9.894)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Test: [0/79]	Time: 0.4442(0.4442)	Loss: 251369889988608.000(251369889988608.000)	Prec@1: 14.844(14.844)	
10-10-22 14:24:Test: [50/79]	Time: 0.0164(0.0277)	Loss: 256605237018624.000(269803234653685.969)	Prec@1: 10.938(9.942)	
10-10-22 14:24:Test: [78/79]	Time: 0.0159(0.0248)	Loss: 271532647514112.000(269357749089861.625)	Prec@1: 0.000(10.000)	
10-10-22 14:24:Step 167 * Prec@1 10.000
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [0/391]	Time 0.295 (0.295)	Data 0.255 (0.255)	Loss 177262641152.000 (177262641152.000)	Prec@1 16.406 (16.406)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [50/391]	Time 0.044 (0.042)	Data 0.001 (0.006)	Loss 200188575744.000 (222986406410.039)	Prec@1 9.375 (10.463)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [100/391]	Time 0.037 (0.042)	Data 0.001 (0.004)	Loss 198095093760.000 (223808656941.624)	Prec@1 9.375 (10.141)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [150/391]	Time 0.042 (0.042)	Data 0.002 (0.003)	Loss 234333274112.000 (223104231491.815)	Prec@1 10.156 (10.089)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [200/391]	Time 0.039 (0.043)	Data 0.002 (0.003)	Loss 291596599296.000 (219317598951.801)	Prec@1 10.938 (10.117)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [250/391]	Time 0.044 (0.045)	Data 0.002 (0.003)	Loss 150950920192.000 (213535215175.394)	Prec@1 8.594 (10.134)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [300/391]	Time 0.041 (0.044)	Data 0.001 (0.003)	Loss 134352093184.000 (207167608757.156)	Prec@1 12.500 (10.193)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [350/391]	Time 0.039 (0.044)	Data 0.001 (0.002)	Loss 182212919296.000 (199874240441.983)	Prec@1 8.594 (10.207)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Test: [0/79]	Time: 0.2770(0.2770)	Loss: 293934072856576.000(293934072856576.000)	Prec@1: 14.844(14.844)	
10-10-22 14:24:Test: [50/79]	Time: 0.0165(0.0226)	Loss: 301330207866880.000(319728668094383.688)	Prec@1: 10.938(9.942)	
10-10-22 14:24:Test: [78/79]	Time: 0.0149(0.0207)	Loss: 339296309477376.000(318970203232377.250)	Prec@1: 0.000(10.000)	
10-10-22 14:24:Step 168 * Prec@1 10.000
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [0/391]	Time 0.415 (0.415)	Data 0.378 (0.378)	Loss 185770033152.000 (185770033152.000)	Prec@1 7.031 (7.031)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [50/391]	Time 0.032 (0.039)	Data 0.001 (0.009)	Loss 130987507712.000 (145840247828.078)	Prec@1 12.500 (10.003)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:24:Num bit 8	Num grad bit 8	
10-10-22 14:24:Iter: [100/391]	Time 0.036 (0.038)	Data 0.002 (0.005)	Loss 98309906432.000 (148782985094.337)	Prec@1 7.031 (9.692)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [150/391]	Time 0.034 (0.037)	Data 0.002 (0.004)	Loss 164604002304.000 (154407538660.874)	Prec@1 7.031 (9.825)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [200/391]	Time 0.031 (0.037)	Data 0.001 (0.003)	Loss 194808807424.000 (160629343145.393)	Prec@1 16.406 (10.075)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [250/391]	Time 0.050 (0.039)	Data 0.002 (0.003)	Loss 124944302080.000 (166597369660.175)	Prec@1 10.156 (10.172)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [300/391]	Time 0.056 (0.042)	Data 0.002 (0.003)	Loss 210164547584.000 (172701161805.395)	Prec@1 9.375 (10.120)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [350/391]	Time 0.046 (0.044)	Data 0.003 (0.003)	Loss 215949934592.000 (179341448206.587)	Prec@1 8.594 (10.161)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Test: [0/79]	Time: 0.3171(0.3171)	Loss: 259557188173824.000(259557188173824.000)	Prec@1: 7.812(7.812)	
10-10-22 14:25:Test: [50/79]	Time: 0.0163(0.0223)	Loss: 251285165047808.000(258609165595507.438)	Prec@1: 13.281(10.386)	
10-10-22 14:25:Test: [78/79]	Time: 0.0176(0.0216)	Loss: 323837614882816.000(259615256668536.844)	Prec@1: 0.000(10.000)	
10-10-22 14:25:Step 169 * Prec@1 10.000
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [0/391]	Time 0.364 (0.364)	Data 0.312 (0.312)	Loss 242675023872.000 (242675023872.000)	Prec@1 8.594 (8.594)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [50/391]	Time 0.045 (0.050)	Data 0.001 (0.008)	Loss 247962206208.000 (249808415081.412)	Prec@1 10.938 (9.972)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [100/391]	Time 0.045 (0.048)	Data 0.001 (0.005)	Loss 297967779840.000 (250957146578.376)	Prec@1 8.594 (10.063)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [150/391]	Time 0.036 (0.046)	Data 0.001 (0.004)	Loss 261506957312.000 (255852638723.391)	Prec@1 7.031 (10.084)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [200/391]	Time 0.046 (0.045)	Data 0.001 (0.003)	Loss 200874917888.000 (264605802047.682)	Prec@1 10.156 (10.079)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [250/391]	Time 0.058 (0.045)	Data 0.002 (0.003)	Loss 419999285248.000 (273196897120.892)	Prec@1 11.719 (10.057)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [300/391]	Time 0.039 (0.045)	Data 0.002 (0.003)	Loss 359905918976.000 (280461519331.083)	Prec@1 7.812 (9.995)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [350/391]	Time 0.032 (0.044)	Data 0.002 (0.003)	Loss 364165496832.000 (284584291593.482)	Prec@1 10.938 (9.956)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Test: [0/79]	Time: 0.4492(0.4492)	Loss: 215352445239296.000(215352445239296.000)	Prec@1: 14.844(14.844)	
10-10-22 14:25:Test: [50/79]	Time: 0.0165(0.0287)	Loss: 223957999419392.000(220664165775159.219)	Prec@1: 10.938(9.942)	
10-10-22 14:25:Test: [78/79]	Time: 0.0148(0.0245)	Loss: 282648610078720.000(221136988690553.250)	Prec@1: 0.000(10.000)	
10-10-22 14:25:Step 170 * Prec@1 10.000
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [0/391]	Time 0.406 (0.406)	Data 0.369 (0.369)	Loss 242591825920.000 (242591825920.000)	Prec@1 10.156 (10.156)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [50/391]	Time 0.073 (0.063)	Data 0.003 (0.009)	Loss 354497265664.000 (292953976109.176)	Prec@1 10.156 (10.064)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [100/391]	Time 0.040 (0.052)	Data 0.001 (0.005)	Loss 332967837696.000 (294941782218.772)	Prec@1 12.500 (9.978)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [150/391]	Time 0.035 (0.047)	Data 0.002 (0.004)	Loss 264369127424.000 (305111405764.662)	Prec@1 14.844 (9.815)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [200/391]	Time 0.044 (0.047)	Data 0.002 (0.004)	Loss 337069539328.000 (313370730292.219)	Prec@1 10.938 (9.838)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [250/391]	Time 0.031 (0.046)	Data 0.001 (0.003)	Loss 269757775872.000 (321647909508.590)	Prec@1 8.594 (9.826)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [300/391]	Time 0.049 (0.044)	Data 0.001 (0.003)	Loss 376631263232.000 (329381938839.389)	Prec@1 14.844 (10.021)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [350/391]	Time 0.046 (0.044)	Data 0.001 (0.003)	Loss 338089574400.000 (340272156496.957)	Prec@1 7.031 (10.032)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Test: [0/79]	Time: 0.2952(0.2952)	Loss: 75691030740992.000(75691030740992.000)	Prec@1: 13.281(13.281)	
10-10-22 14:25:Test: [50/79]	Time: 0.0164(0.0251)	Loss: 73929691496448.000(78159606946715.609)	Prec@1: 15.625(9.773)	
10-10-22 14:25:Test: [78/79]	Time: 0.0150(0.0227)	Loss: 54851060891648.000(78030974799472.234)	Prec@1: 18.750(10.000)	
10-10-22 14:25:Step 171 * Prec@1 10.000
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [0/391]	Time 0.412 (0.412)	Data 0.368 (0.368)	Loss 424819851264.000 (424819851264.000)	Prec@1 14.062 (14.062)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [50/391]	Time 0.031 (0.055)	Data 0.001 (0.009)	Loss 372016349184.000 (394057204675.765)	Prec@1 10.938 (10.126)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:25:Num bit 8	Num grad bit 8	
10-10-22 14:25:Iter: [100/391]	Time 0.031 (0.045)	Data 0.001 (0.005)	Loss 379192377344.000 (387500578511.842)	Prec@1 11.719 (10.295)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [150/391]	Time 0.036 (0.047)	Data 0.001 (0.004)	Loss 352750829568.000 (384797133247.576)	Prec@1 12.500 (10.136)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [200/391]	Time 0.034 (0.045)	Data 0.001 (0.004)	Loss 299771166720.000 (383290709985.433)	Prec@1 11.719 (10.075)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [250/391]	Time 0.037 (0.044)	Data 0.002 (0.003)	Loss 520865349632.000 (387618279113.944)	Prec@1 7.812 (10.134)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [300/391]	Time 0.091 (0.044)	Data 0.003 (0.003)	Loss 566450454528.000 (395834911692.970)	Prec@1 10.938 (9.995)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [350/391]	Time 0.036 (0.044)	Data 0.001 (0.003)	Loss 493783449600.000 (401680189399.157)	Prec@1 11.719 (10.047)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Test: [0/79]	Time: 0.3110(0.3110)	Loss: 85888088408064.000(85888088408064.000)	Prec@1: 11.719(11.719)	
10-10-22 14:26:Test: [50/79]	Time: 0.0174(0.0252)	Loss: 91436431179776.000(91816754876175.062)	Prec@1: 7.031(10.003)	
10-10-22 14:26:Test: [78/79]	Time: 0.0175(0.0235)	Loss: 71919797796864.000(91772183215131.859)	Prec@1: 18.750(10.000)	
10-10-22 14:26:Step 172 * Prec@1 10.000
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [0/391]	Time 0.378 (0.378)	Data 0.332 (0.332)	Loss 564286849024.000 (564286849024.000)	Prec@1 7.812 (7.812)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [50/391]	Time 0.047 (0.048)	Data 0.002 (0.008)	Loss 606580375552.000 (461074451275.294)	Prec@1 9.375 (10.141)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [100/391]	Time 0.032 (0.042)	Data 0.001 (0.005)	Loss 371514933248.000 (455402869668.753)	Prec@1 11.719 (10.179)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [150/391]	Time 0.048 (0.042)	Data 0.002 (0.004)	Loss 537993543680.000 (456597244894.093)	Prec@1 8.594 (10.027)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [200/391]	Time 0.040 (0.042)	Data 0.001 (0.003)	Loss 448893583360.000 (460790642209.114)	Prec@1 11.719 (9.997)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [250/391]	Time 0.044 (0.043)	Data 0.002 (0.003)	Loss 384287309824.000 (464693327570.104)	Prec@1 12.500 (9.889)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [300/391]	Time 0.038 (0.042)	Data 0.002 (0.003)	Loss 435293028352.000 (459663403164.492)	Prec@1 7.812 (9.832)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [350/391]	Time 0.033 (0.042)	Data 0.001 (0.003)	Loss 274945900544.000 (449719876558.405)	Prec@1 16.406 (9.849)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Test: [0/79]	Time: 0.2898(0.2898)	Loss: 75000463753216.000(75000463753216.000)	Prec@1: 14.844(14.844)	
10-10-22 14:26:Test: [50/79]	Time: 0.0161(0.0227)	Loss: 73833004400640.000(73557969166175.375)	Prec@1: 10.938(9.942)	
10-10-22 14:26:Test: [78/79]	Time: 0.0149(0.0204)	Loss: 87755946196992.000(73744861149056.203)	Prec@1: 0.000(10.000)	
10-10-22 14:26:Step 173 * Prec@1 10.000
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [0/391]	Time 0.325 (0.325)	Data 0.291 (0.291)	Loss 282602700800.000 (282602700800.000)	Prec@1 16.406 (16.406)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [50/391]	Time 0.035 (0.038)	Data 0.001 (0.007)	Loss 319571525632.000 (296154928790.588)	Prec@1 7.812 (10.309)	Training FLOPS ratio: 0.062500 (0.062500)	
10-10-22 14:26:Num bit 8	Num grad bit 8	
10-10-22 14:26:Iter: [100/391]	Time 0.040 (0.037)	Data 0.001 (0.004)	Loss 294642221056.000 (292964127328.317)	Prec@1 5.469 (10.241)	Training FLOPS ratio: 0.062500 (0.062500)	
