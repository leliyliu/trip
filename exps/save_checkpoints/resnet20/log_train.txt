06-15-23 18:25:start training resnet20:
06-15-23 18:25:Training messages:
*********************************************************
* Training Setting:                                     *
*     1. weight_update();                               *
*     2. add zero_grad2();                              *
*     3. zero_grad2() executes according to new         *
*        parameters;                                    *
*     4. zero_gr = 0;                                   *
*     5. weight_decay=1e-3;                             *
* Methods: Trip with 8+8;                               *
*********************************************************
06-15-23 18:25:Num bit 8	Num grad bit 8	
06-15-23 18:25:Iter: [0/391]	Time 0.414 (0.414)	Data 0.372 (0.372)	Loss 2.643 (2.643)	Prec@1 14.062 (14.062)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:25:Num bit 8	Num grad bit 8	
06-15-23 18:25:Iter: [50/391]	Time 0.051 (0.056)	Data 0.002 (0.009)	Loss 1.861 (2.047)	Prec@1 30.469 (24.357)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:25:Num bit 8	Num grad bit 8	
06-15-23 18:25:Iter: [100/391]	Time 0.047 (0.051)	Data 0.002 (0.006)	Loss 1.797 (1.938)	Prec@1 28.906 (27.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:25:Num bit 8	Num grad bit 8	
06-15-23 18:25:Iter: [150/391]	Time 0.074 (0.056)	Data 0.003 (0.005)	Loss 1.603 (1.867)	Prec@1 40.625 (30.220)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:25:Num bit 8	Num grad bit 8	
06-15-23 18:25:Iter: [200/391]	Time 0.071 (0.059)	Data 0.003 (0.004)	Loss 1.547 (1.818)	Prec@1 44.531 (32.276)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:25:Num bit 8	Num grad bit 8	
06-15-23 18:25:Iter: [250/391]	Time 0.066 (0.060)	Data 0.002 (0.004)	Loss 1.332 (1.770)	Prec@1 51.562 (33.945)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:25:Num bit 8	Num grad bit 8	
06-15-23 18:25:Iter: [300/391]	Time 0.066 (0.061)	Data 0.003 (0.004)	Loss 1.352 (1.730)	Prec@1 50.000 (35.608)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:25:Num bit 8	Num grad bit 8	
06-15-23 18:25:Iter: [350/391]	Time 0.072 (0.062)	Data 0.003 (0.004)	Loss 1.333 (1.696)	Prec@1 51.562 (36.915)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:25:Test: [0/79]	Time: 0.3658(0.3658)	Loss: 1.441(1.441)	Prec@1: 47.656(47.656)	
06-15-23 18:25:Test: [50/79]	Time: 0.0166(0.0234)	Loss: 1.382(1.541)	Prec@1: 50.781(44.347)	
06-15-23 18:25:Test: [78/79]	Time: 0.0163(0.0210)	Loss: 1.304(1.533)	Prec@1: 43.750(44.470)	
06-15-23 18:25:Step 0 * Prec@1 44.470
06-15-23 18:25:Num bit 8	Num grad bit 8	
06-15-23 18:25:Iter: [0/391]	Time 0.390 (0.390)	Data 0.354 (0.354)	Loss 1.516 (1.516)	Prec@1 43.750 (43.750)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:25:Num bit 8	Num grad bit 8	
06-15-23 18:25:Iter: [50/391]	Time 0.073 (0.072)	Data 0.003 (0.009)	Loss 1.568 (1.434)	Prec@1 46.094 (47.794)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:25:Num bit 8	Num grad bit 8	
06-15-23 18:25:Iter: [100/391]	Time 0.077 (0.067)	Data 0.003 (0.006)	Loss 1.416 (1.406)	Prec@1 54.688 (48.608)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [150/391]	Time 0.053 (0.067)	Data 0.002 (0.005)	Loss 1.393 (1.399)	Prec@1 51.562 (48.805)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [200/391]	Time 0.076 (0.067)	Data 0.003 (0.004)	Loss 1.142 (1.379)	Prec@1 54.688 (49.363)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [250/391]	Time 0.078 (0.069)	Data 0.002 (0.004)	Loss 1.369 (1.362)	Prec@1 44.531 (50.078)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [300/391]	Time 0.075 (0.070)	Data 0.003 (0.004)	Loss 1.203 (1.343)	Prec@1 53.906 (50.882)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError(0, 'Error'))': /api/5288891/envelope/
06-15-23 18:26:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError(0, 'Error'))': /api/5288891/envelope/
06-15-23 18:26:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError(0, 'Error'))': /api/5288891/envelope/
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [350/391]	Time 0.092 (0.070)	Data 0.003 (0.004)	Loss 1.288 (1.326)	Prec@1 50.781 (51.587)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Test: [0/79]	Time: 0.3800(0.3800)	Loss: 1.273(1.273)	Prec@1: 53.906(53.906)	
06-15-23 18:26:Test: [50/79]	Time: 0.0166(0.0238)	Loss: 1.260(1.248)	Prec@1: 50.781(56.005)	
06-15-23 18:26:Test: [78/79]	Time: 0.0150(0.0212)	Loss: 1.181(1.256)	Prec@1: 50.000(55.300)	
06-15-23 18:26:Step 1 * Prec@1 55.300
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [0/391]	Time 0.414 (0.414)	Data 0.366 (0.366)	Loss 1.295 (1.295)	Prec@1 53.906 (53.906)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [50/391]	Time 0.081 (0.072)	Data 0.003 (0.010)	Loss 1.270 (1.176)	Prec@1 53.125 (56.725)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [100/391]	Time 0.076 (0.073)	Data 0.003 (0.006)	Loss 1.133 (1.178)	Prec@1 58.594 (56.606)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [150/391]	Time 0.059 (0.071)	Data 0.002 (0.005)	Loss 1.142 (1.167)	Prec@1 56.250 (57.352)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [200/391]	Time 0.055 (0.067)	Data 0.002 (0.004)	Loss 1.114 (1.152)	Prec@1 60.938 (58.092)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [250/391]	Time 0.054 (0.065)	Data 0.002 (0.004)	Loss 1.120 (1.141)	Prec@1 60.156 (58.507)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [300/391]	Time 0.058 (0.063)	Data 0.002 (0.004)	Loss 1.155 (1.130)	Prec@1 53.125 (58.955)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [350/391]	Time 0.057 (0.063)	Data 0.002 (0.003)	Loss 1.183 (1.123)	Prec@1 54.688 (59.233)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Test: [0/79]	Time: 0.3985(0.3985)	Loss: 1.085(1.085)	Prec@1: 60.938(60.938)	
06-15-23 18:26:Test: [50/79]	Time: 0.0318(0.0330)	Loss: 1.196(1.175)	Prec@1: 57.031(59.513)	
06-15-23 18:26:Test: [78/79]	Time: 0.0184(0.0292)	Loss: 1.008(1.181)	Prec@1: 56.250(59.160)	
06-15-23 18:26:Step 2 * Prec@1 59.160
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [0/391]	Time 0.391 (0.391)	Data 0.351 (0.351)	Loss 1.035 (1.035)	Prec@1 65.625 (65.625)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [50/391]	Time 0.050 (0.063)	Data 0.002 (0.009)	Loss 1.210 (1.018)	Prec@1 58.594 (63.848)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [100/391]	Time 0.053 (0.058)	Data 0.002 (0.006)	Loss 1.120 (1.038)	Prec@1 64.844 (62.809)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [150/391]	Time 0.064 (0.056)	Data 0.002 (0.005)	Loss 1.067 (1.029)	Prec@1 57.812 (62.852)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:26:Num bit 8	Num grad bit 8	
06-15-23 18:26:Iter: [200/391]	Time 0.069 (0.059)	Data 0.003 (0.004)	Loss 1.095 (1.017)	Prec@1 57.812 (63.316)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [250/391]	Time 0.071 (0.061)	Data 0.003 (0.004)	Loss 0.921 (1.010)	Prec@1 64.844 (63.571)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [300/391]	Time 0.068 (0.062)	Data 0.003 (0.004)	Loss 0.940 (1.004)	Prec@1 69.531 (63.969)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [350/391]	Time 0.069 (0.063)	Data 0.003 (0.003)	Loss 0.999 (0.997)	Prec@1 64.844 (64.194)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Test: [0/79]	Time: 0.3810(0.3810)	Loss: 1.208(1.208)	Prec@1: 61.719(61.719)	
06-15-23 18:27:Test: [50/79]	Time: 0.0166(0.0238)	Loss: 1.514(1.282)	Prec@1: 46.875(57.812)	
06-15-23 18:27:Test: [78/79]	Time: 0.0151(0.0212)	Loss: 1.503(1.290)	Prec@1: 56.250(57.260)	
06-15-23 18:27:Step 3 * Prec@1 57.260
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [0/391]	Time 0.414 (0.414)	Data 0.364 (0.364)	Loss 1.008 (1.008)	Prec@1 64.062 (64.062)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [50/391]	Time 0.044 (0.067)	Data 0.002 (0.010)	Loss 0.897 (0.978)	Prec@1 67.969 (65.028)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [100/391]	Time 0.063 (0.063)	Data 0.003 (0.006)	Loss 0.818 (0.946)	Prec@1 67.188 (66.429)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [150/391]	Time 0.062 (0.062)	Data 0.003 (0.005)	Loss 0.877 (0.932)	Prec@1 70.312 (66.805)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [200/391]	Time 0.066 (0.063)	Data 0.002 (0.004)	Loss 0.847 (0.931)	Prec@1 72.656 (66.834)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [250/391]	Time 0.067 (0.063)	Data 0.003 (0.004)	Loss 0.842 (0.924)	Prec@1 65.625 (66.998)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [300/391]	Time 0.066 (0.063)	Data 0.003 (0.004)	Loss 0.746 (0.920)	Prec@1 77.344 (67.234)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [350/391]	Time 0.064 (0.064)	Data 0.002 (0.004)	Loss 0.938 (0.915)	Prec@1 64.844 (67.354)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Test: [0/79]	Time: 0.3734(0.3734)	Loss: 0.849(0.849)	Prec@1: 66.406(66.406)	
06-15-23 18:27:Test: [50/79]	Time: 0.0165(0.0233)	Loss: 0.999(0.914)	Prec@1: 64.062(67.172)	
06-15-23 18:27:Test: [78/79]	Time: 0.0150(0.0209)	Loss: 0.738(0.910)	Prec@1: 62.500(67.130)	
06-15-23 18:27:Step 4 * Prec@1 67.130
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [0/391]	Time 0.393 (0.393)	Data 0.352 (0.352)	Loss 0.719 (0.719)	Prec@1 74.219 (74.219)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [50/391]	Time 0.068 (0.063)	Data 0.002 (0.009)	Loss 0.974 (0.866)	Prec@1 61.719 (69.393)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [100/391]	Time 0.064 (0.065)	Data 0.002 (0.006)	Loss 0.771 (0.862)	Prec@1 67.969 (69.454)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [150/391]	Time 0.073 (0.067)	Data 0.003 (0.005)	Loss 1.023 (0.871)	Prec@1 64.062 (68.983)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [200/391]	Time 0.070 (0.068)	Data 0.003 (0.004)	Loss 0.915 (0.862)	Prec@1 67.188 (69.317)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [250/391]	Time 0.042 (0.065)	Data 0.002 (0.004)	Loss 0.914 (0.859)	Prec@1 61.719 (69.463)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:27:Num bit 8	Num grad bit 8	
06-15-23 18:27:Iter: [300/391]	Time 0.050 (0.062)	Data 0.002 (0.004)	Loss 0.871 (0.857)	Prec@1 67.969 (69.534)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [350/391]	Time 0.066 (0.060)	Data 0.002 (0.003)	Loss 0.797 (0.852)	Prec@1 71.875 (69.625)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Test: [0/79]	Time: 0.3883(0.3883)	Loss: 0.835(0.835)	Prec@1: 67.969(67.969)	
06-15-23 18:28:Test: [50/79]	Time: 0.0166(0.0244)	Loss: 0.979(0.929)	Prec@1: 65.625(67.923)	
06-15-23 18:28:Test: [78/79]	Time: 0.0150(0.0216)	Loss: 0.806(0.929)	Prec@1: 68.750(67.800)	
06-15-23 18:28:Step 5 * Prec@1 67.800
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [0/391]	Time 0.395 (0.395)	Data 0.358 (0.358)	Loss 0.723 (0.723)	Prec@1 73.438 (73.438)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [50/391]	Time 0.071 (0.066)	Data 0.003 (0.009)	Loss 0.841 (0.794)	Prec@1 66.406 (72.289)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [100/391]	Time 0.068 (0.068)	Data 0.002 (0.006)	Loss 0.616 (0.784)	Prec@1 80.469 (72.138)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [150/391]	Time 0.074 (0.069)	Data 0.003 (0.005)	Loss 0.823 (0.798)	Prec@1 67.969 (71.627)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [200/391]	Time 0.059 (0.069)	Data 0.002 (0.004)	Loss 0.667 (0.795)	Prec@1 73.438 (71.712)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [250/391]	Time 0.074 (0.069)	Data 0.003 (0.004)	Loss 0.649 (0.793)	Prec@1 76.562 (71.816)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [300/391]	Time 0.069 (0.070)	Data 0.002 (0.004)	Loss 0.841 (0.795)	Prec@1 71.094 (71.706)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [350/391]	Time 0.072 (0.070)	Data 0.003 (0.004)	Loss 0.705 (0.795)	Prec@1 72.656 (71.666)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Test: [0/79]	Time: 0.3858(0.3858)	Loss: 0.792(0.792)	Prec@1: 73.438(73.438)	
06-15-23 18:28:Test: [50/79]	Time: 0.0163(0.0245)	Loss: 0.863(0.846)	Prec@1: 74.219(70.956)	
06-15-23 18:28:Test: [78/79]	Time: 0.0153(0.0217)	Loss: 0.820(0.844)	Prec@1: 62.500(70.870)	
06-15-23 18:28:Step 6 * Prec@1 70.870
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [0/391]	Time 0.441 (0.441)	Data 0.372 (0.372)	Loss 0.704 (0.704)	Prec@1 75.000 (75.000)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [50/391]	Time 0.047 (0.057)	Data 0.002 (0.009)	Loss 0.678 (0.758)	Prec@1 74.219 (72.503)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [100/391]	Time 0.073 (0.056)	Data 0.003 (0.006)	Loss 0.738 (0.758)	Prec@1 78.906 (72.834)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [150/391]	Time 0.048 (0.061)	Data 0.002 (0.005)	Loss 0.883 (0.755)	Prec@1 68.750 (73.096)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [200/391]	Time 0.073 (0.062)	Data 0.003 (0.004)	Loss 0.945 (0.756)	Prec@1 66.406 (72.994)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [250/391]	Time 0.064 (0.064)	Data 0.002 (0.004)	Loss 0.642 (0.757)	Prec@1 73.438 (73.048)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [300/391]	Time 0.043 (0.063)	Data 0.002 (0.004)	Loss 0.716 (0.759)	Prec@1 75.781 (73.012)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:28:Num bit 8	Num grad bit 8	
06-15-23 18:28:Iter: [350/391]	Time 0.072 (0.063)	Data 0.003 (0.003)	Loss 0.549 (0.758)	Prec@1 78.125 (73.093)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Test: [0/79]	Time: 0.3815(0.3815)	Loss: 0.620(0.620)	Prec@1: 73.438(73.438)	
06-15-23 18:29:Test: [50/79]	Time: 0.0166(0.0237)	Loss: 0.707(0.784)	Prec@1: 76.562(72.855)	
06-15-23 18:29:Test: [78/79]	Time: 0.0150(0.0212)	Loss: 0.703(0.791)	Prec@1: 81.250(72.580)	
06-15-23 18:29:Step 7 * Prec@1 72.580
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [0/391]	Time 0.408 (0.408)	Data 0.371 (0.371)	Loss 0.742 (0.742)	Prec@1 75.000 (75.000)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [50/391]	Time 0.074 (0.068)	Data 0.003 (0.010)	Loss 0.780 (0.725)	Prec@1 77.344 (74.449)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [100/391]	Time 0.073 (0.070)	Data 0.003 (0.006)	Loss 0.761 (0.716)	Prec@1 75.000 (74.853)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [150/391]	Time 0.073 (0.071)	Data 0.003 (0.005)	Loss 0.580 (0.714)	Prec@1 80.469 (74.850)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [200/391]	Time 0.076 (0.071)	Data 0.004 (0.005)	Loss 0.767 (0.722)	Prec@1 75.000 (74.623)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [250/391]	Time 0.056 (0.070)	Data 0.002 (0.004)	Loss 0.732 (0.722)	Prec@1 76.562 (74.667)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [300/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.662 (0.725)	Prec@1 78.125 (74.585)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [350/391]	Time 0.066 (0.070)	Data 0.003 (0.004)	Loss 0.649 (0.725)	Prec@1 72.656 (74.517)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Test: [0/79]	Time: 0.3621(0.3621)	Loss: 0.687(0.687)	Prec@1: 75.781(75.781)	
06-15-23 18:29:Test: [50/79]	Time: 0.0167(0.0237)	Loss: 0.890(0.874)	Prec@1: 72.656(70.021)	
06-15-23 18:29:Test: [78/79]	Time: 0.0211(0.0219)	Loss: 0.583(0.878)	Prec@1: 75.000(70.230)	
06-15-23 18:29:Step 8 * Prec@1 70.230
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [0/391]	Time 0.435 (0.435)	Data 0.397 (0.397)	Loss 0.620 (0.620)	Prec@1 82.812 (82.812)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [50/391]	Time 0.065 (0.065)	Data 0.002 (0.010)	Loss 0.685 (0.735)	Prec@1 74.219 (74.234)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [100/391]	Time 0.073 (0.068)	Data 0.003 (0.006)	Loss 0.814 (0.732)	Prec@1 67.969 (74.234)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [150/391]	Time 0.073 (0.069)	Data 0.002 (0.005)	Loss 0.683 (0.731)	Prec@1 75.781 (73.996)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [200/391]	Time 0.061 (0.069)	Data 0.002 (0.005)	Loss 0.709 (0.731)	Prec@1 79.688 (74.254)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [250/391]	Time 0.072 (0.069)	Data 0.002 (0.004)	Loss 0.737 (0.729)	Prec@1 71.875 (74.362)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [300/391]	Time 0.069 (0.070)	Data 0.003 (0.004)	Loss 0.729 (0.732)	Prec@1 76.562 (74.315)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Num bit 8	Num grad bit 8	
06-15-23 18:29:Iter: [350/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.682 (0.733)	Prec@1 78.906 (74.339)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:29:Test: [0/79]	Time: 0.3663(0.3663)	Loss: 0.654(0.654)	Prec@1: 76.562(76.562)	
06-15-23 18:30:Test: [50/79]	Time: 0.0165(0.0234)	Loss: 0.840(0.838)	Prec@1: 73.438(72.212)	
06-15-23 18:30:Test: [78/79]	Time: 0.0151(0.0210)	Loss: 0.552(0.843)	Prec@1: 68.750(71.800)	
06-15-23 18:30:Step 9 * Prec@1 71.800
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [0/391]	Time 0.394 (0.394)	Data 0.352 (0.352)	Loss 0.641 (0.641)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [50/391]	Time 0.067 (0.068)	Data 0.003 (0.009)	Loss 0.608 (0.719)	Prec@1 78.906 (73.974)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [100/391]	Time 0.070 (0.069)	Data 0.003 (0.006)	Loss 0.650 (0.726)	Prec@1 81.250 (73.871)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [150/391]	Time 0.076 (0.070)	Data 0.003 (0.005)	Loss 0.741 (0.725)	Prec@1 71.875 (74.120)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [200/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.828 (0.726)	Prec@1 71.875 (74.262)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [250/391]	Time 0.074 (0.071)	Data 0.003 (0.004)	Loss 0.931 (0.725)	Prec@1 66.406 (74.365)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [300/391]	Time 0.066 (0.070)	Data 0.003 (0.004)	Loss 0.844 (0.727)	Prec@1 67.188 (74.234)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [350/391]	Time 0.069 (0.071)	Data 0.002 (0.004)	Loss 0.519 (0.729)	Prec@1 86.719 (74.254)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Test: [0/79]	Time: 0.3818(0.3818)	Loss: 0.681(0.681)	Prec@1: 75.781(75.781)	
06-15-23 18:30:Test: [50/79]	Time: 0.0278(0.0239)	Loss: 0.769(0.839)	Prec@1: 74.219(71.415)	
06-15-23 18:30:Test: [78/79]	Time: 0.0150(0.0218)	Loss: 0.594(0.835)	Prec@1: 75.000(71.660)	
06-15-23 18:30:Step 10 * Prec@1 71.660
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [0/391]	Time 0.388 (0.388)	Data 0.352 (0.352)	Loss 0.894 (0.894)	Prec@1 67.969 (67.969)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [50/391]	Time 0.075 (0.071)	Data 0.003 (0.009)	Loss 0.741 (0.737)	Prec@1 75.000 (73.943)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [100/391]	Time 0.073 (0.071)	Data 0.002 (0.006)	Loss 0.752 (0.735)	Prec@1 72.656 (74.126)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [150/391]	Time 0.074 (0.072)	Data 0.003 (0.005)	Loss 0.651 (0.725)	Prec@1 78.125 (74.762)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [200/391]	Time 0.070 (0.072)	Data 0.003 (0.004)	Loss 0.774 (0.724)	Prec@1 72.656 (74.670)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [250/391]	Time 0.051 (0.071)	Data 0.002 (0.004)	Loss 0.620 (0.725)	Prec@1 75.781 (74.536)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [300/391]	Time 0.070 (0.070)	Data 0.003 (0.004)	Loss 0.704 (0.728)	Prec@1 77.344 (74.426)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Num bit 8	Num grad bit 8	
06-15-23 18:30:Iter: [350/391]	Time 0.073 (0.071)	Data 0.003 (0.004)	Loss 0.792 (0.726)	Prec@1 69.531 (74.513)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:30:Test: [0/79]	Time: 0.3868(0.3868)	Loss: 0.717(0.717)	Prec@1: 72.656(72.656)	
06-15-23 18:30:Test: [50/79]	Time: 0.0165(0.0238)	Loss: 0.842(0.847)	Prec@1: 73.438(72.151)	
06-15-23 18:30:Test: [78/79]	Time: 0.0161(0.0213)	Loss: 0.654(0.854)	Prec@1: 68.750(72.110)	
06-15-23 18:30:Step 11 * Prec@1 72.110
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [0/391]	Time 0.385 (0.385)	Data 0.347 (0.347)	Loss 0.647 (0.647)	Prec@1 82.031 (82.031)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [50/391]	Time 0.073 (0.056)	Data 0.003 (0.009)	Loss 0.620 (0.707)	Prec@1 76.562 (75.980)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [100/391]	Time 0.076 (0.064)	Data 0.003 (0.006)	Loss 0.693 (0.716)	Prec@1 77.344 (75.317)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [150/391]	Time 0.076 (0.066)	Data 0.003 (0.005)	Loss 0.745 (0.717)	Prec@1 68.750 (75.129)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [200/391]	Time 0.067 (0.067)	Data 0.003 (0.004)	Loss 0.696 (0.717)	Prec@1 75.781 (74.996)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [250/391]	Time 0.059 (0.067)	Data 0.002 (0.004)	Loss 0.902 (0.716)	Prec@1 68.750 (75.059)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [300/391]	Time 0.063 (0.068)	Data 0.003 (0.004)	Loss 0.896 (0.714)	Prec@1 73.438 (75.158)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [350/391]	Time 0.063 (0.068)	Data 0.002 (0.004)	Loss 0.839 (0.715)	Prec@1 71.094 (75.122)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Test: [0/79]	Time: 0.3616(0.3616)	Loss: 0.644(0.644)	Prec@1: 76.562(76.562)	
06-15-23 18:31:Test: [50/79]	Time: 0.0166(0.0233)	Loss: 0.743(0.782)	Prec@1: 77.344(74.050)	
06-15-23 18:31:Test: [78/79]	Time: 0.0150(0.0209)	Loss: 0.656(0.785)	Prec@1: 75.000(73.890)	
06-15-23 18:31:Step 12 * Prec@1 73.890
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [0/391]	Time 0.386 (0.386)	Data 0.349 (0.349)	Loss 0.964 (0.964)	Prec@1 68.750 (68.750)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [50/391]	Time 0.071 (0.053)	Data 0.003 (0.009)	Loss 0.754 (0.712)	Prec@1 70.312 (74.740)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [100/391]	Time 0.070 (0.061)	Data 0.003 (0.006)	Loss 0.745 (0.705)	Prec@1 73.438 (75.302)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [150/391]	Time 0.074 (0.065)	Data 0.003 (0.005)	Loss 0.628 (0.697)	Prec@1 76.562 (75.642)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [200/391]	Time 0.064 (0.066)	Data 0.002 (0.004)	Loss 0.758 (0.700)	Prec@1 77.344 (75.579)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [250/391]	Time 0.069 (0.068)	Data 0.002 (0.004)	Loss 0.563 (0.709)	Prec@1 77.344 (75.121)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [300/391]	Time 0.076 (0.068)	Data 0.003 (0.004)	Loss 0.821 (0.709)	Prec@1 71.094 (75.202)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [350/391]	Time 0.074 (0.069)	Data 0.003 (0.004)	Loss 0.631 (0.707)	Prec@1 79.688 (75.376)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:31:Test: [0/79]	Time: 0.3733(0.3733)	Loss: 0.851(0.851)	Prec@1: 74.219(74.219)	
06-15-23 18:31:Test: [50/79]	Time: 0.0164(0.0235)	Loss: 1.066(0.969)	Prec@1: 67.969(70.006)	
06-15-23 18:31:Test: [78/79]	Time: 0.0150(0.0210)	Loss: 0.933(0.985)	Prec@1: 62.500(69.690)	
06-15-23 18:31:Step 13 * Prec@1 69.690
06-15-23 18:31:Num bit 8	Num grad bit 8	
06-15-23 18:31:Iter: [0/391]	Time 0.444 (0.444)	Data 0.384 (0.384)	Loss 0.552 (0.552)	Prec@1 77.344 (77.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [50/391]	Time 0.074 (0.077)	Data 0.002 (0.010)	Loss 0.704 (0.684)	Prec@1 75.781 (76.042)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [100/391]	Time 0.073 (0.074)	Data 0.002 (0.006)	Loss 0.556 (0.707)	Prec@1 81.250 (74.977)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [150/391]	Time 0.058 (0.072)	Data 0.002 (0.005)	Loss 0.728 (0.710)	Prec@1 75.000 (74.917)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [200/391]	Time 0.065 (0.069)	Data 0.002 (0.004)	Loss 0.778 (0.704)	Prec@1 73.438 (75.237)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [250/391]	Time 0.060 (0.067)	Data 0.002 (0.004)	Loss 0.635 (0.702)	Prec@1 77.344 (75.535)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [300/391]	Time 0.064 (0.066)	Data 0.002 (0.003)	Loss 0.655 (0.700)	Prec@1 77.344 (75.574)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [350/391]	Time 0.060 (0.065)	Data 0.002 (0.003)	Loss 0.690 (0.696)	Prec@1 71.875 (75.694)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Test: [0/79]	Time: 0.3519(0.3519)	Loss: 1.295(1.295)	Prec@1: 62.500(62.500)	
06-15-23 18:32:Test: [50/79]	Time: 0.0164(0.0240)	Loss: 1.474(1.458)	Prec@1: 59.375(61.811)	
06-15-23 18:32:Test: [78/79]	Time: 0.0148(0.0213)	Loss: 1.047(1.431)	Prec@1: 62.500(62.240)	
06-15-23 18:32:Step 14 * Prec@1 62.240
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [0/391]	Time 0.425 (0.425)	Data 0.369 (0.369)	Loss 0.455 (0.455)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [50/391]	Time 0.059 (0.074)	Data 0.002 (0.010)	Loss 0.531 (0.670)	Prec@1 81.250 (76.409)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [100/391]	Time 0.071 (0.071)	Data 0.003 (0.006)	Loss 0.451 (0.677)	Prec@1 84.375 (76.462)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [150/391]	Time 0.073 (0.071)	Data 0.003 (0.005)	Loss 0.693 (0.679)	Prec@1 78.125 (76.521)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [200/391]	Time 0.069 (0.071)	Data 0.002 (0.005)	Loss 0.738 (0.682)	Prec@1 76.562 (76.508)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [250/391]	Time 0.073 (0.071)	Data 0.003 (0.004)	Loss 0.600 (0.686)	Prec@1 77.344 (76.285)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [300/391]	Time 0.074 (0.071)	Data 0.002 (0.004)	Loss 0.745 (0.683)	Prec@1 71.875 (76.391)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [350/391]	Time 0.063 (0.071)	Data 0.002 (0.004)	Loss 0.775 (0.680)	Prec@1 75.781 (76.440)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Test: [0/79]	Time: 0.3492(0.3492)	Loss: 0.654(0.654)	Prec@1: 77.344(77.344)	
06-15-23 18:32:Test: [50/79]	Time: 0.0170(0.0234)	Loss: 0.796(0.822)	Prec@1: 70.312(72.871)	
06-15-23 18:32:Test: [78/79]	Time: 0.0154(0.0211)	Loss: 0.618(0.821)	Prec@1: 68.750(72.960)	
06-15-23 18:32:Step 15 * Prec@1 72.960
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [0/391]	Time 0.452 (0.452)	Data 0.395 (0.395)	Loss 0.781 (0.781)	Prec@1 74.219 (74.219)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:32:Num bit 8	Num grad bit 8	
06-15-23 18:32:Iter: [50/391]	Time 0.069 (0.076)	Data 0.003 (0.010)	Loss 0.754 (0.693)	Prec@1 77.344 (75.506)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [100/391]	Time 0.066 (0.069)	Data 0.003 (0.006)	Loss 0.790 (0.694)	Prec@1 70.312 (75.766)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [150/391]	Time 0.070 (0.069)	Data 0.003 (0.005)	Loss 0.802 (0.688)	Prec@1 68.750 (75.926)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [200/391]	Time 0.067 (0.068)	Data 0.002 (0.005)	Loss 0.650 (0.688)	Prec@1 79.688 (76.018)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [250/391]	Time 0.066 (0.067)	Data 0.002 (0.004)	Loss 0.983 (0.691)	Prec@1 67.188 (76.030)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [300/391]	Time 0.052 (0.066)	Data 0.002 (0.004)	Loss 0.519 (0.691)	Prec@1 82.812 (75.934)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [350/391]	Time 0.069 (0.066)	Data 0.003 (0.004)	Loss 0.790 (0.687)	Prec@1 72.656 (76.042)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Test: [0/79]	Time: 0.3528(0.3528)	Loss: 0.929(0.929)	Prec@1: 67.188(67.188)	
06-15-23 18:33:Test: [50/79]	Time: 0.0167(0.0232)	Loss: 1.050(1.105)	Prec@1: 65.625(67.387)	
06-15-23 18:33:Test: [78/79]	Time: 0.0151(0.0209)	Loss: 0.756(1.096)	Prec@1: 68.750(67.500)	
06-15-23 18:33:Step 16 * Prec@1 67.500
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [0/391]	Time 0.399 (0.399)	Data 0.363 (0.363)	Loss 0.607 (0.607)	Prec@1 77.344 (77.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [50/391]	Time 0.073 (0.075)	Data 0.003 (0.010)	Loss 0.670 (0.679)	Prec@1 77.344 (76.440)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [100/391]	Time 0.069 (0.074)	Data 0.002 (0.006)	Loss 0.685 (0.681)	Prec@1 78.906 (76.439)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [150/391]	Time 0.077 (0.073)	Data 0.003 (0.005)	Loss 0.622 (0.676)	Prec@1 78.125 (76.702)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [200/391]	Time 0.076 (0.073)	Data 0.003 (0.004)	Loss 0.650 (0.683)	Prec@1 76.562 (76.489)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [250/391]	Time 0.074 (0.072)	Data 0.003 (0.004)	Loss 0.554 (0.682)	Prec@1 78.125 (76.485)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [300/391]	Time 0.073 (0.072)	Data 0.003 (0.004)	Loss 0.619 (0.682)	Prec@1 78.906 (76.568)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [350/391]	Time 0.055 (0.072)	Data 0.002 (0.004)	Loss 0.970 (0.681)	Prec@1 72.656 (76.587)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Test: [0/79]	Time: 0.3961(0.3961)	Loss: 0.985(0.985)	Prec@1: 68.750(68.750)	
06-15-23 18:33:Test: [50/79]	Time: 0.0166(0.0240)	Loss: 1.028(0.991)	Prec@1: 69.531(69.133)	
06-15-23 18:33:Test: [78/79]	Time: 0.0152(0.0214)	Loss: 0.549(0.987)	Prec@1: 68.750(69.140)	
06-15-23 18:33:Step 17 * Prec@1 69.140
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [0/391]	Time 0.395 (0.395)	Data 0.354 (0.354)	Loss 0.624 (0.624)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [50/391]	Time 0.070 (0.077)	Data 0.002 (0.009)	Loss 0.608 (0.658)	Prec@1 79.688 (77.282)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:33:Num bit 8	Num grad bit 8	
06-15-23 18:33:Iter: [100/391]	Time 0.076 (0.075)	Data 0.002 (0.006)	Loss 0.566 (0.668)	Prec@1 80.469 (76.980)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [150/391]	Time 0.071 (0.074)	Data 0.002 (0.005)	Loss 0.544 (0.675)	Prec@1 81.250 (76.728)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [200/391]	Time 0.070 (0.074)	Data 0.002 (0.004)	Loss 0.648 (0.674)	Prec@1 77.344 (76.695)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [250/391]	Time 0.072 (0.074)	Data 0.003 (0.004)	Loss 0.515 (0.668)	Prec@1 78.906 (76.924)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [300/391]	Time 0.073 (0.073)	Data 0.002 (0.004)	Loss 0.599 (0.665)	Prec@1 81.250 (77.019)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [350/391]	Time 0.075 (0.073)	Data 0.002 (0.003)	Loss 0.674 (0.663)	Prec@1 75.781 (77.070)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Test: [0/79]	Time: 0.3801(0.3801)	Loss: 0.685(0.685)	Prec@1: 77.344(77.344)	
06-15-23 18:34:Test: [50/79]	Time: 0.0163(0.0235)	Loss: 0.887(0.919)	Prec@1: 71.875(71.752)	
06-15-23 18:34:Test: [78/79]	Time: 0.0150(0.0210)	Loss: 0.565(0.905)	Prec@1: 81.250(71.990)	
06-15-23 18:34:Step 18 * Prec@1 71.990
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [0/391]	Time 0.412 (0.412)	Data 0.376 (0.376)	Loss 0.613 (0.613)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [50/391]	Time 0.071 (0.064)	Data 0.003 (0.009)	Loss 0.683 (0.656)	Prec@1 75.781 (77.221)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [100/391]	Time 0.071 (0.067)	Data 0.003 (0.006)	Loss 0.662 (0.659)	Prec@1 73.438 (77.050)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [150/391]	Time 0.069 (0.068)	Data 0.003 (0.005)	Loss 0.732 (0.659)	Prec@1 75.000 (76.992)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [200/391]	Time 0.061 (0.069)	Data 0.002 (0.004)	Loss 0.541 (0.666)	Prec@1 81.250 (76.897)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [250/391]	Time 0.072 (0.070)	Data 0.003 (0.004)	Loss 0.542 (0.672)	Prec@1 80.469 (76.724)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [300/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.683 (0.673)	Prec@1 75.781 (76.664)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [350/391]	Time 0.065 (0.070)	Data 0.002 (0.004)	Loss 0.656 (0.669)	Prec@1 82.031 (76.883)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Test: [0/79]	Time: 0.3665(0.3665)	Loss: 0.752(0.752)	Prec@1: 74.219(74.219)	
06-15-23 18:34:Test: [50/79]	Time: 0.0163(0.0234)	Loss: 0.954(0.905)	Prec@1: 68.750(70.358)	
06-15-23 18:34:Test: [78/79]	Time: 0.0152(0.0210)	Loss: 0.700(0.901)	Prec@1: 68.750(70.330)	
06-15-23 18:34:Step 19 * Prec@1 70.330
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [0/391]	Time 0.400 (0.400)	Data 0.363 (0.363)	Loss 0.636 (0.636)	Prec@1 78.906 (78.906)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [50/391]	Time 0.069 (0.071)	Data 0.002 (0.010)	Loss 0.690 (0.669)	Prec@1 74.219 (76.562)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:34:Num bit 8	Num grad bit 8	
06-15-23 18:34:Iter: [100/391]	Time 0.075 (0.072)	Data 0.003 (0.006)	Loss 0.605 (0.656)	Prec@1 82.031 (77.027)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [150/391]	Time 0.072 (0.072)	Data 0.002 (0.005)	Loss 0.608 (0.650)	Prec@1 78.906 (77.209)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [200/391]	Time 0.073 (0.071)	Data 0.003 (0.004)	Loss 0.602 (0.655)	Prec@1 83.594 (77.134)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [250/391]	Time 0.074 (0.071)	Data 0.003 (0.004)	Loss 0.670 (0.657)	Prec@1 75.781 (77.126)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [300/391]	Time 0.069 (0.072)	Data 0.003 (0.004)	Loss 0.494 (0.657)	Prec@1 85.156 (77.089)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [350/391]	Time 0.069 (0.071)	Data 0.002 (0.004)	Loss 0.775 (0.657)	Prec@1 75.781 (77.066)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Test: [0/79]	Time: 0.3944(0.3944)	Loss: 0.801(0.801)	Prec@1: 71.094(71.094)	
06-15-23 18:35:Test: [50/79]	Time: 0.0167(0.0238)	Loss: 0.892(0.973)	Prec@1: 67.969(69.317)	
06-15-23 18:35:Test: [78/79]	Time: 0.0151(0.0211)	Loss: 1.011(0.984)	Prec@1: 62.500(68.980)	
06-15-23 18:35:Step 20 * Prec@1 68.980
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [0/391]	Time 0.409 (0.409)	Data 0.371 (0.371)	Loss 0.727 (0.727)	Prec@1 71.094 (71.094)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [50/391]	Time 0.074 (0.069)	Data 0.003 (0.010)	Loss 0.493 (0.635)	Prec@1 83.594 (78.431)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [100/391]	Time 0.072 (0.070)	Data 0.003 (0.006)	Loss 0.615 (0.636)	Prec@1 75.000 (78.226)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [150/391]	Time 0.073 (0.070)	Data 0.003 (0.005)	Loss 0.731 (0.636)	Prec@1 77.344 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [200/391]	Time 0.077 (0.070)	Data 0.003 (0.004)	Loss 0.767 (0.636)	Prec@1 75.000 (78.063)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [250/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.549 (0.637)	Prec@1 82.812 (78.081)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [300/391]	Time 0.072 (0.070)	Data 0.003 (0.004)	Loss 0.673 (0.639)	Prec@1 76.562 (77.987)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [350/391]	Time 0.081 (0.070)	Data 0.003 (0.004)	Loss 0.672 (0.641)	Prec@1 78.125 (77.909)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Test: [0/79]	Time: 0.3851(0.3851)	Loss: 0.729(0.729)	Prec@1: 74.219(74.219)	
06-15-23 18:35:Test: [50/79]	Time: 0.0167(0.0238)	Loss: 0.938(0.809)	Prec@1: 70.312(72.610)	
06-15-23 18:35:Test: [78/79]	Time: 0.0151(0.0213)	Loss: 0.847(0.822)	Prec@1: 62.500(71.990)	
06-15-23 18:35:Step 21 * Prec@1 71.990
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [0/391]	Time 0.422 (0.422)	Data 0.354 (0.354)	Loss 0.722 (0.722)	Prec@1 69.531 (69.531)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [50/391]	Time 0.047 (0.058)	Data 0.002 (0.009)	Loss 0.649 (0.653)	Prec@1 77.344 (77.175)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:35:Num bit 8	Num grad bit 8	
06-15-23 18:35:Iter: [100/391]	Time 0.066 (0.062)	Data 0.003 (0.006)	Loss 0.591 (0.639)	Prec@1 83.594 (77.862)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [150/391]	Time 0.061 (0.065)	Data 0.002 (0.005)	Loss 0.754 (0.641)	Prec@1 75.000 (77.908)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [200/391]	Time 0.069 (0.066)	Data 0.002 (0.004)	Loss 0.618 (0.638)	Prec@1 78.906 (78.063)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [250/391]	Time 0.072 (0.067)	Data 0.003 (0.004)	Loss 0.781 (0.643)	Prec@1 76.562 (77.845)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [300/391]	Time 0.072 (0.068)	Data 0.003 (0.004)	Loss 0.698 (0.643)	Prec@1 77.344 (77.912)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [350/391]	Time 0.072 (0.068)	Data 0.003 (0.004)	Loss 0.898 (0.644)	Prec@1 71.094 (77.882)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Test: [0/79]	Time: 0.3972(0.3972)	Loss: 0.700(0.700)	Prec@1: 76.562(76.562)	
06-15-23 18:36:Test: [50/79]	Time: 0.0165(0.0240)	Loss: 0.707(0.807)	Prec@1: 73.438(73.713)	
06-15-23 18:36:Test: [78/79]	Time: 0.0151(0.0214)	Loss: 0.800(0.805)	Prec@1: 75.000(73.740)	
06-15-23 18:36:Step 22 * Prec@1 73.740
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [0/391]	Time 0.405 (0.405)	Data 0.350 (0.350)	Loss 0.650 (0.650)	Prec@1 78.906 (78.906)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [50/391]	Time 0.071 (0.073)	Data 0.002 (0.009)	Loss 0.708 (0.646)	Prec@1 75.781 (78.002)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [100/391]	Time 0.072 (0.072)	Data 0.003 (0.006)	Loss 0.463 (0.648)	Prec@1 82.812 (77.669)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [150/391]	Time 0.074 (0.071)	Data 0.003 (0.005)	Loss 0.553 (0.645)	Prec@1 79.688 (77.846)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [200/391]	Time 0.075 (0.071)	Data 0.003 (0.004)	Loss 0.854 (0.636)	Prec@1 74.219 (78.211)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [250/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.563 (0.631)	Prec@1 81.250 (78.374)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [300/391]	Time 0.071 (0.071)	Data 0.002 (0.004)	Loss 0.735 (0.634)	Prec@1 75.781 (78.281)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [350/391]	Time 0.067 (0.070)	Data 0.002 (0.004)	Loss 0.504 (0.636)	Prec@1 81.250 (78.192)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Test: [0/79]	Time: 0.3619(0.3619)	Loss: 1.068(1.068)	Prec@1: 67.188(67.188)	
06-15-23 18:36:Test: [50/79]	Time: 0.0198(0.0264)	Loss: 1.358(1.255)	Prec@1: 58.594(65.349)	
06-15-23 18:36:Test: [78/79]	Time: 0.0183(0.0241)	Loss: 1.226(1.252)	Prec@1: 62.500(65.690)	
06-15-23 18:36:Step 23 * Prec@1 65.690
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [0/391]	Time 0.404 (0.404)	Data 0.367 (0.367)	Loss 0.511 (0.511)	Prec@1 81.250 (81.250)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [50/391]	Time 0.074 (0.067)	Data 0.003 (0.010)	Loss 0.658 (0.624)	Prec@1 77.344 (78.508)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [100/391]	Time 0.069 (0.069)	Data 0.003 (0.006)	Loss 0.631 (0.626)	Prec@1 82.031 (78.945)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:36:Num bit 8	Num grad bit 8	
06-15-23 18:36:Iter: [150/391]	Time 0.072 (0.069)	Data 0.003 (0.005)	Loss 0.651 (0.625)	Prec@1 75.000 (79.010)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [200/391]	Time 0.072 (0.070)	Data 0.002 (0.004)	Loss 0.790 (0.632)	Prec@1 75.781 (78.661)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [250/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.639 (0.631)	Prec@1 75.000 (78.576)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [300/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.558 (0.631)	Prec@1 80.469 (78.455)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [350/391]	Time 0.078 (0.071)	Data 0.003 (0.004)	Loss 0.730 (0.630)	Prec@1 79.688 (78.477)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Test: [0/79]	Time: 0.3697(0.3697)	Loss: 0.914(0.914)	Prec@1: 67.188(67.188)	
06-15-23 18:37:Test: [50/79]	Time: 0.0164(0.0234)	Loss: 1.234(1.088)	Prec@1: 62.500(66.912)	
06-15-23 18:37:Test: [78/79]	Time: 0.0150(0.0209)	Loss: 0.820(1.078)	Prec@1: 62.500(67.000)	
06-15-23 18:37:Step 24 * Prec@1 67.000
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [0/391]	Time 0.413 (0.413)	Data 0.376 (0.376)	Loss 0.659 (0.659)	Prec@1 75.781 (75.781)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [50/391]	Time 0.071 (0.064)	Data 0.003 (0.009)	Loss 0.492 (0.632)	Prec@1 80.469 (78.064)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [100/391]	Time 0.069 (0.067)	Data 0.003 (0.006)	Loss 0.703 (0.638)	Prec@1 75.781 (78.040)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [150/391]	Time 0.066 (0.067)	Data 0.003 (0.005)	Loss 0.586 (0.629)	Prec@1 80.469 (78.435)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [200/391]	Time 0.057 (0.067)	Data 0.002 (0.004)	Loss 0.633 (0.627)	Prec@1 78.125 (78.397)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [250/391]	Time 0.034 (0.065)	Data 0.002 (0.004)	Loss 0.445 (0.626)	Prec@1 83.594 (78.355)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [300/391]	Time 0.049 (0.062)	Data 0.002 (0.004)	Loss 0.566 (0.623)	Prec@1 79.688 (78.442)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [350/391]	Time 0.043 (0.059)	Data 0.002 (0.003)	Loss 0.488 (0.623)	Prec@1 83.594 (78.425)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Test: [0/79]	Time: 0.3867(0.3867)	Loss: 0.716(0.716)	Prec@1: 74.219(74.219)	
06-15-23 18:37:Test: [50/79]	Time: 0.0168(0.0239)	Loss: 1.002(0.903)	Prec@1: 64.062(69.608)	
06-15-23 18:37:Test: [78/79]	Time: 0.0151(0.0213)	Loss: 0.784(0.898)	Prec@1: 68.750(69.610)	
06-15-23 18:37:Step 25 * Prec@1 69.610
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [0/391]	Time 0.384 (0.384)	Data 0.348 (0.348)	Loss 0.679 (0.679)	Prec@1 77.344 (77.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [50/391]	Time 0.069 (0.061)	Data 0.003 (0.009)	Loss 0.524 (0.616)	Prec@1 79.688 (79.136)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [100/391]	Time 0.070 (0.064)	Data 0.003 (0.006)	Loss 0.440 (0.607)	Prec@1 85.938 (79.208)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [150/391]	Time 0.066 (0.063)	Data 0.003 (0.005)	Loss 0.451 (0.603)	Prec@1 84.375 (79.387)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [200/391]	Time 0.071 (0.065)	Data 0.003 (0.004)	Loss 0.761 (0.613)	Prec@1 76.562 (78.965)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:37:Num bit 8	Num grad bit 8	
06-15-23 18:37:Iter: [250/391]	Time 0.073 (0.066)	Data 0.003 (0.004)	Loss 0.610 (0.610)	Prec@1 82.812 (79.109)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [300/391]	Time 0.073 (0.067)	Data 0.003 (0.004)	Loss 0.703 (0.613)	Prec@1 78.125 (79.005)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [350/391]	Time 0.071 (0.068)	Data 0.003 (0.004)	Loss 0.591 (0.616)	Prec@1 82.812 (78.913)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Test: [0/79]	Time: 0.3665(0.3665)	Loss: 0.722(0.722)	Prec@1: 75.000(75.000)	
06-15-23 18:38:Test: [50/79]	Time: 0.0163(0.0233)	Loss: 0.912(0.916)	Prec@1: 71.094(72.212)	
06-15-23 18:38:Test: [78/79]	Time: 0.0150(0.0208)	Loss: 0.339(0.900)	Prec@1: 87.500(72.560)	
06-15-23 18:38:Step 26 * Prec@1 72.560
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [0/391]	Time 0.387 (0.387)	Data 0.350 (0.350)	Loss 0.413 (0.413)	Prec@1 87.500 (87.500)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [50/391]	Time 0.073 (0.068)	Data 0.003 (0.009)	Loss 0.570 (0.604)	Prec@1 79.688 (78.998)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [100/391]	Time 0.068 (0.070)	Data 0.002 (0.006)	Loss 0.616 (0.604)	Prec@1 78.906 (78.767)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [150/391]	Time 0.075 (0.070)	Data 0.003 (0.005)	Loss 0.709 (0.617)	Prec@1 75.000 (78.498)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [200/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.563 (0.622)	Prec@1 82.031 (78.420)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [250/391]	Time 0.074 (0.071)	Data 0.003 (0.004)	Loss 0.747 (0.626)	Prec@1 77.344 (78.414)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [300/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.550 (0.620)	Prec@1 82.812 (78.543)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [350/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.623 (0.616)	Prec@1 73.438 (78.748)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Test: [0/79]	Time: 0.3809(0.3809)	Loss: 0.926(0.926)	Prec@1: 63.281(63.281)	
06-15-23 18:38:Test: [50/79]	Time: 0.0166(0.0238)	Loss: 1.619(1.229)	Prec@1: 56.250(63.450)	
06-15-23 18:38:Test: [78/79]	Time: 0.0150(0.0213)	Loss: 1.108(1.238)	Prec@1: 62.500(62.870)	
06-15-23 18:38:Step 27 * Prec@1 62.870
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [0/391]	Time 0.414 (0.414)	Data 0.377 (0.377)	Loss 0.497 (0.497)	Prec@1 82.812 (82.812)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [50/391]	Time 0.055 (0.071)	Data 0.002 (0.010)	Loss 0.598 (0.586)	Prec@1 78.125 (79.825)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [100/391]	Time 0.061 (0.070)	Data 0.002 (0.006)	Loss 0.480 (0.600)	Prec@1 85.156 (79.587)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [150/391]	Time 0.066 (0.069)	Data 0.002 (0.005)	Loss 0.631 (0.604)	Prec@1 77.344 (79.144)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [200/391]	Time 0.069 (0.069)	Data 0.003 (0.004)	Loss 0.716 (0.611)	Prec@1 72.656 (78.797)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:38:Num bit 8	Num grad bit 8	
06-15-23 18:38:Iter: [250/391]	Time 0.069 (0.070)	Data 0.002 (0.004)	Loss 0.513 (0.611)	Prec@1 82.031 (78.891)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [300/391]	Time 0.073 (0.070)	Data 0.002 (0.004)	Loss 0.646 (0.615)	Prec@1 74.219 (78.660)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [350/391]	Time 0.067 (0.070)	Data 0.003 (0.004)	Loss 0.571 (0.616)	Prec@1 79.688 (78.675)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Test: [0/79]	Time: 0.3633(0.3633)	Loss: 0.954(0.954)	Prec@1: 73.438(73.438)	
06-15-23 18:39:Test: [50/79]	Time: 0.0166(0.0231)	Loss: 1.262(1.216)	Prec@1: 63.281(65.365)	
06-15-23 18:39:Test: [78/79]	Time: 0.0150(0.0207)	Loss: 1.112(1.202)	Prec@1: 56.250(65.410)	
06-15-23 18:39:Step 28 * Prec@1 65.410
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [0/391]	Time 0.405 (0.405)	Data 0.350 (0.350)	Loss 0.777 (0.777)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [50/391]	Time 0.046 (0.047)	Data 0.002 (0.009)	Loss 0.759 (0.593)	Prec@1 75.781 (79.442)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [100/391]	Time 0.047 (0.048)	Data 0.002 (0.005)	Loss 0.530 (0.596)	Prec@1 83.594 (79.633)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [150/391]	Time 0.050 (0.049)	Data 0.002 (0.004)	Loss 0.486 (0.611)	Prec@1 83.594 (79.010)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [200/391]	Time 0.039 (0.049)	Data 0.002 (0.004)	Loss 0.583 (0.610)	Prec@1 81.250 (79.190)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [250/391]	Time 0.067 (0.049)	Data 0.003 (0.003)	Loss 0.832 (0.612)	Prec@1 73.438 (79.056)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [300/391]	Time 0.072 (0.053)	Data 0.002 (0.003)	Loss 0.640 (0.612)	Prec@1 78.906 (79.031)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [350/391]	Time 0.075 (0.056)	Data 0.003 (0.003)	Loss 0.556 (0.611)	Prec@1 79.688 (78.964)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Test: [0/79]	Time: 0.3594(0.3594)	Loss: 0.896(0.896)	Prec@1: 71.875(71.875)	
06-15-23 18:39:Test: [50/79]	Time: 0.0200(0.0242)	Loss: 1.244(0.916)	Prec@1: 62.500(71.431)	
06-15-23 18:39:Test: [78/79]	Time: 0.0151(0.0221)	Loss: 0.874(0.918)	Prec@1: 75.000(71.290)	
06-15-23 18:39:Step 29 * Prec@1 71.290
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [0/391]	Time 0.416 (0.416)	Data 0.358 (0.358)	Loss 0.706 (0.706)	Prec@1 82.031 (82.031)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [50/391]	Time 0.075 (0.077)	Data 0.003 (0.009)	Loss 0.693 (0.618)	Prec@1 78.906 (79.197)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [100/391]	Time 0.073 (0.075)	Data 0.003 (0.006)	Loss 0.691 (0.608)	Prec@1 75.000 (79.138)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [150/391]	Time 0.077 (0.074)	Data 0.002 (0.005)	Loss 0.513 (0.604)	Prec@1 80.469 (79.237)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [200/391]	Time 0.075 (0.074)	Data 0.003 (0.004)	Loss 0.555 (0.602)	Prec@1 79.688 (79.120)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [250/391]	Time 0.071 (0.074)	Data 0.003 (0.004)	Loss 0.551 (0.605)	Prec@1 78.906 (79.077)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [300/391]	Time 0.071 (0.073)	Data 0.002 (0.004)	Loss 0.550 (0.611)	Prec@1 83.594 (78.828)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:39:Num bit 8	Num grad bit 8	
06-15-23 18:39:Iter: [350/391]	Time 0.075 (0.073)	Data 0.002 (0.003)	Loss 0.485 (0.604)	Prec@1 84.375 (79.064)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Test: [0/79]	Time: 0.3624(0.3624)	Loss: 0.948(0.948)	Prec@1: 69.531(69.531)	
06-15-23 18:40:Test: [50/79]	Time: 0.0199(0.0254)	Loss: 0.849(1.003)	Prec@1: 71.875(67.157)	
06-15-23 18:40:Test: [78/79]	Time: 0.0183(0.0234)	Loss: 1.053(1.004)	Prec@1: 62.500(66.900)	
06-15-23 18:40:Step 30 * Prec@1 66.900
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [0/391]	Time 0.389 (0.389)	Data 0.353 (0.353)	Loss 0.534 (0.534)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [50/391]	Time 0.072 (0.071)	Data 0.003 (0.009)	Loss 0.625 (0.600)	Prec@1 76.562 (79.029)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [100/391]	Time 0.070 (0.071)	Data 0.003 (0.006)	Loss 0.567 (0.613)	Prec@1 82.031 (78.837)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [150/391]	Time 0.073 (0.070)	Data 0.003 (0.005)	Loss 0.642 (0.601)	Prec@1 77.344 (79.341)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [200/391]	Time 0.069 (0.070)	Data 0.003 (0.004)	Loss 0.713 (0.602)	Prec@1 74.219 (79.404)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [250/391]	Time 0.069 (0.070)	Data 0.003 (0.004)	Loss 0.505 (0.599)	Prec@1 82.812 (79.495)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [300/391]	Time 0.061 (0.070)	Data 0.002 (0.004)	Loss 0.718 (0.604)	Prec@1 73.438 (79.283)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [350/391]	Time 0.066 (0.070)	Data 0.002 (0.004)	Loss 0.621 (0.604)	Prec@1 77.344 (79.209)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Test: [0/79]	Time: 0.3651(0.3651)	Loss: 1.117(1.117)	Prec@1: 66.406(66.406)	
06-15-23 18:40:Test: [50/79]	Time: 0.0166(0.0244)	Loss: 1.251(1.258)	Prec@1: 63.281(63.174)	
06-15-23 18:40:Test: [78/79]	Time: 0.0151(0.0216)	Loss: 0.635(1.234)	Prec@1: 81.250(63.800)	
06-15-23 18:40:Step 31 * Prec@1 63.800
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [0/391]	Time 0.419 (0.419)	Data 0.375 (0.375)	Loss 0.583 (0.583)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [50/391]	Time 0.069 (0.075)	Data 0.002 (0.010)	Loss 0.457 (0.607)	Prec@1 84.375 (79.197)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [100/391]	Time 0.070 (0.073)	Data 0.002 (0.007)	Loss 0.692 (0.604)	Prec@1 75.781 (79.262)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [150/391]	Time 0.073 (0.072)	Data 0.002 (0.005)	Loss 0.468 (0.609)	Prec@1 83.594 (79.061)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [200/391]	Time 0.076 (0.071)	Data 0.003 (0.004)	Loss 0.506 (0.612)	Prec@1 85.938 (79.027)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [250/391]	Time 0.064 (0.071)	Data 0.002 (0.004)	Loss 0.468 (0.614)	Prec@1 82.812 (79.053)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [300/391]	Time 0.061 (0.071)	Data 0.002 (0.004)	Loss 0.839 (0.615)	Prec@1 73.438 (79.023)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:40:Num bit 8	Num grad bit 8	
06-15-23 18:40:Iter: [350/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.684 (0.614)	Prec@1 73.438 (79.011)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Test: [0/79]	Time: 0.3791(0.3791)	Loss: 0.827(0.827)	Prec@1: 72.656(72.656)	
06-15-23 18:41:Test: [50/79]	Time: 0.0164(0.0245)	Loss: 0.988(0.990)	Prec@1: 71.094(70.481)	
06-15-23 18:41:Test: [78/79]	Time: 0.0149(0.0216)	Loss: 0.970(0.990)	Prec@1: 75.000(70.240)	
06-15-23 18:41:Step 32 * Prec@1 70.240
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [0/391]	Time 0.383 (0.383)	Data 0.347 (0.347)	Loss 0.822 (0.822)	Prec@1 73.438 (73.438)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [50/391]	Time 0.071 (0.074)	Data 0.002 (0.009)	Loss 0.508 (0.634)	Prec@1 82.812 (78.462)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [100/391]	Time 0.061 (0.071)	Data 0.003 (0.006)	Loss 0.559 (0.619)	Prec@1 84.375 (78.736)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [150/391]	Time 0.065 (0.070)	Data 0.002 (0.005)	Loss 0.564 (0.612)	Prec@1 83.594 (78.968)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [200/391]	Time 0.060 (0.070)	Data 0.002 (0.004)	Loss 0.600 (0.608)	Prec@1 74.219 (79.038)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [250/391]	Time 0.069 (0.070)	Data 0.002 (0.004)	Loss 0.531 (0.609)	Prec@1 79.688 (79.031)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [300/391]	Time 0.076 (0.068)	Data 0.003 (0.004)	Loss 0.744 (0.605)	Prec@1 75.000 (79.140)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [350/391]	Time 0.067 (0.068)	Data 0.003 (0.004)	Loss 0.609 (0.607)	Prec@1 78.906 (79.067)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Test: [0/79]	Time: 0.3874(0.3874)	Loss: 0.848(0.848)	Prec@1: 72.656(72.656)	
06-15-23 18:41:Test: [50/79]	Time: 0.0163(0.0236)	Loss: 1.070(0.991)	Prec@1: 71.875(69.868)	
06-15-23 18:41:Test: [78/79]	Time: 0.0150(0.0211)	Loss: 0.418(0.979)	Prec@1: 75.000(70.390)	
06-15-23 18:41:Step 33 * Prec@1 70.390
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [0/391]	Time 0.409 (0.409)	Data 0.363 (0.363)	Loss 0.547 (0.547)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [50/391]	Time 0.067 (0.065)	Data 0.003 (0.009)	Loss 0.574 (0.580)	Prec@1 80.469 (80.178)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [100/391]	Time 0.076 (0.067)	Data 0.003 (0.006)	Loss 0.508 (0.603)	Prec@1 84.375 (79.394)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [150/391]	Time 0.073 (0.069)	Data 0.003 (0.005)	Loss 0.746 (0.607)	Prec@1 75.000 (79.217)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [200/391]	Time 0.075 (0.070)	Data 0.003 (0.004)	Loss 0.758 (0.616)	Prec@1 74.219 (78.891)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [250/391]	Time 0.080 (0.070)	Data 0.003 (0.004)	Loss 0.758 (0.618)	Prec@1 74.219 (78.903)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [300/391]	Time 0.071 (0.071)	Data 0.003 (0.004)	Loss 0.568 (0.618)	Prec@1 79.688 (78.831)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:41:Num bit 8	Num grad bit 8	
06-15-23 18:41:Iter: [350/391]	Time 0.074 (0.071)	Data 0.003 (0.004)	Loss 0.634 (0.618)	Prec@1 74.219 (78.795)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Test: [0/79]	Time: 0.3772(0.3772)	Loss: 4.740(4.740)	Prec@1: 25.781(25.781)	
06-15-23 18:42:Test: [50/79]	Time: 0.0166(0.0241)	Loss: 6.396(5.324)	Prec@1: 16.406(25.368)	
06-15-23 18:42:Test: [78/79]	Time: 0.0149(0.0213)	Loss: 4.018(5.303)	Prec@1: 50.000(25.380)	
06-15-23 18:42:Step 34 * Prec@1 25.380
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [0/391]	Time 0.401 (0.401)	Data 0.364 (0.364)	Loss 0.884 (0.884)	Prec@1 68.750 (68.750)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [50/391]	Time 0.072 (0.075)	Data 0.003 (0.010)	Loss 0.456 (0.582)	Prec@1 83.594 (79.948)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [100/391]	Time 0.074 (0.073)	Data 0.002 (0.006)	Loss 0.562 (0.596)	Prec@1 78.906 (79.633)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [150/391]	Time 0.072 (0.073)	Data 0.003 (0.005)	Loss 0.479 (0.597)	Prec@1 81.250 (79.662)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [200/391]	Time 0.070 (0.072)	Data 0.003 (0.004)	Loss 0.597 (0.603)	Prec@1 78.906 (79.439)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [250/391]	Time 0.069 (0.071)	Data 0.003 (0.004)	Loss 0.527 (0.601)	Prec@1 84.375 (79.554)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [300/391]	Time 0.067 (0.070)	Data 0.002 (0.004)	Loss 0.666 (0.605)	Prec@1 77.344 (79.397)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [350/391]	Time 0.072 (0.070)	Data 0.003 (0.004)	Loss 0.602 (0.603)	Prec@1 77.344 (79.414)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Test: [0/79]	Time: 0.4008(0.4008)	Loss: 2.371(2.371)	Prec@1: 43.750(43.750)	
06-15-23 18:42:Test: [50/79]	Time: 0.0197(0.0278)	Loss: 2.888(2.486)	Prec@1: 29.688(38.312)	
06-15-23 18:42:Test: [78/79]	Time: 0.0184(0.0250)	Loss: 2.770(2.485)	Prec@1: 31.250(38.180)	
06-15-23 18:42:Step 35 * Prec@1 38.180
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [0/391]	Time 0.420 (0.420)	Data 0.383 (0.383)	Loss 0.848 (0.848)	Prec@1 68.750 (68.750)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [50/391]	Time 0.072 (0.066)	Data 0.003 (0.010)	Loss 0.542 (0.617)	Prec@1 82.812 (79.013)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [100/391]	Time 0.069 (0.068)	Data 0.003 (0.006)	Loss 0.594 (0.608)	Prec@1 82.031 (79.355)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [150/391]	Time 0.061 (0.069)	Data 0.002 (0.005)	Loss 0.659 (0.613)	Prec@1 78.906 (79.232)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [200/391]	Time 0.069 (0.069)	Data 0.002 (0.005)	Loss 0.647 (0.616)	Prec@1 77.344 (79.097)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [250/391]	Time 0.075 (0.070)	Data 0.003 (0.004)	Loss 0.667 (0.613)	Prec@1 78.906 (79.118)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [300/391]	Time 0.072 (0.070)	Data 0.003 (0.004)	Loss 0.584 (0.607)	Prec@1 78.906 (79.288)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Num bit 8	Num grad bit 8	
06-15-23 18:42:Iter: [350/391]	Time 0.074 (0.071)	Data 0.003 (0.004)	Loss 0.597 (0.608)	Prec@1 79.688 (79.238)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:42:Test: [0/79]	Time: 0.3432(0.3432)	Loss: 0.652(0.652)	Prec@1: 78.125(78.125)	
06-15-23 18:42:Test: [50/79]	Time: 0.0165(0.0228)	Loss: 0.921(0.898)	Prec@1: 70.312(72.289)	
06-15-23 18:43:Test: [78/79]	Time: 0.0149(0.0205)	Loss: 0.603(0.908)	Prec@1: 81.250(72.070)	
06-15-23 18:43:Step 36 * Prec@1 72.070
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [0/391]	Time 0.391 (0.391)	Data 0.355 (0.355)	Loss 0.505 (0.505)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [50/391]	Time 0.070 (0.069)	Data 0.003 (0.009)	Loss 0.651 (0.622)	Prec@1 76.562 (78.753)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [100/391]	Time 0.069 (0.069)	Data 0.002 (0.006)	Loss 0.498 (0.614)	Prec@1 85.938 (78.999)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [150/391]	Time 0.071 (0.069)	Data 0.003 (0.005)	Loss 0.668 (0.619)	Prec@1 79.688 (78.974)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [200/391]	Time 0.076 (0.070)	Data 0.003 (0.004)	Loss 0.656 (0.611)	Prec@1 77.344 (79.264)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [250/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.513 (0.606)	Prec@1 82.031 (79.336)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [300/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.967 (0.604)	Prec@1 71.094 (79.420)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [350/391]	Time 0.068 (0.070)	Data 0.003 (0.004)	Loss 0.498 (0.604)	Prec@1 82.812 (79.438)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Test: [0/79]	Time: 0.3838(0.3838)	Loss: 1.013(1.013)	Prec@1: 61.719(61.719)	
06-15-23 18:43:Test: [50/79]	Time: 0.0165(0.0237)	Loss: 1.247(1.112)	Prec@1: 63.281(64.507)	
06-15-23 18:43:Test: [78/79]	Time: 0.0152(0.0212)	Loss: 1.034(1.113)	Prec@1: 68.750(64.370)	
06-15-23 18:43:Step 37 * Prec@1 64.370
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [0/391]	Time 0.381 (0.381)	Data 0.344 (0.344)	Loss 0.763 (0.763)	Prec@1 75.000 (75.000)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [50/391]	Time 0.071 (0.062)	Data 0.003 (0.009)	Loss 0.823 (0.575)	Prec@1 74.219 (80.025)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [100/391]	Time 0.069 (0.067)	Data 0.002 (0.006)	Loss 0.604 (0.594)	Prec@1 81.250 (79.417)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [150/391]	Time 0.062 (0.065)	Data 0.002 (0.005)	Loss 0.834 (0.609)	Prec@1 70.312 (79.124)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [200/391]	Time 0.074 (0.067)	Data 0.003 (0.004)	Loss 0.791 (0.612)	Prec@1 75.781 (79.015)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [250/391]	Time 0.074 (0.068)	Data 0.003 (0.004)	Loss 0.610 (0.614)	Prec@1 75.781 (78.906)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [300/391]	Time 0.073 (0.068)	Data 0.002 (0.004)	Loss 0.755 (0.612)	Prec@1 75.781 (78.976)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [350/391]	Time 0.071 (0.069)	Data 0.003 (0.004)	Loss 0.536 (0.608)	Prec@1 78.906 (79.093)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:43:Test: [0/79]	Time: 0.3577(0.3577)	Loss: 1.043(1.043)	Prec@1: 70.312(70.312)	
06-15-23 18:43:Test: [50/79]	Time: 0.0165(0.0232)	Loss: 1.125(1.246)	Prec@1: 67.188(65.334)	
06-15-23 18:43:Test: [78/79]	Time: 0.0153(0.0208)	Loss: 1.442(1.258)	Prec@1: 62.500(65.200)	
06-15-23 18:43:Step 38 * Prec@1 65.200
06-15-23 18:43:Num bit 8	Num grad bit 8	
06-15-23 18:43:Iter: [0/391]	Time 0.396 (0.396)	Data 0.359 (0.359)	Loss 0.510 (0.510)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [50/391]	Time 0.074 (0.070)	Data 0.003 (0.009)	Loss 0.510 (0.585)	Prec@1 82.812 (80.116)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [100/391]	Time 0.068 (0.071)	Data 0.003 (0.006)	Loss 0.482 (0.596)	Prec@1 84.375 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [150/391]	Time 0.069 (0.067)	Data 0.003 (0.005)	Loss 0.647 (0.594)	Prec@1 76.562 (79.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [200/391]	Time 0.067 (0.068)	Data 0.003 (0.004)	Loss 0.633 (0.594)	Prec@1 78.125 (79.571)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [250/391]	Time 0.071 (0.069)	Data 0.003 (0.004)	Loss 0.658 (0.594)	Prec@1 76.562 (79.544)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [300/391]	Time 0.075 (0.069)	Data 0.003 (0.004)	Loss 0.713 (0.596)	Prec@1 74.219 (79.540)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [350/391]	Time 0.072 (0.069)	Data 0.003 (0.004)	Loss 0.631 (0.594)	Prec@1 80.469 (79.645)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Test: [0/79]	Time: 0.3851(0.3851)	Loss: 3.570(3.570)	Prec@1: 39.844(39.844)	
06-15-23 18:44:Test: [50/79]	Time: 0.0165(0.0236)	Loss: 4.089(3.625)	Prec@1: 31.250(38.051)	
06-15-23 18:44:Test: [78/79]	Time: 0.0149(0.0210)	Loss: 3.642(3.608)	Prec@1: 43.750(38.250)	
06-15-23 18:44:Step 39 * Prec@1 38.250
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [0/391]	Time 0.400 (0.400)	Data 0.358 (0.358)	Loss 0.697 (0.697)	Prec@1 74.219 (74.219)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [50/391]	Time 0.074 (0.074)	Data 0.003 (0.010)	Loss 0.460 (0.562)	Prec@1 83.594 (80.607)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [100/391]	Time 0.073 (0.073)	Data 0.003 (0.006)	Loss 0.501 (0.584)	Prec@1 84.375 (79.981)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [150/391]	Time 0.073 (0.072)	Data 0.002 (0.005)	Loss 0.647 (0.588)	Prec@1 78.906 (79.832)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [200/391]	Time 0.065 (0.071)	Data 0.003 (0.004)	Loss 0.597 (0.601)	Prec@1 79.688 (79.563)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [250/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.547 (0.602)	Prec@1 82.812 (79.479)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [300/391]	Time 0.060 (0.071)	Data 0.002 (0.004)	Loss 0.536 (0.600)	Prec@1 83.594 (79.490)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [350/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.701 (0.604)	Prec@1 75.000 (79.278)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:44:Test: [0/79]	Time: 0.3693(0.3693)	Loss: 1.284(1.284)	Prec@1: 60.938(60.938)	
06-15-23 18:44:Test: [50/79]	Time: 0.0167(0.0236)	Loss: 1.382(1.342)	Prec@1: 53.906(57.797)	
06-15-23 18:44:Test: [78/79]	Time: 0.0152(0.0211)	Loss: 0.847(1.336)	Prec@1: 68.750(57.920)	
06-15-23 18:44:Step 40 * Prec@1 57.920
06-15-23 18:44:Num bit 8	Num grad bit 8	
06-15-23 18:44:Iter: [0/391]	Time 0.390 (0.390)	Data 0.354 (0.354)	Loss 0.616 (0.616)	Prec@1 74.219 (74.219)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [50/391]	Time 0.069 (0.070)	Data 0.002 (0.009)	Loss 0.747 (0.589)	Prec@1 76.562 (80.162)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [100/391]	Time 0.076 (0.072)	Data 0.003 (0.006)	Loss 0.640 (0.585)	Prec@1 76.562 (80.113)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [150/391]	Time 0.074 (0.072)	Data 0.003 (0.005)	Loss 0.727 (0.587)	Prec@1 77.344 (80.081)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [200/391]	Time 0.069 (0.072)	Data 0.003 (0.004)	Loss 0.775 (0.589)	Prec@1 73.438 (80.162)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [250/391]	Time 0.069 (0.072)	Data 0.003 (0.004)	Loss 0.660 (0.597)	Prec@1 77.344 (79.831)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [300/391]	Time 0.074 (0.072)	Data 0.003 (0.004)	Loss 0.671 (0.602)	Prec@1 78.125 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [350/391]	Time 0.068 (0.072)	Data 0.002 (0.004)	Loss 0.641 (0.602)	Prec@1 78.125 (79.696)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Test: [0/79]	Time: 0.3692(0.3692)	Loss: 1.064(1.064)	Prec@1: 67.969(67.969)	
06-15-23 18:45:Test: [50/79]	Time: 0.0166(0.0252)	Loss: 1.380(1.240)	Prec@1: 56.250(64.262)	
06-15-23 18:45:Test: [78/79]	Time: 0.0160(0.0221)	Loss: 0.868(1.218)	Prec@1: 81.250(64.740)	
06-15-23 18:45:Step 41 * Prec@1 64.740
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [0/391]	Time 0.401 (0.401)	Data 0.365 (0.365)	Loss 0.814 (0.814)	Prec@1 76.562 (76.562)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [50/391]	Time 0.070 (0.069)	Data 0.003 (0.009)	Loss 0.510 (0.597)	Prec@1 82.812 (79.979)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [100/391]	Time 0.069 (0.070)	Data 0.003 (0.006)	Loss 0.485 (0.590)	Prec@1 80.469 (79.912)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [150/391]	Time 0.063 (0.070)	Data 0.002 (0.005)	Loss 0.653 (0.596)	Prec@1 74.219 (79.341)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [200/391]	Time 0.070 (0.070)	Data 0.003 (0.004)	Loss 0.665 (0.595)	Prec@1 77.344 (79.524)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [250/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.501 (0.598)	Prec@1 83.594 (79.504)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [300/391]	Time 0.072 (0.070)	Data 0.003 (0.004)	Loss 0.748 (0.595)	Prec@1 75.000 (79.654)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [350/391]	Time 0.070 (0.070)	Data 0.003 (0.004)	Loss 0.454 (0.594)	Prec@1 85.156 (79.745)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Test: [0/79]	Time: 0.3821(0.3821)	Loss: 1.910(1.910)	Prec@1: 53.906(53.906)	
06-15-23 18:45:Test: [50/79]	Time: 0.0167(0.0237)	Loss: 1.483(1.804)	Prec@1: 61.719(54.182)	
06-15-23 18:45:Test: [78/79]	Time: 0.0152(0.0211)	Loss: 1.573(1.808)	Prec@1: 56.250(54.040)	
06-15-23 18:45:Step 42 * Prec@1 54.040
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [0/391]	Time 0.410 (0.410)	Data 0.367 (0.367)	Loss 0.793 (0.793)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:45:Num bit 8	Num grad bit 8	
06-15-23 18:45:Iter: [50/391]	Time 0.064 (0.073)	Data 0.002 (0.010)	Loss 0.631 (0.626)	Prec@1 79.688 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [100/391]	Time 0.076 (0.073)	Data 0.003 (0.006)	Loss 0.620 (0.631)	Prec@1 80.469 (78.597)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [150/391]	Time 0.074 (0.073)	Data 0.003 (0.005)	Loss 0.624 (0.616)	Prec@1 79.688 (79.118)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [200/391]	Time 0.066 (0.071)	Data 0.003 (0.004)	Loss 0.617 (0.612)	Prec@1 80.469 (79.248)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [250/391]	Time 0.072 (0.070)	Data 0.003 (0.004)	Loss 0.492 (0.610)	Prec@1 83.594 (79.308)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [300/391]	Time 0.064 (0.070)	Data 0.003 (0.004)	Loss 0.507 (0.608)	Prec@1 82.031 (79.371)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [350/391]	Time 0.056 (0.068)	Data 0.002 (0.004)	Loss 0.757 (0.610)	Prec@1 72.656 (79.262)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Test: [0/79]	Time: 0.3481(0.3481)	Loss: 2.939(2.939)	Prec@1: 42.969(42.969)	
06-15-23 18:46:Test: [50/79]	Time: 0.0165(0.0229)	Loss: 3.541(3.381)	Prec@1: 41.406(43.137)	
06-15-23 18:46:Test: [78/79]	Time: 0.0149(0.0206)	Loss: 1.799(3.367)	Prec@1: 62.500(43.210)	
06-15-23 18:46:Step 43 * Prec@1 43.210
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [0/391]	Time 0.408 (0.408)	Data 0.372 (0.372)	Loss 0.564 (0.564)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [50/391]	Time 0.058 (0.070)	Data 0.002 (0.010)	Loss 0.632 (0.625)	Prec@1 81.250 (78.569)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [100/391]	Time 0.069 (0.070)	Data 0.003 (0.006)	Loss 0.533 (0.612)	Prec@1 81.250 (78.914)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [150/391]	Time 0.068 (0.069)	Data 0.002 (0.005)	Loss 0.502 (0.608)	Prec@1 82.812 (79.067)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [200/391]	Time 0.068 (0.070)	Data 0.003 (0.004)	Loss 0.488 (0.602)	Prec@1 80.469 (79.303)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [250/391]	Time 0.070 (0.070)	Data 0.003 (0.004)	Loss 0.622 (0.599)	Prec@1 76.562 (79.414)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [300/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.644 (0.604)	Prec@1 78.125 (79.285)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [350/391]	Time 0.058 (0.069)	Data 0.003 (0.004)	Loss 0.696 (0.601)	Prec@1 77.344 (79.351)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Test: [0/79]	Time: 0.3580(0.3580)	Loss: 3.762(3.762)	Prec@1: 34.375(34.375)	
06-15-23 18:46:Test: [50/79]	Time: 0.0166(0.0232)	Loss: 3.922(3.738)	Prec@1: 32.031(33.578)	
06-15-23 18:46:Test: [78/79]	Time: 0.0151(0.0209)	Loss: 1.081(3.708)	Prec@1: 68.750(34.030)	
06-15-23 18:46:Step 44 * Prec@1 34.030
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [0/391]	Time 0.388 (0.388)	Data 0.352 (0.352)	Loss 1.167 (1.167)	Prec@1 66.406 (66.406)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [50/391]	Time 0.070 (0.065)	Data 0.002 (0.009)	Loss 0.518 (0.610)	Prec@1 81.250 (79.596)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:46:Num bit 8	Num grad bit 8	
06-15-23 18:46:Iter: [100/391]	Time 0.074 (0.068)	Data 0.003 (0.006)	Loss 0.715 (0.609)	Prec@1 73.438 (79.138)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [150/391]	Time 0.074 (0.069)	Data 0.003 (0.005)	Loss 0.468 (0.617)	Prec@1 85.938 (78.948)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [200/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.774 (0.622)	Prec@1 73.438 (78.778)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [250/391]	Time 0.069 (0.069)	Data 0.002 (0.004)	Loss 0.579 (0.621)	Prec@1 82.812 (78.850)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [300/391]	Time 0.068 (0.070)	Data 0.003 (0.004)	Loss 0.614 (0.624)	Prec@1 80.469 (78.787)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [350/391]	Time 0.057 (0.067)	Data 0.002 (0.004)	Loss 0.752 (0.623)	Prec@1 74.219 (78.835)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Test: [0/79]	Time: 0.3788(0.3788)	Loss: 1.906(1.906)	Prec@1: 54.688(54.688)	
06-15-23 18:47:Test: [50/79]	Time: 0.0165(0.0236)	Loss: 2.262(2.053)	Prec@1: 44.531(52.007)	
06-15-23 18:47:Test: [78/79]	Time: 0.0149(0.0211)	Loss: 2.311(2.024)	Prec@1: 50.000(52.440)	
06-15-23 18:47:Step 45 * Prec@1 52.440
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [0/391]	Time 0.396 (0.396)	Data 0.359 (0.359)	Loss 0.664 (0.664)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [50/391]	Time 0.069 (0.065)	Data 0.003 (0.009)	Loss 0.607 (0.634)	Prec@1 75.000 (78.585)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [100/391]	Time 0.072 (0.068)	Data 0.003 (0.006)	Loss 0.536 (0.621)	Prec@1 82.812 (78.922)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [150/391]	Time 0.074 (0.069)	Data 0.003 (0.005)	Loss 0.635 (0.618)	Prec@1 79.688 (79.087)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [200/391]	Time 0.062 (0.069)	Data 0.002 (0.004)	Loss 0.606 (0.616)	Prec@1 83.594 (79.069)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [250/391]	Time 0.068 (0.069)	Data 0.002 (0.004)	Loss 0.707 (0.613)	Prec@1 81.250 (79.211)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [300/391]	Time 0.069 (0.070)	Data 0.002 (0.004)	Loss 0.545 (0.608)	Prec@1 83.594 (79.327)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [350/391]	Time 0.067 (0.070)	Data 0.003 (0.004)	Loss 0.656 (0.608)	Prec@1 78.906 (79.307)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Test: [0/79]	Time: 0.3726(0.3726)	Loss: 1.287(1.287)	Prec@1: 65.625(65.625)	
06-15-23 18:47:Test: [50/79]	Time: 0.0167(0.0237)	Loss: 1.675(1.463)	Prec@1: 52.344(60.738)	
06-15-23 18:47:Test: [78/79]	Time: 0.0153(0.0213)	Loss: 1.278(1.453)	Prec@1: 62.500(61.300)	
06-15-23 18:47:Step 46 * Prec@1 61.300
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [0/391]	Time 0.389 (0.389)	Data 0.352 (0.352)	Loss 0.601 (0.601)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [50/391]	Time 0.051 (0.052)	Data 0.002 (0.009)	Loss 0.662 (0.641)	Prec@1 80.469 (78.646)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [100/391]	Time 0.050 (0.051)	Data 0.002 (0.006)	Loss 0.722 (0.617)	Prec@1 71.094 (79.100)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [150/391]	Time 0.051 (0.050)	Data 0.002 (0.004)	Loss 0.658 (0.627)	Prec@1 77.344 (78.834)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:47:Num bit 8	Num grad bit 8	
06-15-23 18:47:Iter: [200/391]	Time 0.051 (0.050)	Data 0.002 (0.004)	Loss 0.436 (0.618)	Prec@1 85.156 (79.104)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [250/391]	Time 0.051 (0.050)	Data 0.002 (0.004)	Loss 0.577 (0.610)	Prec@1 80.469 (79.270)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [300/391]	Time 0.052 (0.050)	Data 0.002 (0.003)	Loss 0.644 (0.611)	Prec@1 82.812 (79.220)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [350/391]	Time 0.083 (0.051)	Data 0.003 (0.003)	Loss 0.573 (0.611)	Prec@1 78.906 (79.205)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Test: [0/79]	Time: 0.3765(0.3765)	Loss: 1.653(1.653)	Prec@1: 52.344(52.344)	
06-15-23 18:48:Test: [50/79]	Time: 0.0165(0.0235)	Loss: 1.907(1.756)	Prec@1: 47.656(48.591)	
06-15-23 18:48:Test: [78/79]	Time: 0.0149(0.0210)	Loss: 2.018(1.759)	Prec@1: 50.000(48.440)	
06-15-23 18:48:Step 47 * Prec@1 48.440
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [0/391]	Time 0.390 (0.390)	Data 0.354 (0.354)	Loss 0.564 (0.564)	Prec@1 77.344 (77.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [50/391]	Time 0.047 (0.065)	Data 0.002 (0.009)	Loss 0.703 (0.607)	Prec@1 78.125 (79.136)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [100/391]	Time 0.049 (0.058)	Data 0.002 (0.006)	Loss 0.545 (0.605)	Prec@1 80.469 (79.378)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [150/391]	Time 0.042 (0.054)	Data 0.002 (0.004)	Loss 0.785 (0.610)	Prec@1 75.781 (79.299)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [200/391]	Time 0.066 (0.054)	Data 0.003 (0.004)	Loss 0.647 (0.606)	Prec@1 77.344 (79.349)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [250/391]	Time 0.072 (0.057)	Data 0.003 (0.004)	Loss 0.437 (0.608)	Prec@1 85.938 (79.298)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [300/391]	Time 0.046 (0.056)	Data 0.002 (0.003)	Loss 0.747 (0.609)	Prec@1 74.219 (79.249)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [350/391]	Time 0.060 (0.056)	Data 0.002 (0.003)	Loss 0.800 (0.614)	Prec@1 75.000 (79.233)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Test: [0/79]	Time: 0.3672(0.3672)	Loss: 4.641(4.641)	Prec@1: 35.156(35.156)	
06-15-23 18:48:Test: [50/79]	Time: 0.0167(0.0244)	Loss: 5.220(4.651)	Prec@1: 28.125(36.994)	
06-15-23 18:48:Test: [78/79]	Time: 0.0152(0.0217)	Loss: 2.888(4.606)	Prec@1: 37.500(37.450)	
06-15-23 18:48:Step 48 * Prec@1 37.450
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [0/391]	Time 0.417 (0.417)	Data 0.381 (0.381)	Loss 0.763 (0.763)	Prec@1 75.000 (75.000)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [50/391]	Time 0.039 (0.052)	Data 0.002 (0.009)	Loss 0.508 (0.607)	Prec@1 82.031 (78.968)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [100/391]	Time 0.054 (0.046)	Data 0.002 (0.006)	Loss 0.583 (0.594)	Prec@1 77.344 (79.610)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [150/391]	Time 0.074 (0.054)	Data 0.003 (0.005)	Loss 0.622 (0.614)	Prec@1 77.344 (79.082)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [200/391]	Time 0.072 (0.059)	Data 0.003 (0.004)	Loss 0.443 (0.606)	Prec@1 83.594 (79.466)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [250/391]	Time 0.043 (0.060)	Data 0.002 (0.004)	Loss 0.793 (0.604)	Prec@1 73.438 (79.470)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [300/391]	Time 0.044 (0.057)	Data 0.002 (0.004)	Loss 0.612 (0.604)	Prec@1 79.688 (79.566)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Num bit 8	Num grad bit 8	
06-15-23 18:48:Iter: [350/391]	Time 0.047 (0.057)	Data 0.002 (0.003)	Loss 0.648 (0.605)	Prec@1 81.250 (79.516)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:48:Test: [0/79]	Time: 0.3469(0.3469)	Loss: 2.964(2.964)	Prec@1: 39.062(39.062)	
06-15-23 18:48:Test: [50/79]	Time: 0.0164(0.0237)	Loss: 3.860(3.559)	Prec@1: 28.906(36.780)	
06-15-23 18:49:Test: [78/79]	Time: 0.0151(0.0211)	Loss: 1.873(3.566)	Prec@1: 68.750(37.210)	
06-15-23 18:49:Step 49 * Prec@1 37.210
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [0/391]	Time 0.394 (0.394)	Data 0.357 (0.357)	Loss 0.640 (0.640)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [50/391]	Time 0.069 (0.070)	Data 0.003 (0.009)	Loss 0.677 (0.583)	Prec@1 78.906 (80.116)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [100/391]	Time 0.063 (0.068)	Data 0.002 (0.006)	Loss 0.478 (0.580)	Prec@1 82.031 (80.654)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [150/391]	Time 0.034 (0.067)	Data 0.002 (0.005)	Loss 0.821 (0.591)	Prec@1 75.781 (80.246)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [200/391]	Time 0.049 (0.061)	Data 0.003 (0.004)	Loss 0.738 (0.602)	Prec@1 78.906 (79.882)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [250/391]	Time 0.052 (0.059)	Data 0.002 (0.004)	Loss 0.606 (0.598)	Prec@1 77.344 (79.961)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [300/391]	Time 0.050 (0.057)	Data 0.002 (0.003)	Loss 0.706 (0.601)	Prec@1 76.562 (79.755)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [350/391]	Time 0.074 (0.058)	Data 0.003 (0.003)	Loss 0.827 (0.606)	Prec@1 75.781 (79.619)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Test: [0/79]	Time: 0.3693(0.3693)	Loss: 2.402(2.402)	Prec@1: 42.188(42.188)	
06-15-23 18:49:Test: [50/79]	Time: 0.0164(0.0233)	Loss: 3.212(2.736)	Prec@1: 40.625(46.890)	
06-15-23 18:49:Test: [78/79]	Time: 0.0150(0.0208)	Loss: 1.836(2.689)	Prec@1: 43.750(47.130)	
06-15-23 18:49:Step 50 * Prec@1 47.130
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [0/391]	Time 0.383 (0.383)	Data 0.346 (0.346)	Loss 0.568 (0.568)	Prec@1 82.812 (82.812)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [50/391]	Time 0.087 (0.056)	Data 0.002 (0.009)	Loss 0.502 (0.576)	Prec@1 80.469 (80.270)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [100/391]	Time 0.058 (0.054)	Data 0.002 (0.006)	Loss 0.471 (0.591)	Prec@1 83.594 (79.796)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [150/391]	Time 0.069 (0.055)	Data 0.002 (0.004)	Loss 0.558 (0.598)	Prec@1 78.906 (79.713)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [200/391]	Time 0.054 (0.057)	Data 0.002 (0.004)	Loss 0.577 (0.594)	Prec@1 84.375 (79.928)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [250/391]	Time 0.050 (0.056)	Data 0.002 (0.004)	Loss 0.632 (0.596)	Prec@1 81.250 (79.837)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [300/391]	Time 0.053 (0.055)	Data 0.002 (0.003)	Loss 0.543 (0.595)	Prec@1 82.031 (79.778)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [350/391]	Time 0.057 (0.054)	Data 0.002 (0.003)	Loss 0.654 (0.602)	Prec@1 76.562 (79.652)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Test: [0/79]	Time: 0.3922(0.3922)	Loss: 4.190(4.190)	Prec@1: 42.969(42.969)	
06-15-23 18:49:Test: [50/79]	Time: 0.0166(0.0239)	Loss: 4.231(3.913)	Prec@1: 35.938(43.735)	
06-15-23 18:49:Test: [78/79]	Time: 0.0151(0.0214)	Loss: 3.767(3.880)	Prec@1: 43.750(44.140)	
06-15-23 18:49:Step 51 * Prec@1 44.140
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [0/391]	Time 0.451 (0.451)	Data 0.393 (0.393)	Loss 0.596 (0.596)	Prec@1 76.562 (76.562)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [50/391]	Time 0.032 (0.072)	Data 0.001 (0.010)	Loss 0.515 (0.586)	Prec@1 83.594 (79.626)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [100/391]	Time 0.073 (0.065)	Data 0.003 (0.006)	Loss 0.735 (0.604)	Prec@1 75.000 (79.208)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:49:Num bit 8	Num grad bit 8	
06-15-23 18:49:Iter: [150/391]	Time 0.045 (0.063)	Data 0.002 (0.005)	Loss 0.646 (0.601)	Prec@1 77.344 (79.558)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [200/391]	Time 0.060 (0.059)	Data 0.002 (0.004)	Loss 0.612 (0.609)	Prec@1 79.688 (79.283)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [250/391]	Time 0.060 (0.057)	Data 0.003 (0.004)	Loss 0.603 (0.599)	Prec@1 79.688 (79.712)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [300/391]	Time 0.076 (0.059)	Data 0.002 (0.004)	Loss 0.620 (0.599)	Prec@1 78.906 (79.734)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [350/391]	Time 0.073 (0.060)	Data 0.003 (0.003)	Loss 0.813 (0.600)	Prec@1 75.000 (79.694)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Test: [0/79]	Time: 0.3648(0.3648)	Loss: 4.505(4.505)	Prec@1: 31.250(31.250)	
06-15-23 18:50:Test: [50/79]	Time: 0.0167(0.0235)	Loss: 4.521(4.396)	Prec@1: 20.312(26.578)	
06-15-23 18:50:Test: [78/79]	Time: 0.0152(0.0210)	Loss: 2.816(4.430)	Prec@1: 25.000(26.420)	
06-15-23 18:50:Step 52 * Prec@1 26.420
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [0/391]	Time 0.427 (0.427)	Data 0.357 (0.357)	Loss 0.668 (0.668)	Prec@1 82.812 (82.812)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [50/391]	Time 0.069 (0.067)	Data 0.003 (0.009)	Loss 0.487 (0.582)	Prec@1 84.375 (80.132)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [100/391]	Time 0.071 (0.068)	Data 0.003 (0.006)	Loss 0.749 (0.599)	Prec@1 73.438 (79.448)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [150/391]	Time 0.070 (0.069)	Data 0.003 (0.005)	Loss 0.617 (0.602)	Prec@1 77.344 (79.537)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [200/391]	Time 0.071 (0.069)	Data 0.003 (0.004)	Loss 0.492 (0.609)	Prec@1 83.594 (79.330)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [250/391]	Time 0.050 (0.067)	Data 0.002 (0.004)	Loss 0.612 (0.607)	Prec@1 79.688 (79.454)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [300/391]	Time 0.064 (0.067)	Data 0.003 (0.004)	Loss 0.625 (0.604)	Prec@1 78.906 (79.493)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [350/391]	Time 0.074 (0.068)	Data 0.003 (0.004)	Loss 0.637 (0.608)	Prec@1 74.219 (79.351)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Test: [0/79]	Time: 0.3764(0.3764)	Loss: 1.821(1.821)	Prec@1: 54.688(54.688)	
06-15-23 18:50:Test: [50/79]	Time: 0.0173(0.0237)	Loss: 1.960(2.015)	Prec@1: 46.875(48.851)	
06-15-23 18:50:Test: [78/79]	Time: 0.0151(0.0212)	Loss: 1.541(2.013)	Prec@1: 62.500(48.710)	
06-15-23 18:50:Step 53 * Prec@1 48.710
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [0/391]	Time 0.391 (0.391)	Data 0.354 (0.354)	Loss 0.729 (0.729)	Prec@1 75.000 (75.000)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [50/391]	Time 0.075 (0.064)	Data 0.003 (0.009)	Loss 0.627 (0.614)	Prec@1 76.562 (79.213)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [100/391]	Time 0.063 (0.061)	Data 0.002 (0.006)	Loss 0.580 (0.607)	Prec@1 73.438 (79.363)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [150/391]	Time 0.074 (0.064)	Data 0.003 (0.005)	Loss 0.742 (0.604)	Prec@1 78.906 (79.548)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [200/391]	Time 0.046 (0.062)	Data 0.002 (0.004)	Loss 0.670 (0.605)	Prec@1 76.562 (79.485)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:50:Num bit 8	Num grad bit 8	
06-15-23 18:50:Iter: [250/391]	Time 0.049 (0.058)	Data 0.002 (0.004)	Loss 0.447 (0.604)	Prec@1 85.938 (79.554)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [300/391]	Time 0.065 (0.057)	Data 0.002 (0.003)	Loss 0.597 (0.601)	Prec@1 82.812 (79.677)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [350/391]	Time 0.076 (0.059)	Data 0.003 (0.003)	Loss 0.853 (0.604)	Prec@1 71.094 (79.554)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Test: [0/79]	Time: 0.3597(0.3597)	Loss: 1.048(1.048)	Prec@1: 65.625(65.625)	
06-15-23 18:51:Test: [50/79]	Time: 0.0165(0.0232)	Loss: 1.238(1.167)	Prec@1: 63.281(62.592)	
06-15-23 18:51:Test: [78/79]	Time: 0.0149(0.0208)	Loss: 1.223(1.173)	Prec@1: 50.000(62.100)	
06-15-23 18:51:Step 54 * Prec@1 62.100
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [0/391]	Time 0.392 (0.392)	Data 0.355 (0.355)	Loss 0.393 (0.393)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [50/391]	Time 0.067 (0.070)	Data 0.003 (0.009)	Loss 0.626 (0.585)	Prec@1 78.125 (80.362)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [100/391]	Time 0.062 (0.063)	Data 0.003 (0.006)	Loss 0.470 (0.582)	Prec@1 85.156 (80.136)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [150/391]	Time 0.063 (0.065)	Data 0.002 (0.005)	Loss 0.557 (0.592)	Prec@1 80.469 (79.719)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [200/391]	Time 0.068 (0.067)	Data 0.002 (0.004)	Loss 0.514 (0.597)	Prec@1 79.688 (79.621)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [250/391]	Time 0.073 (0.068)	Data 0.003 (0.004)	Loss 0.589 (0.592)	Prec@1 78.125 (79.734)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [300/391]	Time 0.067 (0.068)	Data 0.002 (0.004)	Loss 0.551 (0.594)	Prec@1 77.344 (79.685)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [350/391]	Time 0.074 (0.068)	Data 0.003 (0.004)	Loss 0.614 (0.593)	Prec@1 78.125 (79.683)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Test: [0/79]	Time: 0.3700(0.3700)	Loss: 6.316(6.316)	Prec@1: 21.094(21.094)	
06-15-23 18:51:Test: [50/79]	Time: 0.0164(0.0234)	Loss: 6.615(6.314)	Prec@1: 16.406(23.024)	
06-15-23 18:51:Test: [78/79]	Time: 0.0150(0.0210)	Loss: 4.213(6.357)	Prec@1: 31.250(23.080)	
06-15-23 18:51:Step 55 * Prec@1 23.080
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [0/391]	Time 0.382 (0.382)	Data 0.342 (0.342)	Loss 0.653 (0.653)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [50/391]	Time 0.039 (0.066)	Data 0.002 (0.009)	Loss 0.906 (0.635)	Prec@1 69.531 (78.676)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [100/391]	Time 0.054 (0.063)	Data 0.002 (0.006)	Loss 0.432 (0.614)	Prec@1 84.375 (79.486)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [150/391]	Time 0.068 (0.064)	Data 0.003 (0.005)	Loss 0.585 (0.607)	Prec@1 79.688 (79.703)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [200/391]	Time 0.072 (0.064)	Data 0.003 (0.004)	Loss 0.645 (0.602)	Prec@1 78.125 (79.796)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [250/391]	Time 0.071 (0.064)	Data 0.003 (0.004)	Loss 0.452 (0.603)	Prec@1 84.375 (79.697)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [300/391]	Time 0.068 (0.065)	Data 0.003 (0.004)	Loss 0.468 (0.605)	Prec@1 83.594 (79.615)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:51:Num bit 8	Num grad bit 8	
06-15-23 18:51:Iter: [350/391]	Time 0.061 (0.066)	Data 0.002 (0.003)	Loss 0.752 (0.609)	Prec@1 77.344 (79.454)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Test: [0/79]	Time: 0.3951(0.3951)	Loss: 0.901(0.901)	Prec@1: 73.438(73.438)	
06-15-23 18:52:Test: [50/79]	Time: 0.0166(0.0240)	Loss: 1.061(1.048)	Prec@1: 63.281(66.376)	
06-15-23 18:52:Test: [78/79]	Time: 0.0149(0.0214)	Loss: 0.481(1.039)	Prec@1: 75.000(66.550)	
06-15-23 18:52:Step 56 * Prec@1 66.550
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [0/391]	Time 0.388 (0.388)	Data 0.348 (0.348)	Loss 0.593 (0.593)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [50/391]	Time 0.074 (0.073)	Data 0.003 (0.009)	Loss 0.445 (0.604)	Prec@1 85.938 (79.151)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [100/391]	Time 0.075 (0.072)	Data 0.003 (0.006)	Loss 0.537 (0.595)	Prec@1 78.906 (79.417)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [150/391]	Time 0.071 (0.073)	Data 0.003 (0.005)	Loss 0.581 (0.585)	Prec@1 76.562 (79.972)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [200/391]	Time 0.073 (0.072)	Data 0.003 (0.004)	Loss 0.693 (0.596)	Prec@1 76.562 (79.618)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [250/391]	Time 0.073 (0.072)	Data 0.003 (0.004)	Loss 0.669 (0.590)	Prec@1 78.906 (79.800)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [300/391]	Time 0.076 (0.072)	Data 0.003 (0.004)	Loss 0.854 (0.590)	Prec@1 70.312 (79.828)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [350/391]	Time 0.076 (0.072)	Data 0.003 (0.004)	Loss 0.522 (0.595)	Prec@1 81.250 (79.674)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Test: [0/79]	Time: 0.3872(0.3872)	Loss: 1.641(1.641)	Prec@1: 53.906(53.906)	
06-15-23 18:52:Test: [50/79]	Time: 0.0265(0.0293)	Loss: 1.813(1.772)	Prec@1: 49.219(49.939)	
06-15-23 18:52:Test: [78/79]	Time: 0.0153(0.0256)	Loss: 0.742(1.752)	Prec@1: 68.750(50.310)	
06-15-23 18:52:Step 57 * Prec@1 50.310
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [0/391]	Time 0.418 (0.418)	Data 0.378 (0.378)	Loss 0.466 (0.466)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [50/391]	Time 0.075 (0.068)	Data 0.003 (0.010)	Loss 0.610 (0.605)	Prec@1 78.125 (79.197)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [100/391]	Time 0.066 (0.069)	Data 0.002 (0.006)	Loss 0.623 (0.601)	Prec@1 75.781 (79.626)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [150/391]	Time 0.066 (0.069)	Data 0.003 (0.005)	Loss 0.428 (0.594)	Prec@1 85.156 (79.625)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [200/391]	Time 0.076 (0.068)	Data 0.003 (0.004)	Loss 0.538 (0.591)	Prec@1 80.469 (79.765)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [250/391]	Time 0.074 (0.069)	Data 0.003 (0.004)	Loss 0.725 (0.596)	Prec@1 76.562 (79.616)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [300/391]	Time 0.075 (0.069)	Data 0.002 (0.004)	Loss 0.819 (0.599)	Prec@1 76.562 (79.597)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:52:Num bit 8	Num grad bit 8	
06-15-23 18:52:Iter: [350/391]	Time 0.072 (0.069)	Data 0.002 (0.004)	Loss 0.538 (0.597)	Prec@1 79.688 (79.641)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Test: [0/79]	Time: 0.3898(0.3898)	Loss: 1.382(1.382)	Prec@1: 58.594(58.594)	
06-15-23 18:53:Test: [50/79]	Time: 0.0168(0.0240)	Loss: 1.593(1.708)	Prec@1: 53.125(54.228)	
06-15-23 18:53:Test: [78/79]	Time: 0.0151(0.0214)	Loss: 1.637(1.708)	Prec@1: 62.500(54.580)	
06-15-23 18:53:Step 58 * Prec@1 54.580
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [0/391]	Time 0.430 (0.430)	Data 0.386 (0.386)	Loss 0.489 (0.489)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [50/391]	Time 0.069 (0.074)	Data 0.002 (0.010)	Loss 0.604 (0.613)	Prec@1 82.031 (79.228)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [100/391]	Time 0.068 (0.072)	Data 0.003 (0.006)	Loss 0.589 (0.594)	Prec@1 78.906 (79.757)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [150/391]	Time 0.058 (0.065)	Data 0.002 (0.005)	Loss 0.819 (0.599)	Prec@1 71.094 (79.636)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [200/391]	Time 0.069 (0.066)	Data 0.003 (0.004)	Loss 0.656 (0.595)	Prec@1 75.000 (79.726)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [250/391]	Time 0.075 (0.067)	Data 0.003 (0.004)	Loss 0.489 (0.591)	Prec@1 82.812 (79.980)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [300/391]	Time 0.070 (0.067)	Data 0.003 (0.004)	Loss 0.592 (0.596)	Prec@1 82.812 (79.939)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [350/391]	Time 0.072 (0.067)	Data 0.002 (0.004)	Loss 0.653 (0.598)	Prec@1 81.250 (79.897)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Test: [0/79]	Time: 0.3786(0.3786)	Loss: 1.867(1.867)	Prec@1: 53.906(53.906)	
06-15-23 18:53:Test: [50/79]	Time: 0.0164(0.0236)	Loss: 2.143(2.140)	Prec@1: 45.312(44.393)	
06-15-23 18:53:Test: [78/79]	Time: 0.0150(0.0211)	Loss: 2.389(2.136)	Prec@1: 31.250(44.440)	
06-15-23 18:53:Step 59 * Prec@1 44.440
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [0/391]	Time 0.402 (0.402)	Data 0.357 (0.357)	Loss 0.418 (0.418)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [50/391]	Time 0.074 (0.061)	Data 0.003 (0.009)	Loss 0.560 (0.621)	Prec@1 82.812 (78.922)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [100/391]	Time 0.073 (0.066)	Data 0.003 (0.006)	Loss 0.489 (0.614)	Prec@1 84.375 (79.239)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [150/391]	Time 0.069 (0.067)	Data 0.002 (0.005)	Loss 0.665 (0.616)	Prec@1 78.906 (79.212)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [200/391]	Time 0.063 (0.067)	Data 0.002 (0.004)	Loss 0.570 (0.612)	Prec@1 82.031 (79.248)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [250/391]	Time 0.066 (0.068)	Data 0.003 (0.004)	Loss 0.654 (0.620)	Prec@1 73.438 (79.000)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [300/391]	Time 0.066 (0.068)	Data 0.003 (0.004)	Loss 0.546 (0.615)	Prec@1 80.469 (79.179)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Num bit 8	Num grad bit 8	
06-15-23 18:53:Iter: [350/391]	Time 0.073 (0.069)	Data 0.003 (0.004)	Loss 0.569 (0.617)	Prec@1 82.031 (79.167)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:53:Test: [0/79]	Time: 0.4043(0.4043)	Loss: 2.216(2.216)	Prec@1: 57.812(57.812)	
06-15-23 18:54:Test: [50/79]	Time: 0.0166(0.0242)	Loss: 3.012(2.919)	Prec@1: 48.438(43.581)	
06-15-23 18:54:Test: [78/79]	Time: 0.0151(0.0215)	Loss: 2.425(2.858)	Prec@1: 37.500(43.900)	
06-15-23 18:54:Step 60 * Prec@1 43.900
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [0/391]	Time 0.398 (0.398)	Data 0.361 (0.361)	Loss 0.703 (0.703)	Prec@1 78.906 (78.906)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [50/391]	Time 0.070 (0.058)	Data 0.003 (0.009)	Loss 0.777 (0.584)	Prec@1 71.094 (79.979)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [100/391]	Time 0.067 (0.062)	Data 0.002 (0.006)	Loss 0.477 (0.585)	Prec@1 89.062 (80.043)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [150/391]	Time 0.064 (0.065)	Data 0.002 (0.005)	Loss 0.512 (0.593)	Prec@1 81.250 (79.962)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [200/391]	Time 0.072 (0.062)	Data 0.002 (0.004)	Loss 0.432 (0.610)	Prec@1 85.156 (79.447)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [250/391]	Time 0.072 (0.064)	Data 0.003 (0.004)	Loss 0.662 (0.610)	Prec@1 78.125 (79.532)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [300/391]	Time 0.072 (0.066)	Data 0.003 (0.004)	Loss 0.688 (0.610)	Prec@1 76.562 (79.568)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [350/391]	Time 0.040 (0.065)	Data 0.002 (0.004)	Loss 0.379 (0.605)	Prec@1 90.625 (79.699)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Test: [0/79]	Time: 0.3703(0.3703)	Loss: 2.860(2.860)	Prec@1: 45.312(45.312)	
06-15-23 18:54:Test: [50/79]	Time: 0.0168(0.0236)	Loss: 2.901(2.833)	Prec@1: 42.969(44.562)	
06-15-23 18:54:Test: [78/79]	Time: 0.0153(0.0212)	Loss: 1.094(2.809)	Prec@1: 56.250(45.150)	
06-15-23 18:54:Step 61 * Prec@1 45.150
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [0/391]	Time 0.421 (0.421)	Data 0.384 (0.384)	Loss 0.505 (0.505)	Prec@1 81.250 (81.250)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [50/391]	Time 0.074 (0.067)	Data 0.003 (0.010)	Loss 0.611 (0.588)	Prec@1 80.469 (80.147)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [100/391]	Time 0.064 (0.068)	Data 0.002 (0.006)	Loss 0.842 (0.591)	Prec@1 71.875 (79.927)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [150/391]	Time 0.072 (0.069)	Data 0.006 (0.005)	Loss 0.532 (0.593)	Prec@1 82.031 (79.672)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [200/391]	Time 0.070 (0.069)	Data 0.003 (0.004)	Loss 0.580 (0.591)	Prec@1 77.344 (79.765)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [250/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.573 (0.590)	Prec@1 83.594 (79.890)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [300/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.584 (0.592)	Prec@1 82.031 (79.882)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [350/391]	Time 0.067 (0.069)	Data 0.002 (0.004)	Loss 0.530 (0.588)	Prec@1 81.250 (79.995)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:54:Test: [0/79]	Time: 0.3609(0.3609)	Loss: 0.925(0.925)	Prec@1: 69.531(69.531)	
06-15-23 18:54:Test: [50/79]	Time: 0.0169(0.0233)	Loss: 1.405(1.430)	Prec@1: 65.625(63.097)	
06-15-23 18:54:Test: [78/79]	Time: 0.0151(0.0209)	Loss: 0.933(1.439)	Prec@1: 81.250(62.870)	
06-15-23 18:54:Step 62 * Prec@1 62.870
06-15-23 18:54:Num bit 8	Num grad bit 8	
06-15-23 18:54:Iter: [0/391]	Time 0.387 (0.387)	Data 0.350 (0.350)	Loss 0.604 (0.604)	Prec@1 82.812 (82.812)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [50/391]	Time 0.072 (0.073)	Data 0.002 (0.009)	Loss 0.975 (0.632)	Prec@1 67.188 (78.922)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [100/391]	Time 0.073 (0.072)	Data 0.003 (0.006)	Loss 0.490 (0.599)	Prec@1 80.469 (79.904)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [150/391]	Time 0.074 (0.071)	Data 0.003 (0.005)	Loss 0.759 (0.607)	Prec@1 79.688 (79.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [200/391]	Time 0.070 (0.071)	Data 0.003 (0.004)	Loss 0.475 (0.599)	Prec@1 82.031 (79.754)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [250/391]	Time 0.073 (0.071)	Data 0.003 (0.004)	Loss 0.707 (0.594)	Prec@1 75.781 (79.921)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [300/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.628 (0.591)	Prec@1 80.469 (80.147)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [350/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.593 (0.590)	Prec@1 76.562 (80.122)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Test: [0/79]	Time: 0.3863(0.3863)	Loss: 3.537(3.537)	Prec@1: 32.031(32.031)	
06-15-23 18:55:Test: [50/79]	Time: 0.0165(0.0275)	Loss: 3.717(3.650)	Prec@1: 25.000(28.569)	
06-15-23 18:55:Test: [78/79]	Time: 0.0149(0.0236)	Loss: 3.135(3.672)	Prec@1: 25.000(28.070)	
06-15-23 18:55:Step 63 * Prec@1 28.070
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [0/391]	Time 0.390 (0.390)	Data 0.353 (0.353)	Loss 0.577 (0.577)	Prec@1 82.031 (82.031)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [50/391]	Time 0.070 (0.075)	Data 0.002 (0.009)	Loss 0.502 (0.565)	Prec@1 84.375 (81.005)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [100/391]	Time 0.072 (0.074)	Data 0.003 (0.006)	Loss 0.694 (0.585)	Prec@1 73.438 (80.152)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [150/391]	Time 0.068 (0.071)	Data 0.003 (0.005)	Loss 0.434 (0.580)	Prec@1 83.594 (80.308)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [200/391]	Time 0.071 (0.069)	Data 0.003 (0.004)	Loss 0.469 (0.578)	Prec@1 84.375 (80.325)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [250/391]	Time 0.049 (0.068)	Data 0.002 (0.004)	Loss 0.654 (0.573)	Prec@1 79.688 (80.472)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [300/391]	Time 0.073 (0.067)	Data 0.003 (0.004)	Loss 0.548 (0.580)	Prec@1 82.031 (80.373)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [350/391]	Time 0.064 (0.067)	Data 0.002 (0.004)	Loss 1.006 (0.586)	Prec@1 70.312 (80.168)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Test: [0/79]	Time: 0.3862(0.3862)	Loss: 2.709(2.709)	Prec@1: 31.250(31.250)	
06-15-23 18:55:Test: [50/79]	Time: 0.0200(0.0280)	Loss: 3.011(2.760)	Prec@1: 32.031(31.464)	
06-15-23 18:55:Test: [78/79]	Time: 0.0183(0.0250)	Loss: 2.835(2.750)	Prec@1: 12.500(31.680)	
06-15-23 18:55:Step 64 * Prec@1 31.680
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [0/391]	Time 0.393 (0.393)	Data 0.357 (0.357)	Loss 0.729 (0.729)	Prec@1 77.344 (77.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:55:Num bit 8	Num grad bit 8	
06-15-23 18:55:Iter: [50/391]	Time 0.079 (0.074)	Data 0.003 (0.010)	Loss 0.572 (0.596)	Prec@1 82.812 (79.994)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [100/391]	Time 0.075 (0.073)	Data 0.003 (0.006)	Loss 0.944 (0.606)	Prec@1 70.312 (79.842)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [150/391]	Time 0.069 (0.072)	Data 0.002 (0.005)	Loss 0.626 (0.599)	Prec@1 78.906 (79.869)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [200/391]	Time 0.070 (0.072)	Data 0.003 (0.004)	Loss 0.527 (0.598)	Prec@1 78.125 (79.827)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [250/391]	Time 0.076 (0.072)	Data 0.003 (0.004)	Loss 0.750 (0.599)	Prec@1 77.344 (79.852)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [300/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.764 (0.599)	Prec@1 79.688 (79.848)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [350/391]	Time 0.070 (0.071)	Data 0.003 (0.004)	Loss 0.657 (0.596)	Prec@1 78.906 (79.888)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Test: [0/79]	Time: 0.3772(0.3772)	Loss: 2.391(2.391)	Prec@1: 48.438(48.438)	
06-15-23 18:56:Test: [50/79]	Time: 0.0622(0.0245)	Loss: 2.970(2.699)	Prec@1: 32.812(41.023)	
06-15-23 18:56:Test: [78/79]	Time: 0.0149(0.0216)	Loss: 2.311(2.676)	Prec@1: 50.000(41.380)	
06-15-23 18:56:Step 65 * Prec@1 41.380
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [0/391]	Time 0.397 (0.397)	Data 0.352 (0.352)	Loss 0.810 (0.810)	Prec@1 74.219 (74.219)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [50/391]	Time 0.037 (0.042)	Data 0.002 (0.008)	Loss 0.612 (0.562)	Prec@1 80.469 (80.699)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [100/391]	Time 0.074 (0.049)	Data 0.003 (0.005)	Loss 0.489 (0.581)	Prec@1 85.938 (80.260)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [150/391]	Time 0.074 (0.057)	Data 0.003 (0.005)	Loss 0.589 (0.578)	Prec@1 81.250 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [200/391]	Time 0.058 (0.060)	Data 0.002 (0.004)	Loss 0.426 (0.578)	Prec@1 82.812 (80.379)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [250/391]	Time 0.045 (0.058)	Data 0.002 (0.004)	Loss 0.832 (0.576)	Prec@1 72.656 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [300/391]	Time 0.048 (0.056)	Data 0.002 (0.003)	Loss 0.801 (0.580)	Prec@1 76.562 (80.313)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [350/391]	Time 0.052 (0.055)	Data 0.002 (0.003)	Loss 0.592 (0.586)	Prec@1 79.688 (80.146)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Test: [0/79]	Time: 0.3837(0.3837)	Loss: 3.145(3.145)	Prec@1: 40.625(40.625)	
06-15-23 18:56:Test: [50/79]	Time: 0.0164(0.0236)	Loss: 4.107(3.709)	Prec@1: 31.250(38.695)	
06-15-23 18:56:Test: [78/79]	Time: 0.0151(0.0210)	Loss: 3.717(3.703)	Prec@1: 43.750(38.180)	
06-15-23 18:56:Step 66 * Prec@1 38.180
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [0/391]	Time 0.410 (0.410)	Data 0.362 (0.362)	Loss 0.767 (0.767)	Prec@1 75.781 (75.781)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [50/391]	Time 0.066 (0.050)	Data 0.002 (0.009)	Loss 0.511 (0.585)	Prec@1 82.031 (80.545)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [100/391]	Time 0.069 (0.059)	Data 0.003 (0.006)	Loss 0.347 (0.564)	Prec@1 88.281 (81.103)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:56:Num bit 8	Num grad bit 8	
06-15-23 18:56:Iter: [150/391]	Time 0.072 (0.063)	Data 0.003 (0.005)	Loss 0.712 (0.579)	Prec@1 78.906 (80.572)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [200/391]	Time 0.072 (0.065)	Data 0.003 (0.004)	Loss 0.535 (0.588)	Prec@1 80.469 (80.360)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [250/391]	Time 0.069 (0.066)	Data 0.002 (0.004)	Loss 0.569 (0.593)	Prec@1 78.906 (80.179)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [300/391]	Time 0.068 (0.066)	Data 0.003 (0.004)	Loss 0.406 (0.594)	Prec@1 87.500 (80.150)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [350/391]	Time 0.060 (0.067)	Data 0.002 (0.004)	Loss 0.689 (0.596)	Prec@1 77.344 (80.086)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Test: [0/79]	Time: 0.3739(0.3739)	Loss: 2.432(2.432)	Prec@1: 40.625(40.625)	
06-15-23 18:57:Test: [50/79]	Time: 0.0161(0.0233)	Loss: 2.385(2.588)	Prec@1: 42.188(37.255)	
06-15-23 18:57:Test: [78/79]	Time: 0.0148(0.0209)	Loss: 2.355(2.612)	Prec@1: 25.000(37.080)	
06-15-23 18:57:Step 67 * Prec@1 37.080
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [0/391]	Time 0.396 (0.396)	Data 0.359 (0.359)	Loss 0.756 (0.756)	Prec@1 74.219 (74.219)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [50/391]	Time 0.075 (0.062)	Data 0.006 (0.009)	Loss 0.764 (0.614)	Prec@1 75.000 (78.952)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [100/391]	Time 0.072 (0.066)	Data 0.003 (0.006)	Loss 0.519 (0.589)	Prec@1 82.812 (80.051)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [150/391]	Time 0.066 (0.067)	Data 0.003 (0.005)	Loss 0.453 (0.595)	Prec@1 85.938 (79.972)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [200/391]	Time 0.073 (0.068)	Data 0.003 (0.004)	Loss 0.509 (0.607)	Prec@1 79.688 (79.575)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [250/391]	Time 0.065 (0.069)	Data 0.003 (0.004)	Loss 0.635 (0.612)	Prec@1 82.031 (79.613)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [300/391]	Time 0.071 (0.069)	Data 0.003 (0.004)	Loss 1.064 (0.608)	Prec@1 67.188 (79.633)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [350/391]	Time 0.069 (0.069)	Data 0.003 (0.004)	Loss 0.482 (0.606)	Prec@1 85.938 (79.739)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Test: [0/79]	Time: 0.3618(0.3618)	Loss: 0.932(0.932)	Prec@1: 70.312(70.312)	
06-15-23 18:57:Test: [50/79]	Time: 0.0164(0.0232)	Loss: 1.100(1.113)	Prec@1: 63.281(63.036)	
06-15-23 18:57:Test: [78/79]	Time: 0.0153(0.0209)	Loss: 0.841(1.111)	Prec@1: 62.500(62.710)	
06-15-23 18:57:Step 68 * Prec@1 62.710
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [0/391]	Time 0.419 (0.419)	Data 0.379 (0.379)	Loss 0.529 (0.529)	Prec@1 82.812 (82.812)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [50/391]	Time 0.073 (0.069)	Data 0.003 (0.010)	Loss 0.714 (0.590)	Prec@1 81.250 (80.239)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [100/391]	Time 0.073 (0.071)	Data 0.003 (0.006)	Loss 0.374 (0.578)	Prec@1 87.500 (80.337)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [150/391]	Time 0.073 (0.071)	Data 0.003 (0.005)	Loss 0.412 (0.581)	Prec@1 88.281 (80.246)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:57:Num bit 8	Num grad bit 8	
06-15-23 18:57:Iter: [200/391]	Time 0.073 (0.071)	Data 0.002 (0.005)	Loss 0.524 (0.578)	Prec@1 80.469 (80.158)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [250/391]	Time 0.071 (0.071)	Data 0.003 (0.004)	Loss 0.658 (0.583)	Prec@1 78.906 (80.005)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [300/391]	Time 0.075 (0.072)	Data 0.003 (0.004)	Loss 0.445 (0.581)	Prec@1 87.500 (80.087)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [350/391]	Time 0.074 (0.072)	Data 0.003 (0.004)	Loss 0.680 (0.589)	Prec@1 78.906 (79.834)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Test: [0/79]	Time: 0.3902(0.3902)	Loss: 5.133(5.133)	Prec@1: 41.406(41.406)	
06-15-23 18:58:Test: [50/79]	Time: 0.0167(0.0247)	Loss: 5.577(5.195)	Prec@1: 35.938(38.879)	
06-15-23 18:58:Test: [78/79]	Time: 0.0150(0.0218)	Loss: 3.072(5.166)	Prec@1: 56.250(39.010)	
06-15-23 18:58:Step 69 * Prec@1 39.010
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [0/391]	Time 0.384 (0.384)	Data 0.346 (0.346)	Loss 1.110 (1.110)	Prec@1 67.969 (67.969)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [50/391]	Time 0.072 (0.066)	Data 0.004 (0.009)	Loss 0.473 (0.619)	Prec@1 80.469 (79.519)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [100/391]	Time 0.071 (0.068)	Data 0.003 (0.006)	Loss 0.569 (0.609)	Prec@1 81.250 (79.958)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [150/391]	Time 0.077 (0.069)	Data 0.003 (0.005)	Loss 0.539 (0.604)	Prec@1 77.344 (80.008)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [200/391]	Time 0.070 (0.067)	Data 0.003 (0.004)	Loss 0.635 (0.593)	Prec@1 78.125 (80.166)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [250/391]	Time 0.071 (0.067)	Data 0.003 (0.004)	Loss 0.510 (0.594)	Prec@1 81.250 (80.117)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [300/391]	Time 0.072 (0.068)	Data 0.003 (0.004)	Loss 0.715 (0.590)	Prec@1 76.562 (80.183)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [350/391]	Time 0.072 (0.068)	Data 0.003 (0.004)	Loss 0.714 (0.588)	Prec@1 75.781 (80.188)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Test: [0/79]	Time: 0.3612(0.3612)	Loss: 2.304(2.304)	Prec@1: 28.906(28.906)	
06-15-23 18:58:Test: [50/79]	Time: 0.0166(0.0253)	Loss: 2.553(2.333)	Prec@1: 29.688(30.101)	
06-15-23 18:58:Test: [78/79]	Time: 0.0150(0.0221)	Loss: 2.995(2.341)	Prec@1: 12.500(30.030)	
06-15-23 18:58:Step 70 * Prec@1 30.030
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [0/391]	Time 0.405 (0.405)	Data 0.348 (0.348)	Loss 0.558 (0.558)	Prec@1 77.344 (77.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [50/391]	Time 0.068 (0.072)	Data 0.002 (0.009)	Loss 0.519 (0.569)	Prec@1 80.469 (80.270)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [100/391]	Time 0.070 (0.070)	Data 0.003 (0.006)	Loss 0.827 (0.581)	Prec@1 69.531 (80.020)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [150/391]	Time 0.073 (0.070)	Data 0.003 (0.005)	Loss 0.529 (0.574)	Prec@1 85.156 (80.319)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [200/391]	Time 0.071 (0.070)	Data 0.002 (0.004)	Loss 0.567 (0.573)	Prec@1 79.688 (80.488)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:58:Num bit 8	Num grad bit 8	
06-15-23 18:58:Iter: [250/391]	Time 0.041 (0.065)	Data 0.001 (0.004)	Loss 0.813 (0.580)	Prec@1 71.875 (80.453)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [300/391]	Time 0.076 (0.064)	Data 0.003 (0.004)	Loss 0.390 (0.583)	Prec@1 89.062 (80.419)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [350/391]	Time 0.069 (0.065)	Data 0.003 (0.003)	Loss 0.659 (0.585)	Prec@1 76.562 (80.382)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Test: [0/79]	Time: 0.3776(0.3776)	Loss: 3.341(3.341)	Prec@1: 39.844(39.844)	
06-15-23 18:59:Test: [50/79]	Time: 0.0165(0.0236)	Loss: 4.129(3.443)	Prec@1: 35.938(43.888)	
06-15-23 18:59:Test: [78/79]	Time: 0.0151(0.0210)	Loss: 4.200(3.478)	Prec@1: 43.750(43.290)	
06-15-23 18:59:Step 71 * Prec@1 43.290
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [0/391]	Time 0.400 (0.400)	Data 0.355 (0.355)	Loss 0.593 (0.593)	Prec@1 81.250 (81.250)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [50/391]	Time 0.042 (0.050)	Data 0.002 (0.009)	Loss 0.653 (0.606)	Prec@1 81.250 (80.116)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [100/391]	Time 0.041 (0.050)	Data 0.002 (0.006)	Loss 0.598 (0.602)	Prec@1 80.469 (79.804)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [150/391]	Time 0.060 (0.049)	Data 0.002 (0.004)	Loss 0.594 (0.589)	Prec@1 84.375 (80.257)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [200/391]	Time 0.068 (0.051)	Data 0.003 (0.004)	Loss 0.732 (0.593)	Prec@1 76.562 (80.014)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [250/391]	Time 0.075 (0.055)	Data 0.003 (0.004)	Loss 0.397 (0.586)	Prec@1 85.938 (80.195)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [300/391]	Time 0.060 (0.058)	Data 0.003 (0.003)	Loss 0.388 (0.589)	Prec@1 85.156 (80.082)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [350/391]	Time 0.064 (0.058)	Data 0.003 (0.003)	Loss 0.633 (0.586)	Prec@1 81.250 (80.170)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Test: [0/79]	Time: 0.4514(0.4514)	Loss: 1.745(1.745)	Prec@1: 43.750(43.750)	
06-15-23 18:59:Test: [50/79]	Time: 0.0167(0.0305)	Loss: 2.100(1.844)	Prec@1: 43.750(49.740)	
06-15-23 18:59:Test: [78/79]	Time: 0.0150(0.0269)	Loss: 1.668(1.859)	Prec@1: 43.750(49.510)	
06-15-23 18:59:Step 72 * Prec@1 49.510
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [0/391]	Time 0.399 (0.399)	Data 0.355 (0.355)	Loss 0.358 (0.358)	Prec@1 88.281 (88.281)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [50/391]	Time 0.071 (0.075)	Data 0.002 (0.009)	Loss 0.485 (0.577)	Prec@1 84.375 (80.346)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [100/391]	Time 0.054 (0.068)	Data 0.003 (0.006)	Loss 0.491 (0.607)	Prec@1 82.031 (79.502)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [150/391]	Time 0.055 (0.064)	Data 0.002 (0.005)	Loss 0.487 (0.601)	Prec@1 83.594 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [200/391]	Time 0.053 (0.060)	Data 0.002 (0.004)	Loss 0.614 (0.593)	Prec@1 76.562 (80.080)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [250/391]	Time 0.046 (0.059)	Data 0.002 (0.004)	Loss 0.472 (0.590)	Prec@1 82.812 (80.179)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [300/391]	Time 0.053 (0.057)	Data 0.002 (0.003)	Loss 0.561 (0.592)	Prec@1 83.594 (80.121)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Num bit 8	Num grad bit 8	
06-15-23 18:59:Iter: [350/391]	Time 0.064 (0.057)	Data 0.003 (0.003)	Loss 0.577 (0.590)	Prec@1 82.812 (80.186)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 18:59:Test: [0/79]	Time: 0.3718(0.3718)	Loss: 1.450(1.450)	Prec@1: 50.781(50.781)	
06-15-23 18:59:Test: [50/79]	Time: 0.0200(0.0266)	Loss: 1.596(1.646)	Prec@1: 54.688(46.523)	
06-15-23 19:00:Test: [78/79]	Time: 0.0183(0.0242)	Loss: 0.982(1.657)	Prec@1: 62.500(46.200)	
06-15-23 19:00:Step 73 * Prec@1 46.200
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [0/391]	Time 0.396 (0.396)	Data 0.347 (0.347)	Loss 0.795 (0.795)	Prec@1 74.219 (74.219)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [50/391]	Time 0.074 (0.070)	Data 0.003 (0.009)	Loss 0.611 (0.575)	Prec@1 78.906 (80.591)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [100/391]	Time 0.064 (0.071)	Data 0.002 (0.006)	Loss 0.571 (0.584)	Prec@1 82.031 (80.260)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [150/391]	Time 0.074 (0.072)	Data 0.003 (0.005)	Loss 0.470 (0.594)	Prec@1 82.031 (79.827)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [200/391]	Time 0.077 (0.072)	Data 0.003 (0.004)	Loss 0.681 (0.589)	Prec@1 78.125 (80.037)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [250/391]	Time 0.072 (0.072)	Data 0.003 (0.004)	Loss 0.522 (0.589)	Prec@1 84.375 (79.927)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [300/391]	Time 0.070 (0.072)	Data 0.003 (0.004)	Loss 0.466 (0.589)	Prec@1 81.250 (79.957)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [350/391]	Time 0.075 (0.072)	Data 0.004 (0.004)	Loss 0.728 (0.588)	Prec@1 76.562 (79.919)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Test: [0/79]	Time: 0.3800(0.3800)	Loss: 4.578(4.578)	Prec@1: 25.781(25.781)	
06-15-23 19:00:Test: [50/79]	Time: 0.0164(0.0235)	Loss: 5.174(5.209)	Prec@1: 28.906(27.543)	
06-15-23 19:00:Test: [78/79]	Time: 0.0149(0.0210)	Loss: 5.230(5.181)	Prec@1: 18.750(27.680)	
06-15-23 19:00:Step 74 * Prec@1 27.680
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [0/391]	Time 0.397 (0.397)	Data 0.361 (0.361)	Loss 0.663 (0.663)	Prec@1 81.250 (81.250)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [50/391]	Time 0.069 (0.070)	Data 0.003 (0.009)	Loss 0.636 (0.571)	Prec@1 78.906 (81.112)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [100/391]	Time 0.071 (0.070)	Data 0.003 (0.006)	Loss 0.730 (0.580)	Prec@1 76.562 (80.693)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [150/391]	Time 0.064 (0.070)	Data 0.002 (0.005)	Loss 0.818 (0.586)	Prec@1 73.438 (80.339)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [200/391]	Time 0.072 (0.071)	Data 0.002 (0.004)	Loss 0.600 (0.590)	Prec@1 81.250 (80.185)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [250/391]	Time 0.073 (0.071)	Data 0.003 (0.004)	Loss 0.499 (0.591)	Prec@1 84.375 (80.089)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [300/391]	Time 0.071 (0.071)	Data 0.003 (0.004)	Loss 0.823 (0.588)	Prec@1 70.312 (80.246)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Num bit 8	Num grad bit 8	
06-15-23 19:00:Iter: [350/391]	Time 0.073 (0.071)	Data 0.003 (0.004)	Loss 0.661 (0.585)	Prec@1 78.125 (80.308)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:00:Test: [0/79]	Time: 0.3720(0.3720)	Loss: 2.898(2.898)	Prec@1: 39.844(39.844)	
06-15-23 19:00:Test: [50/79]	Time: 0.0164(0.0234)	Loss: 3.423(3.135)	Prec@1: 32.812(36.673)	
06-15-23 19:00:Test: [78/79]	Time: 0.0150(0.0210)	Loss: 1.376(3.128)	Prec@1: 62.500(36.900)	
06-15-23 19:01:Step 75 * Prec@1 36.900
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [0/391]	Time 0.385 (0.385)	Data 0.349 (0.349)	Loss 0.643 (0.643)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [50/391]	Time 0.054 (0.044)	Data 0.003 (0.008)	Loss 0.667 (0.592)	Prec@1 78.906 (80.806)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [100/391]	Time 0.075 (0.056)	Data 0.003 (0.006)	Loss 0.566 (0.581)	Prec@1 81.250 (80.515)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [150/391]	Time 0.040 (0.051)	Data 0.001 (0.004)	Loss 0.858 (0.584)	Prec@1 71.875 (80.329)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [200/391]	Time 0.069 (0.055)	Data 0.002 (0.004)	Loss 0.629 (0.586)	Prec@1 79.688 (80.228)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [250/391]	Time 0.068 (0.058)	Data 0.002 (0.004)	Loss 0.747 (0.583)	Prec@1 71.094 (80.301)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [300/391]	Time 0.076 (0.059)	Data 0.003 (0.003)	Loss 0.631 (0.588)	Prec@1 75.781 (80.175)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [350/391]	Time 0.072 (0.061)	Data 0.003 (0.003)	Loss 0.483 (0.594)	Prec@1 82.812 (79.990)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Test: [0/79]	Time: 0.4001(0.4001)	Loss: 1.501(1.501)	Prec@1: 64.062(64.062)	
06-15-23 19:01:Test: [50/79]	Time: 0.0163(0.0240)	Loss: 1.741(1.710)	Prec@1: 57.031(58.088)	
06-15-23 19:01:Test: [78/79]	Time: 0.0149(0.0214)	Loss: 1.762(1.731)	Prec@1: 50.000(57.650)	
06-15-23 19:01:Step 76 * Prec@1 57.650
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [0/391]	Time 0.442 (0.442)	Data 0.397 (0.397)	Loss 0.756 (0.756)	Prec@1 74.219 (74.219)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [50/391]	Time 0.071 (0.068)	Data 0.003 (0.010)	Loss 0.575 (0.585)	Prec@1 80.469 (80.591)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [100/391]	Time 0.066 (0.069)	Data 0.003 (0.006)	Loss 0.607 (0.565)	Prec@1 79.688 (80.747)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [150/391]	Time 0.070 (0.070)	Data 0.003 (0.005)	Loss 0.637 (0.571)	Prec@1 78.125 (80.738)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [200/391]	Time 0.064 (0.069)	Data 0.002 (0.004)	Loss 0.432 (0.571)	Prec@1 82.812 (80.721)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [250/391]	Time 0.073 (0.069)	Data 0.003 (0.004)	Loss 0.507 (0.573)	Prec@1 81.250 (80.699)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [300/391]	Time 0.077 (0.069)	Data 0.003 (0.004)	Loss 0.640 (0.576)	Prec@1 81.250 (80.534)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [350/391]	Time 0.059 (0.069)	Data 0.002 (0.004)	Loss 0.688 (0.584)	Prec@1 77.344 (80.306)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Test: [0/79]	Time: 0.3657(0.3657)	Loss: 2.747(2.747)	Prec@1: 42.188(42.188)	
06-15-23 19:01:Test: [50/79]	Time: 0.0163(0.0234)	Loss: 3.250(3.144)	Prec@1: 35.938(45.695)	
06-15-23 19:01:Test: [78/79]	Time: 0.0151(0.0209)	Loss: 2.102(3.093)	Prec@1: 50.000(46.450)	
06-15-23 19:01:Step 77 * Prec@1 46.450
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [0/391]	Time 0.391 (0.391)	Data 0.352 (0.352)	Loss 0.616 (0.616)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:01:Num bit 8	Num grad bit 8	
06-15-23 19:01:Iter: [50/391]	Time 0.042 (0.053)	Data 0.002 (0.009)	Loss 0.672 (0.591)	Prec@1 77.344 (80.254)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [100/391]	Time 0.058 (0.052)	Data 0.003 (0.005)	Loss 0.465 (0.583)	Prec@1 84.375 (80.616)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [150/391]	Time 0.051 (0.051)	Data 0.002 (0.004)	Loss 0.684 (0.582)	Prec@1 75.781 (80.401)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [200/391]	Time 0.074 (0.052)	Data 0.003 (0.004)	Loss 0.575 (0.584)	Prec@1 78.906 (80.329)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [250/391]	Time 0.073 (0.056)	Data 0.003 (0.004)	Loss 0.660 (0.584)	Prec@1 75.000 (80.382)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [300/391]	Time 0.072 (0.059)	Data 0.003 (0.003)	Loss 0.606 (0.586)	Prec@1 78.125 (80.352)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [350/391]	Time 0.046 (0.060)	Data 0.002 (0.003)	Loss 0.680 (0.585)	Prec@1 77.344 (80.302)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Test: [0/79]	Time: 0.3802(0.3802)	Loss: 2.301(2.301)	Prec@1: 47.656(47.656)	
06-15-23 19:02:Test: [50/79]	Time: 0.0166(0.0237)	Loss: 2.736(2.591)	Prec@1: 43.750(42.877)	
06-15-23 19:02:Test: [78/79]	Time: 0.0150(0.0212)	Loss: 2.429(2.579)	Prec@1: 50.000(43.280)	
06-15-23 19:02:Step 78 * Prec@1 43.280
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [0/391]	Time 0.439 (0.439)	Data 0.380 (0.380)	Loss 0.703 (0.703)	Prec@1 78.906 (78.906)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [50/391]	Time 0.073 (0.076)	Data 0.003 (0.010)	Loss 0.733 (0.617)	Prec@1 77.344 (79.519)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [100/391]	Time 0.070 (0.068)	Data 0.002 (0.006)	Loss 0.465 (0.596)	Prec@1 82.812 (79.834)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [150/391]	Time 0.074 (0.069)	Data 0.003 (0.005)	Loss 0.609 (0.590)	Prec@1 82.031 (80.117)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [200/391]	Time 0.071 (0.069)	Data 0.003 (0.004)	Loss 0.547 (0.590)	Prec@1 77.344 (79.991)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [250/391]	Time 0.070 (0.070)	Data 0.003 (0.004)	Loss 0.476 (0.586)	Prec@1 81.250 (80.201)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [300/391]	Time 0.055 (0.067)	Data 0.002 (0.004)	Loss 0.576 (0.583)	Prec@1 81.250 (80.347)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [350/391]	Time 0.050 (0.065)	Data 0.002 (0.003)	Loss 0.483 (0.585)	Prec@1 82.812 (80.268)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Test: [0/79]	Time: 0.3762(0.3762)	Loss: 3.615(3.615)	Prec@1: 46.875(46.875)	
06-15-23 19:02:Test: [50/79]	Time: 0.0165(0.0239)	Loss: 4.043(4.172)	Prec@1: 37.500(38.480)	
06-15-23 19:02:Test: [78/79]	Time: 0.0150(0.0212)	Loss: 2.745(4.083)	Prec@1: 50.000(38.990)	
06-15-23 19:02:Step 79 * Prec@1 38.990
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [0/391]	Time 0.433 (0.433)	Data 0.379 (0.379)	Loss 0.729 (0.729)	Prec@1 78.906 (78.906)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [50/391]	Time 0.069 (0.074)	Data 0.002 (0.010)	Loss 0.537 (0.561)	Prec@1 78.125 (81.495)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [100/391]	Time 0.075 (0.073)	Data 0.003 (0.006)	Loss 0.626 (0.586)	Prec@1 75.781 (80.353)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:02:Num bit 8	Num grad bit 8	
06-15-23 19:02:Iter: [150/391]	Time 0.074 (0.072)	Data 0.003 (0.005)	Loss 0.786 (0.586)	Prec@1 74.219 (80.210)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [200/391]	Time 0.063 (0.072)	Data 0.002 (0.004)	Loss 0.798 (0.587)	Prec@1 77.344 (80.325)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [250/391]	Time 0.068 (0.071)	Data 0.002 (0.004)	Loss 0.549 (0.591)	Prec@1 85.156 (80.195)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [300/391]	Time 0.074 (0.071)	Data 0.002 (0.004)	Loss 0.808 (0.597)	Prec@1 75.000 (80.012)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [350/391]	Time 0.077 (0.071)	Data 0.003 (0.003)	Loss 0.776 (0.593)	Prec@1 73.438 (80.110)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Test: [0/79]	Time: 0.4063(0.4063)	Loss: 4.237(4.237)	Prec@1: 33.594(33.594)	
06-15-23 19:03:Test: [50/79]	Time: 0.0165(0.0243)	Loss: 4.188(3.600)	Prec@1: 32.812(39.982)	
06-15-23 19:03:Test: [78/79]	Time: 0.0151(0.0215)	Loss: 3.335(3.623)	Prec@1: 50.000(39.730)	
06-15-23 19:03:Step 80 * Prec@1 39.730
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [0/391]	Time 0.451 (0.451)	Data 0.382 (0.382)	Loss 0.644 (0.644)	Prec@1 73.438 (73.438)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [50/391]	Time 0.064 (0.063)	Data 0.002 (0.010)	Loss 0.604 (0.575)	Prec@1 80.469 (80.423)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [100/391]	Time 0.066 (0.063)	Data 0.002 (0.006)	Loss 0.417 (0.564)	Prec@1 85.938 (81.026)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [150/391]	Time 0.063 (0.066)	Data 0.002 (0.005)	Loss 0.607 (0.562)	Prec@1 80.469 (81.090)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [200/391]	Time 0.071 (0.067)	Data 0.002 (0.004)	Loss 0.378 (0.560)	Prec@1 89.062 (81.017)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [250/391]	Time 0.063 (0.067)	Data 0.002 (0.004)	Loss 0.487 (0.570)	Prec@1 84.375 (80.777)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [300/391]	Time 0.077 (0.067)	Data 0.002 (0.004)	Loss 0.444 (0.571)	Prec@1 88.281 (80.791)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [350/391]	Time 0.077 (0.067)	Data 0.002 (0.003)	Loss 0.696 (0.573)	Prec@1 77.344 (80.743)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Test: [0/79]	Time: 0.3527(0.3527)	Loss: 3.773(3.773)	Prec@1: 22.656(22.656)	
06-15-23 19:03:Test: [50/79]	Time: 0.0166(0.0233)	Loss: 3.739(4.011)	Prec@1: 31.250(27.589)	
06-15-23 19:03:Test: [78/79]	Time: 0.0152(0.0209)	Loss: 2.497(4.024)	Prec@1: 37.500(27.350)	
06-15-23 19:03:Step 81 * Prec@1 27.350
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [0/391]	Time 0.417 (0.417)	Data 0.363 (0.363)	Loss 0.630 (0.630)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [50/391]	Time 0.074 (0.075)	Data 0.003 (0.010)	Loss 0.626 (0.577)	Prec@1 77.344 (80.055)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [100/391]	Time 0.039 (0.061)	Data 0.001 (0.006)	Loss 0.595 (0.567)	Prec@1 82.812 (80.577)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [150/391]	Time 0.075 (0.059)	Data 0.003 (0.005)	Loss 0.689 (0.569)	Prec@1 78.125 (80.795)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:03:Num bit 8	Num grad bit 8	
06-15-23 19:03:Iter: [200/391]	Time 0.069 (0.062)	Data 0.002 (0.004)	Loss 0.661 (0.574)	Prec@1 78.906 (80.512)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [250/391]	Time 0.079 (0.064)	Data 0.003 (0.004)	Loss 0.767 (0.583)	Prec@1 73.438 (80.232)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [300/391]	Time 0.071 (0.065)	Data 0.002 (0.004)	Loss 0.607 (0.586)	Prec@1 81.250 (80.191)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [350/391]	Time 0.071 (0.066)	Data 0.003 (0.004)	Loss 0.643 (0.585)	Prec@1 80.469 (80.188)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Test: [0/79]	Time: 0.3823(0.3823)	Loss: 1.694(1.694)	Prec@1: 53.125(53.125)	
06-15-23 19:04:Test: [50/79]	Time: 0.0166(0.0237)	Loss: 1.858(1.621)	Prec@1: 49.219(58.655)	
06-15-23 19:04:Test: [78/79]	Time: 0.0149(0.0217)	Loss: 2.465(1.639)	Prec@1: 50.000(58.370)	
06-15-23 19:04:Step 82 * Prec@1 58.370
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [0/391]	Time 0.385 (0.385)	Data 0.348 (0.348)	Loss 0.870 (0.870)	Prec@1 71.875 (71.875)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [50/391]	Time 0.070 (0.070)	Data 0.003 (0.009)	Loss 0.542 (0.619)	Prec@1 82.812 (79.473)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [100/391]	Time 0.060 (0.070)	Data 0.002 (0.006)	Loss 0.499 (0.608)	Prec@1 82.031 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [150/391]	Time 0.074 (0.070)	Data 0.003 (0.005)	Loss 0.579 (0.601)	Prec@1 78.906 (80.008)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [200/391]	Time 0.072 (0.070)	Data 0.003 (0.004)	Loss 0.705 (0.602)	Prec@1 75.000 (80.014)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [250/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.451 (0.597)	Prec@1 85.156 (80.114)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [300/391]	Time 0.072 (0.071)	Data 0.002 (0.004)	Loss 0.632 (0.593)	Prec@1 78.125 (80.277)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [350/391]	Time 0.069 (0.071)	Data 0.003 (0.004)	Loss 0.383 (0.591)	Prec@1 86.719 (80.233)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Test: [0/79]	Time: 0.3904(0.3904)	Loss: 5.975(5.975)	Prec@1: 25.781(25.781)	
06-15-23 19:04:Test: [50/79]	Time: 0.0169(0.0239)	Loss: 7.054(6.115)	Prec@1: 17.969(26.639)	
06-15-23 19:04:Test: [78/79]	Time: 0.0152(0.0214)	Loss: 5.078(6.185)	Prec@1: 50.000(26.330)	
06-15-23 19:04:Step 83 * Prec@1 26.330
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [0/391]	Time 0.394 (0.394)	Data 0.358 (0.358)	Loss 0.367 (0.367)	Prec@1 88.281 (88.281)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [50/391]	Time 0.069 (0.072)	Data 0.003 (0.009)	Loss 0.727 (0.586)	Prec@1 76.562 (81.112)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [100/391]	Time 0.063 (0.072)	Data 0.002 (0.006)	Loss 0.579 (0.574)	Prec@1 80.469 (80.925)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [150/391]	Time 0.065 (0.071)	Data 0.003 (0.005)	Loss 0.801 (0.582)	Prec@1 71.875 (80.541)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:04:Num bit 8	Num grad bit 8	
06-15-23 19:04:Iter: [200/391]	Time 0.077 (0.071)	Data 0.003 (0.004)	Loss 0.592 (0.585)	Prec@1 81.250 (80.317)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [250/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.447 (0.589)	Prec@1 82.812 (80.226)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [300/391]	Time 0.073 (0.071)	Data 0.003 (0.004)	Loss 0.634 (0.592)	Prec@1 82.812 (80.233)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [350/391]	Time 0.073 (0.072)	Data 0.003 (0.004)	Loss 0.587 (0.589)	Prec@1 80.469 (80.268)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Test: [0/79]	Time: 0.3719(0.3719)	Loss: 2.807(2.807)	Prec@1: 46.875(46.875)	
06-15-23 19:05:Test: [50/79]	Time: 0.0169(0.0235)	Loss: 2.892(2.582)	Prec@1: 46.094(47.227)	
06-15-23 19:05:Test: [78/79]	Time: 0.0150(0.0210)	Loss: 3.564(2.603)	Prec@1: 25.000(46.660)	
06-15-23 19:05:Step 84 * Prec@1 46.660
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [0/391]	Time 0.393 (0.393)	Data 0.354 (0.354)	Loss 0.409 (0.409)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [50/391]	Time 0.063 (0.072)	Data 0.002 (0.009)	Loss 0.385 (0.570)	Prec@1 87.500 (81.081)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [100/391]	Time 0.053 (0.067)	Data 0.002 (0.006)	Loss 0.467 (0.568)	Prec@1 83.594 (80.871)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [150/391]	Time 0.072 (0.066)	Data 0.003 (0.005)	Loss 0.675 (0.580)	Prec@1 76.562 (80.422)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [200/391]	Time 0.074 (0.067)	Data 0.003 (0.004)	Loss 0.491 (0.580)	Prec@1 82.031 (80.442)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [250/391]	Time 0.050 (0.066)	Data 0.002 (0.004)	Loss 0.449 (0.575)	Prec@1 83.594 (80.609)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [300/391]	Time 0.052 (0.063)	Data 0.002 (0.003)	Loss 0.553 (0.576)	Prec@1 82.812 (80.562)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [350/391]	Time 0.048 (0.061)	Data 0.002 (0.003)	Loss 0.560 (0.576)	Prec@1 79.688 (80.533)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Test: [0/79]	Time: 0.3439(0.3439)	Loss: 3.850(3.850)	Prec@1: 37.500(37.500)	
06-15-23 19:05:Test: [50/79]	Time: 0.0165(0.0227)	Loss: 4.558(4.298)	Prec@1: 35.938(37.760)	
06-15-23 19:05:Test: [78/79]	Time: 0.0149(0.0205)	Loss: 2.949(4.240)	Prec@1: 43.750(37.950)	
06-15-23 19:05:Step 85 * Prec@1 37.950
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [0/391]	Time 0.409 (0.409)	Data 0.349 (0.349)	Loss 0.594 (0.594)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [50/391]	Time 0.074 (0.068)	Data 0.003 (0.009)	Loss 0.577 (0.595)	Prec@1 82.812 (80.607)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [100/391]	Time 0.061 (0.068)	Data 0.002 (0.006)	Loss 0.515 (0.586)	Prec@1 81.250 (80.678)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [150/391]	Time 0.074 (0.069)	Data 0.003 (0.005)	Loss 0.634 (0.579)	Prec@1 78.125 (80.686)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [200/391]	Time 0.067 (0.069)	Data 0.002 (0.004)	Loss 0.521 (0.574)	Prec@1 82.031 (80.869)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [250/391]	Time 0.073 (0.069)	Data 0.003 (0.004)	Loss 0.415 (0.580)	Prec@1 85.156 (80.668)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:05:Num bit 8	Num grad bit 8	
06-15-23 19:05:Iter: [300/391]	Time 0.071 (0.069)	Data 0.002 (0.004)	Loss 0.460 (0.584)	Prec@1 84.375 (80.560)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [350/391]	Time 0.069 (0.069)	Data 0.002 (0.004)	Loss 0.490 (0.583)	Prec@1 80.469 (80.556)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Test: [0/79]	Time: 0.4141(0.4141)	Loss: 1.945(1.945)	Prec@1: 48.438(48.438)	
06-15-23 19:06:Test: [50/79]	Time: 0.0164(0.0244)	Loss: 2.220(2.200)	Prec@1: 42.969(45.772)	
06-15-23 19:06:Test: [78/79]	Time: 0.0150(0.0215)	Loss: 2.719(2.198)	Prec@1: 31.250(45.850)	
06-15-23 19:06:Step 86 * Prec@1 45.850
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [0/391]	Time 0.391 (0.391)	Data 0.352 (0.352)	Loss 0.646 (0.646)	Prec@1 76.562 (76.562)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [50/391]	Time 0.053 (0.049)	Data 0.002 (0.009)	Loss 0.543 (0.596)	Prec@1 78.906 (80.285)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [100/391]	Time 0.047 (0.050)	Data 0.003 (0.006)	Loss 0.439 (0.595)	Prec@1 83.594 (80.237)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [150/391]	Time 0.047 (0.049)	Data 0.002 (0.004)	Loss 0.491 (0.592)	Prec@1 82.812 (80.412)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [200/391]	Time 0.049 (0.049)	Data 0.002 (0.004)	Loss 0.924 (0.588)	Prec@1 71.094 (80.585)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [250/391]	Time 0.047 (0.049)	Data 0.002 (0.004)	Loss 0.569 (0.584)	Prec@1 82.031 (80.599)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [300/391]	Time 0.041 (0.048)	Data 0.002 (0.003)	Loss 0.649 (0.587)	Prec@1 78.125 (80.425)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [350/391]	Time 0.076 (0.049)	Data 0.002 (0.003)	Loss 0.475 (0.589)	Prec@1 84.375 (80.384)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Test: [0/79]	Time: 0.4222(0.4222)	Loss: 3.005(3.005)	Prec@1: 41.406(41.406)	
06-15-23 19:06:Test: [50/79]	Time: 0.0200(0.0278)	Loss: 3.898(3.667)	Prec@1: 27.344(30.362)	
06-15-23 19:06:Test: [78/79]	Time: 0.0155(0.0245)	Loss: 2.986(3.661)	Prec@1: 37.500(30.260)	
06-15-23 19:06:Step 87 * Prec@1 30.260
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [0/391]	Time 0.392 (0.392)	Data 0.344 (0.344)	Loss 0.707 (0.707)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [50/391]	Time 0.069 (0.070)	Data 0.002 (0.009)	Loss 0.746 (0.567)	Prec@1 74.219 (80.729)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [100/391]	Time 0.071 (0.070)	Data 0.003 (0.006)	Loss 0.479 (0.583)	Prec@1 82.031 (80.221)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [150/391]	Time 0.075 (0.068)	Data 0.003 (0.005)	Loss 0.609 (0.596)	Prec@1 84.375 (80.024)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [200/391]	Time 0.070 (0.069)	Data 0.003 (0.004)	Loss 0.570 (0.598)	Prec@1 79.688 (79.921)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [250/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.331 (0.594)	Prec@1 88.281 (80.061)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [300/391]	Time 0.058 (0.069)	Data 0.002 (0.004)	Loss 0.565 (0.597)	Prec@1 79.688 (80.105)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [350/391]	Time 0.073 (0.069)	Data 0.003 (0.004)	Loss 0.521 (0.592)	Prec@1 80.469 (80.228)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:06:Test: [0/79]	Time: 0.3683(0.3683)	Loss: 2.775(2.775)	Prec@1: 49.219(49.219)	
06-15-23 19:06:Test: [50/79]	Time: 0.0165(0.0234)	Loss: 2.801(2.655)	Prec@1: 51.562(51.409)	
06-15-23 19:06:Test: [78/79]	Time: 0.0151(0.0209)	Loss: 3.869(2.671)	Prec@1: 25.000(50.770)	
06-15-23 19:06:Step 88 * Prec@1 50.770
06-15-23 19:06:Num bit 8	Num grad bit 8	
06-15-23 19:06:Iter: [0/391]	Time 0.380 (0.380)	Data 0.343 (0.343)	Loss 0.392 (0.392)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [50/391]	Time 0.071 (0.070)	Data 0.003 (0.009)	Loss 0.455 (0.548)	Prec@1 79.688 (81.066)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [100/391]	Time 0.059 (0.070)	Data 0.002 (0.006)	Loss 0.762 (0.582)	Prec@1 75.000 (80.360)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [150/391]	Time 0.066 (0.067)	Data 0.002 (0.005)	Loss 0.634 (0.577)	Prec@1 77.344 (80.495)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [200/391]	Time 0.062 (0.067)	Data 0.002 (0.004)	Loss 0.425 (0.577)	Prec@1 87.500 (80.480)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [250/391]	Time 0.041 (0.062)	Data 0.002 (0.004)	Loss 0.482 (0.573)	Prec@1 82.812 (80.643)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [300/391]	Time 0.048 (0.059)	Data 0.002 (0.003)	Loss 0.855 (0.578)	Prec@1 74.219 (80.500)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [350/391]	Time 0.068 (0.058)	Data 0.003 (0.003)	Loss 0.537 (0.576)	Prec@1 83.594 (80.524)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Test: [0/79]	Time: 0.3755(0.3755)	Loss: 2.274(2.274)	Prec@1: 39.844(39.844)	
06-15-23 19:07:Test: [50/79]	Time: 0.0164(0.0235)	Loss: 2.435(2.636)	Prec@1: 37.500(32.230)	
06-15-23 19:07:Test: [78/79]	Time: 0.0149(0.0210)	Loss: 1.990(2.627)	Prec@1: 56.250(32.220)	
06-15-23 19:07:Step 89 * Prec@1 32.220
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [0/391]	Time 0.418 (0.418)	Data 0.382 (0.382)	Loss 0.551 (0.551)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [50/391]	Time 0.069 (0.073)	Data 0.003 (0.010)	Loss 0.611 (0.587)	Prec@1 77.344 (79.825)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [100/391]	Time 0.071 (0.072)	Data 0.003 (0.006)	Loss 0.641 (0.608)	Prec@1 80.469 (79.533)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [150/391]	Time 0.069 (0.071)	Data 0.003 (0.005)	Loss 0.489 (0.609)	Prec@1 82.812 (79.558)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [200/391]	Time 0.073 (0.072)	Data 0.003 (0.005)	Loss 0.516 (0.604)	Prec@1 83.594 (79.843)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [250/391]	Time 0.070 (0.072)	Data 0.003 (0.004)	Loss 0.590 (0.607)	Prec@1 79.688 (79.880)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [300/391]	Time 0.072 (0.072)	Data 0.003 (0.004)	Loss 0.560 (0.598)	Prec@1 82.031 (80.098)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [350/391]	Time 0.072 (0.072)	Data 0.003 (0.004)	Loss 0.525 (0.593)	Prec@1 80.469 (80.253)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Test: [0/79]	Time: 0.3806(0.3806)	Loss: 4.696(4.696)	Prec@1: 34.375(34.375)	
06-15-23 19:07:Test: [50/79]	Time: 0.0163(0.0235)	Loss: 5.429(5.157)	Prec@1: 27.344(28.140)	
06-15-23 19:07:Test: [78/79]	Time: 0.0150(0.0210)	Loss: 5.338(5.136)	Prec@1: 6.250(28.240)	
06-15-23 19:07:Step 90 * Prec@1 28.240
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [0/391]	Time 0.404 (0.404)	Data 0.366 (0.366)	Loss 0.693 (0.693)	Prec@1 78.906 (78.906)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [50/391]	Time 0.067 (0.071)	Data 0.002 (0.010)	Loss 0.596 (0.570)	Prec@1 75.000 (80.775)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:07:Num bit 8	Num grad bit 8	
06-15-23 19:07:Iter: [100/391]	Time 0.071 (0.071)	Data 0.003 (0.006)	Loss 0.430 (0.578)	Prec@1 85.156 (80.507)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [150/391]	Time 0.074 (0.071)	Data 0.003 (0.005)	Loss 0.713 (0.574)	Prec@1 78.125 (80.655)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [200/391]	Time 0.073 (0.071)	Data 0.003 (0.004)	Loss 0.471 (0.581)	Prec@1 83.594 (80.449)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [250/391]	Time 0.066 (0.070)	Data 0.002 (0.004)	Loss 0.787 (0.583)	Prec@1 71.875 (80.294)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [300/391]	Time 0.047 (0.069)	Data 0.002 (0.004)	Loss 0.751 (0.581)	Prec@1 74.219 (80.388)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [350/391]	Time 0.043 (0.066)	Data 0.002 (0.003)	Loss 0.548 (0.583)	Prec@1 79.688 (80.362)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Test: [0/79]	Time: 0.3807(0.3807)	Loss: 1.243(1.243)	Prec@1: 66.406(66.406)	
06-15-23 19:08:Test: [50/79]	Time: 0.0164(0.0236)	Loss: 1.643(1.528)	Prec@1: 56.250(60.600)	
06-15-23 19:08:Test: [78/79]	Time: 0.0149(0.0211)	Loss: 1.120(1.534)	Prec@1: 56.250(60.780)	
06-15-23 19:08:Step 91 * Prec@1 60.780
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [0/391]	Time 0.389 (0.389)	Data 0.352 (0.352)	Loss 0.370 (0.370)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [50/391]	Time 0.069 (0.070)	Data 0.002 (0.009)	Loss 0.543 (0.591)	Prec@1 81.250 (79.795)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [100/391]	Time 0.073 (0.071)	Data 0.003 (0.006)	Loss 0.620 (0.581)	Prec@1 82.031 (80.415)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [150/391]	Time 0.073 (0.071)	Data 0.003 (0.005)	Loss 0.555 (0.584)	Prec@1 79.688 (80.401)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [200/391]	Time 0.074 (0.071)	Data 0.003 (0.004)	Loss 0.678 (0.584)	Prec@1 81.250 (80.430)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [250/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.666 (0.584)	Prec@1 80.469 (80.410)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [300/391]	Time 0.070 (0.070)	Data 0.003 (0.004)	Loss 0.510 (0.587)	Prec@1 84.375 (80.297)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [350/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.760 (0.592)	Prec@1 78.906 (80.144)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Test: [0/79]	Time: 0.3688(0.3688)	Loss: 1.937(1.937)	Prec@1: 54.688(54.688)	
06-15-23 19:08:Test: [50/79]	Time: 0.0165(0.0267)	Loss: 2.369(2.024)	Prec@1: 44.531(50.429)	
06-15-23 19:08:Test: [78/79]	Time: 0.0150(0.0231)	Loss: 2.008(2.026)	Prec@1: 43.750(50.340)	
06-15-23 19:08:Step 92 * Prec@1 50.340
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [0/391]	Time 0.378 (0.378)	Data 0.342 (0.342)	Loss 0.784 (0.784)	Prec@1 73.438 (73.438)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [50/391]	Time 0.073 (0.066)	Data 0.003 (0.009)	Loss 0.639 (0.566)	Prec@1 79.688 (80.607)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:08:Num bit 8	Num grad bit 8	
06-15-23 19:08:Iter: [100/391]	Time 0.061 (0.068)	Data 0.003 (0.006)	Loss 0.487 (0.599)	Prec@1 85.938 (80.043)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [150/391]	Time 0.068 (0.068)	Data 0.002 (0.005)	Loss 0.805 (0.592)	Prec@1 77.344 (80.314)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [200/391]	Time 0.073 (0.069)	Data 0.003 (0.004)	Loss 0.482 (0.587)	Prec@1 84.375 (80.438)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [250/391]	Time 0.070 (0.068)	Data 0.003 (0.004)	Loss 0.478 (0.588)	Prec@1 85.156 (80.291)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [300/391]	Time 0.073 (0.068)	Data 0.002 (0.004)	Loss 0.987 (0.590)	Prec@1 68.750 (80.222)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [350/391]	Time 0.073 (0.068)	Data 0.003 (0.004)	Loss 0.591 (0.585)	Prec@1 78.906 (80.333)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Test: [0/79]	Time: 0.3881(0.3881)	Loss: 2.949(2.949)	Prec@1: 46.875(46.875)	
06-15-23 19:09:Test: [50/79]	Time: 0.0166(0.0238)	Loss: 3.665(3.111)	Prec@1: 33.594(46.661)	
06-15-23 19:09:Test: [78/79]	Time: 0.0151(0.0212)	Loss: 2.627(3.081)	Prec@1: 56.250(46.830)	
06-15-23 19:09:Step 93 * Prec@1 46.830
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [0/391]	Time 0.402 (0.402)	Data 0.354 (0.354)	Loss 0.713 (0.713)	Prec@1 77.344 (77.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [50/391]	Time 0.074 (0.073)	Data 0.003 (0.009)	Loss 0.609 (0.581)	Prec@1 77.344 (80.438)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [100/391]	Time 0.063 (0.071)	Data 0.002 (0.006)	Loss 0.617 (0.577)	Prec@1 80.469 (80.569)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [150/391]	Time 0.061 (0.071)	Data 0.002 (0.005)	Loss 0.555 (0.587)	Prec@1 83.594 (80.495)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [200/391]	Time 0.073 (0.071)	Data 0.003 (0.004)	Loss 0.569 (0.583)	Prec@1 84.375 (80.636)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [250/391]	Time 0.065 (0.071)	Data 0.003 (0.004)	Loss 0.491 (0.592)	Prec@1 83.594 (80.322)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [300/391]	Time 0.071 (0.071)	Data 0.003 (0.004)	Loss 0.502 (0.590)	Prec@1 81.250 (80.368)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [350/391]	Time 0.066 (0.071)	Data 0.003 (0.004)	Loss 0.301 (0.588)	Prec@1 89.844 (80.386)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Test: [0/79]	Time: 0.3814(0.3814)	Loss: 14.685(14.685)	Prec@1: 7.812(7.812)	
06-15-23 19:09:Test: [50/79]	Time: 0.0168(0.0280)	Loss: 14.424(14.956)	Prec@1: 11.719(9.926)	
06-15-23 19:09:Test: [78/79]	Time: 0.0152(0.0240)	Loss: 14.921(14.847)	Prec@1: 6.250(10.110)	
06-15-23 19:09:Step 94 * Prec@1 10.110
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [0/391]	Time 0.384 (0.384)	Data 0.346 (0.346)	Loss 0.858 (0.858)	Prec@1 69.531 (69.531)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [50/391]	Time 0.042 (0.052)	Data 0.002 (0.009)	Loss 0.462 (0.593)	Prec@1 81.250 (80.316)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [100/391]	Time 0.050 (0.049)	Data 0.002 (0.005)	Loss 0.552 (0.571)	Prec@1 79.688 (80.863)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [150/391]	Time 0.050 (0.049)	Data 0.002 (0.004)	Loss 0.624 (0.576)	Prec@1 78.906 (80.500)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:09:Num bit 8	Num grad bit 8	
06-15-23 19:09:Iter: [200/391]	Time 0.048 (0.049)	Data 0.002 (0.004)	Loss 0.551 (0.570)	Prec@1 79.688 (80.741)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [250/391]	Time 0.050 (0.049)	Data 0.002 (0.004)	Loss 0.681 (0.568)	Prec@1 76.562 (80.886)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [300/391]	Time 0.066 (0.050)	Data 0.002 (0.003)	Loss 0.551 (0.565)	Prec@1 84.375 (81.009)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [350/391]	Time 0.064 (0.053)	Data 0.002 (0.003)	Loss 0.789 (0.566)	Prec@1 75.781 (81.092)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Test: [0/79]	Time: 0.3736(0.3736)	Loss: 3.295(3.295)	Prec@1: 49.219(49.219)	
06-15-23 19:10:Test: [50/79]	Time: 0.0166(0.0235)	Loss: 2.954(2.963)	Prec@1: 46.094(50.506)	
06-15-23 19:10:Test: [78/79]	Time: 0.0151(0.0210)	Loss: 3.133(2.957)	Prec@1: 50.000(50.210)	
06-15-23 19:10:Step 95 * Prec@1 50.210
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [0/391]	Time 0.414 (0.414)	Data 0.374 (0.374)	Loss 0.421 (0.421)	Prec@1 87.500 (87.500)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [50/391]	Time 0.067 (0.070)	Data 0.002 (0.010)	Loss 0.529 (0.588)	Prec@1 85.156 (80.699)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [100/391]	Time 0.072 (0.071)	Data 0.003 (0.006)	Loss 0.584 (0.587)	Prec@1 80.469 (80.716)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [150/391]	Time 0.071 (0.071)	Data 0.002 (0.005)	Loss 0.714 (0.590)	Prec@1 75.781 (80.567)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [200/391]	Time 0.070 (0.071)	Data 0.003 (0.004)	Loss 0.545 (0.574)	Prec@1 84.375 (80.978)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [250/391]	Time 0.071 (0.072)	Data 0.003 (0.004)	Loss 0.567 (0.579)	Prec@1 80.469 (80.827)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [300/391]	Time 0.069 (0.071)	Data 0.003 (0.004)	Loss 0.412 (0.583)	Prec@1 87.500 (80.718)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [350/391]	Time 0.071 (0.071)	Data 0.003 (0.004)	Loss 0.487 (0.577)	Prec@1 81.250 (80.825)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Test: [0/79]	Time: 0.3711(0.3711)	Loss: 3.358(3.358)	Prec@1: 48.438(48.438)	
06-15-23 19:10:Test: [50/79]	Time: 0.0166(0.0235)	Loss: 4.819(4.161)	Prec@1: 31.250(39.859)	
06-15-23 19:10:Test: [78/79]	Time: 0.0150(0.0210)	Loss: 2.835(4.074)	Prec@1: 56.250(40.470)	
06-15-23 19:10:Step 96 * Prec@1 40.470
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [0/391]	Time 0.431 (0.431)	Data 0.385 (0.385)	Loss 0.601 (0.601)	Prec@1 77.344 (77.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [50/391]	Time 0.077 (0.071)	Data 0.002 (0.010)	Loss 0.386 (0.591)	Prec@1 85.938 (80.132)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [100/391]	Time 0.073 (0.070)	Data 0.002 (0.006)	Loss 0.477 (0.565)	Prec@1 82.031 (80.685)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [150/391]	Time 0.067 (0.070)	Data 0.003 (0.005)	Loss 0.540 (0.567)	Prec@1 81.250 (80.831)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [200/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.398 (0.574)	Prec@1 84.375 (80.640)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:10:Num bit 8	Num grad bit 8	
06-15-23 19:10:Iter: [250/391]	Time 0.072 (0.070)	Data 0.003 (0.004)	Loss 0.503 (0.568)	Prec@1 83.594 (80.755)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [300/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.502 (0.573)	Prec@1 81.250 (80.705)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [350/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.708 (0.577)	Prec@1 73.438 (80.529)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Test: [0/79]	Time: 0.3707(0.3707)	Loss: 4.095(4.095)	Prec@1: 40.625(40.625)	
06-15-23 19:11:Test: [50/79]	Time: 0.0165(0.0248)	Loss: 4.115(3.806)	Prec@1: 38.281(41.774)	
06-15-23 19:11:Test: [78/79]	Time: 0.0153(0.0220)	Loss: 3.095(3.792)	Prec@1: 37.500(41.440)	
06-15-23 19:11:Step 97 * Prec@1 41.440
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [0/391]	Time 0.420 (0.420)	Data 0.383 (0.383)	Loss 0.693 (0.693)	Prec@1 75.781 (75.781)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [50/391]	Time 0.064 (0.068)	Data 0.002 (0.010)	Loss 0.589 (0.565)	Prec@1 81.250 (81.081)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [100/391]	Time 0.068 (0.068)	Data 0.003 (0.006)	Loss 0.666 (0.601)	Prec@1 76.562 (79.997)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [150/391]	Time 0.074 (0.069)	Data 0.002 (0.005)	Loss 0.620 (0.605)	Prec@1 80.469 (79.822)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [200/391]	Time 0.072 (0.070)	Data 0.003 (0.004)	Loss 0.444 (0.596)	Prec@1 82.031 (80.166)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [250/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.555 (0.592)	Prec@1 82.812 (80.235)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [300/391]	Time 0.073 (0.070)	Data 0.002 (0.004)	Loss 0.585 (0.590)	Prec@1 77.344 (80.264)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [350/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.531 (0.589)	Prec@1 82.031 (80.264)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Test: [0/79]	Time: 0.3490(0.3490)	Loss: 2.120(2.120)	Prec@1: 47.656(47.656)	
06-15-23 19:11:Test: [50/79]	Time: 0.0167(0.0231)	Loss: 2.404(2.237)	Prec@1: 43.750(44.240)	
06-15-23 19:11:Test: [78/79]	Time: 0.0151(0.0208)	Loss: 2.847(2.254)	Prec@1: 37.500(44.070)	
06-15-23 19:11:Step 98 * Prec@1 44.070
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [0/391]	Time 0.392 (0.392)	Data 0.347 (0.347)	Loss 0.489 (0.489)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [50/391]	Time 0.066 (0.055)	Data 0.002 (0.009)	Loss 0.515 (0.606)	Prec@1 81.250 (80.101)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [100/391]	Time 0.071 (0.062)	Data 0.002 (0.006)	Loss 0.660 (0.591)	Prec@1 81.250 (80.415)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [150/391]	Time 0.068 (0.065)	Data 0.003 (0.005)	Loss 0.481 (0.591)	Prec@1 82.812 (80.314)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [200/391]	Time 0.074 (0.067)	Data 0.003 (0.004)	Loss 0.502 (0.592)	Prec@1 83.594 (80.232)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [250/391]	Time 0.069 (0.067)	Data 0.003 (0.004)	Loss 0.554 (0.588)	Prec@1 78.125 (80.341)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:11:Num bit 8	Num grad bit 8	
06-15-23 19:11:Iter: [300/391]	Time 0.069 (0.067)	Data 0.003 (0.004)	Loss 0.462 (0.594)	Prec@1 84.375 (80.170)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [350/391]	Time 0.072 (0.067)	Data 0.003 (0.003)	Loss 0.495 (0.593)	Prec@1 83.594 (80.168)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Test: [0/79]	Time: 0.3687(0.3687)	Loss: 3.153(3.153)	Prec@1: 44.531(44.531)	
06-15-23 19:12:Test: [50/79]	Time: 0.0166(0.0234)	Loss: 4.152(3.574)	Prec@1: 33.594(40.763)	
06-15-23 19:12:Test: [78/79]	Time: 0.0152(0.0216)	Loss: 2.178(3.585)	Prec@1: 56.250(40.380)	
06-15-23 19:12:Step 99 * Prec@1 40.380
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [0/391]	Time 0.389 (0.389)	Data 0.353 (0.353)	Loss 0.573 (0.573)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [50/391]	Time 0.068 (0.063)	Data 0.003 (0.009)	Loss 0.426 (0.499)	Prec@1 86.719 (82.629)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [100/391]	Time 0.061 (0.067)	Data 0.002 (0.006)	Loss 0.362 (0.484)	Prec@1 88.281 (83.161)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [150/391]	Time 0.034 (0.066)	Data 0.002 (0.005)	Loss 0.361 (0.483)	Prec@1 85.938 (83.164)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [200/391]	Time 0.053 (0.060)	Data 0.002 (0.004)	Loss 0.438 (0.484)	Prec@1 85.938 (83.221)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [250/391]	Time 0.048 (0.058)	Data 0.002 (0.004)	Loss 0.676 (0.485)	Prec@1 78.125 (83.208)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [300/391]	Time 0.052 (0.057)	Data 0.002 (0.003)	Loss 0.454 (0.484)	Prec@1 82.031 (83.280)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [350/391]	Time 0.052 (0.056)	Data 0.003 (0.003)	Loss 0.412 (0.486)	Prec@1 87.500 (83.280)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Test: [0/79]	Time: 0.3710(0.3710)	Loss: 6.695(6.695)	Prec@1: 37.500(37.500)	
06-15-23 19:12:Test: [50/79]	Time: 0.0165(0.0235)	Loss: 6.134(6.161)	Prec@1: 31.250(34.881)	
06-15-23 19:12:Test: [78/79]	Time: 0.0150(0.0210)	Loss: 5.942(6.132)	Prec@1: 37.500(34.710)	
06-15-23 19:12:Step 100 * Prec@1 34.710
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [0/391]	Time 0.393 (0.393)	Data 0.348 (0.348)	Loss 0.585 (0.585)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [50/391]	Time 0.069 (0.063)	Data 0.002 (0.009)	Loss 0.580 (0.472)	Prec@1 79.688 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [100/391]	Time 0.042 (0.056)	Data 0.001 (0.005)	Loss 0.413 (0.482)	Prec@1 85.938 (83.215)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [150/391]	Time 0.077 (0.061)	Data 0.003 (0.005)	Loss 0.495 (0.492)	Prec@1 81.250 (83.014)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [200/391]	Time 0.073 (0.063)	Data 0.003 (0.004)	Loss 0.505 (0.488)	Prec@1 85.156 (83.291)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [250/391]	Time 0.070 (0.065)	Data 0.003 (0.004)	Loss 0.494 (0.486)	Prec@1 82.812 (83.304)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [300/391]	Time 0.073 (0.066)	Data 0.003 (0.004)	Loss 0.561 (0.482)	Prec@1 83.594 (83.498)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [350/391]	Time 0.065 (0.067)	Data 0.002 (0.003)	Loss 0.524 (0.480)	Prec@1 84.375 (83.540)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:12:Test: [0/79]	Time: 0.3769(0.3769)	Loss: 1.698(1.698)	Prec@1: 56.250(56.250)	
06-15-23 19:12:Test: [50/79]	Time: 0.0164(0.0234)	Loss: 1.783(1.748)	Prec@1: 57.812(56.740)	
06-15-23 19:12:Test: [78/79]	Time: 0.0150(0.0210)	Loss: 0.967(1.711)	Prec@1: 81.250(57.470)	
06-15-23 19:12:Step 101 * Prec@1 57.470
06-15-23 19:12:Num bit 8	Num grad bit 8	
06-15-23 19:12:Iter: [0/391]	Time 0.397 (0.397)	Data 0.360 (0.360)	Loss 0.391 (0.391)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [50/391]	Time 0.074 (0.071)	Data 0.003 (0.009)	Loss 0.460 (0.465)	Prec@1 81.250 (83.961)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [100/391]	Time 0.070 (0.069)	Data 0.002 (0.006)	Loss 0.465 (0.474)	Prec@1 84.375 (83.625)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [150/391]	Time 0.068 (0.068)	Data 0.002 (0.005)	Loss 0.428 (0.477)	Prec@1 85.938 (83.521)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [200/391]	Time 0.061 (0.067)	Data 0.002 (0.004)	Loss 0.409 (0.479)	Prec@1 87.500 (83.617)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [250/391]	Time 0.069 (0.068)	Data 0.003 (0.004)	Loss 0.455 (0.475)	Prec@1 83.594 (83.684)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [300/391]	Time 0.064 (0.068)	Data 0.003 (0.004)	Loss 0.711 (0.479)	Prec@1 79.688 (83.521)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [350/391]	Time 0.064 (0.067)	Data 0.003 (0.003)	Loss 0.512 (0.479)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Test: [0/79]	Time: 0.3854(0.3854)	Loss: 5.318(5.318)	Prec@1: 23.438(23.438)	
06-15-23 19:13:Test: [50/79]	Time: 0.0199(0.0270)	Loss: 5.607(4.972)	Prec@1: 21.094(30.607)	
06-15-23 19:13:Test: [78/79]	Time: 0.0182(0.0244)	Loss: 4.692(4.999)	Prec@1: 25.000(30.620)	
06-15-23 19:13:Step 102 * Prec@1 30.620
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [0/391]	Time 0.430 (0.430)	Data 0.394 (0.394)	Loss 0.487 (0.487)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [50/391]	Time 0.071 (0.061)	Data 0.003 (0.010)	Loss 0.527 (0.480)	Prec@1 84.375 (84.038)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [100/391]	Time 0.071 (0.066)	Data 0.003 (0.006)	Loss 0.494 (0.478)	Prec@1 83.594 (83.818)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [150/391]	Time 0.067 (0.068)	Data 0.003 (0.005)	Loss 0.435 (0.475)	Prec@1 86.719 (83.827)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [200/391]	Time 0.074 (0.068)	Data 0.004 (0.004)	Loss 0.489 (0.479)	Prec@1 83.594 (83.656)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [250/391]	Time 0.075 (0.069)	Data 0.003 (0.004)	Loss 0.569 (0.480)	Prec@1 79.688 (83.612)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [300/391]	Time 0.069 (0.069)	Data 0.002 (0.004)	Loss 0.461 (0.481)	Prec@1 82.812 (83.560)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [350/391]	Time 0.063 (0.069)	Data 0.002 (0.004)	Loss 0.404 (0.480)	Prec@1 86.719 (83.587)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Test: [0/79]	Time: 0.3749(0.3749)	Loss: 1.850(1.850)	Prec@1: 53.906(53.906)	
06-15-23 19:13:Test: [50/79]	Time: 0.0164(0.0233)	Loss: 2.060(1.824)	Prec@1: 51.562(57.138)	
06-15-23 19:13:Test: [78/79]	Time: 0.0150(0.0209)	Loss: 1.431(1.833)	Prec@1: 56.250(56.700)	
06-15-23 19:13:Step 103 * Prec@1 56.700
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [0/391]	Time 0.412 (0.412)	Data 0.362 (0.362)	Loss 0.418 (0.418)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:13:Num bit 8	Num grad bit 8	
06-15-23 19:13:Iter: [50/391]	Time 0.042 (0.054)	Data 0.002 (0.009)	Loss 0.429 (0.482)	Prec@1 85.156 (83.640)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [100/391]	Time 0.056 (0.051)	Data 0.002 (0.005)	Loss 0.571 (0.488)	Prec@1 77.344 (83.238)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [150/391]	Time 0.042 (0.050)	Data 0.001 (0.004)	Loss 0.568 (0.490)	Prec@1 79.688 (83.097)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [200/391]	Time 0.069 (0.052)	Data 0.002 (0.004)	Loss 0.446 (0.490)	Prec@1 85.156 (83.112)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [250/391]	Time 0.073 (0.056)	Data 0.003 (0.003)	Loss 0.423 (0.488)	Prec@1 87.500 (83.264)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [300/391]	Time 0.071 (0.058)	Data 0.003 (0.003)	Loss 0.426 (0.487)	Prec@1 84.375 (83.277)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [350/391]	Time 0.065 (0.059)	Data 0.003 (0.003)	Loss 0.417 (0.488)	Prec@1 88.281 (83.293)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Test: [0/79]	Time: 0.3742(0.3742)	Loss: 1.145(1.145)	Prec@1: 60.156(60.156)	
06-15-23 19:14:Test: [50/79]	Time: 0.0198(0.0268)	Loss: 1.271(1.516)	Prec@1: 65.625(58.885)	
06-15-23 19:14:Test: [78/79]	Time: 0.0183(0.0242)	Loss: 1.172(1.527)	Prec@1: 56.250(58.360)	
06-15-23 19:14:Step 104 * Prec@1 58.360
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [0/391]	Time 0.394 (0.394)	Data 0.355 (0.355)	Loss 0.369 (0.369)	Prec@1 89.844 (89.844)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [50/391]	Time 0.073 (0.064)	Data 0.003 (0.009)	Loss 0.434 (0.476)	Prec@1 85.156 (84.145)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [100/391]	Time 0.071 (0.068)	Data 0.003 (0.006)	Loss 0.404 (0.478)	Prec@1 87.500 (83.849)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [150/391]	Time 0.072 (0.069)	Data 0.003 (0.005)	Loss 0.387 (0.479)	Prec@1 85.938 (83.563)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [200/391]	Time 0.074 (0.070)	Data 0.002 (0.004)	Loss 0.559 (0.479)	Prec@1 79.688 (83.609)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [250/391]	Time 0.069 (0.070)	Data 0.003 (0.004)	Loss 0.583 (0.483)	Prec@1 79.688 (83.354)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [300/391]	Time 0.069 (0.070)	Data 0.003 (0.004)	Loss 0.462 (0.480)	Prec@1 82.812 (83.454)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [350/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.461 (0.482)	Prec@1 82.031 (83.407)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Test: [0/79]	Time: 0.3748(0.3748)	Loss: 1.845(1.845)	Prec@1: 53.125(53.125)	
06-15-23 19:14:Test: [50/79]	Time: 0.0166(0.0277)	Loss: 2.354(2.178)	Prec@1: 45.312(45.910)	
06-15-23 19:14:Test: [78/79]	Time: 0.0150(0.0237)	Loss: 2.299(2.167)	Prec@1: 43.750(45.880)	
06-15-23 19:14:Step 105 * Prec@1 45.880
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [0/391]	Time 0.390 (0.390)	Data 0.350 (0.350)	Loss 0.337 (0.337)	Prec@1 90.625 (90.625)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [50/391]	Time 0.069 (0.069)	Data 0.002 (0.009)	Loss 0.577 (0.457)	Prec@1 79.688 (84.421)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:14:Num bit 8	Num grad bit 8	
06-15-23 19:14:Iter: [100/391]	Time 0.065 (0.070)	Data 0.002 (0.006)	Loss 0.403 (0.474)	Prec@1 85.156 (83.609)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [150/391]	Time 0.071 (0.069)	Data 0.003 (0.005)	Loss 0.393 (0.480)	Prec@1 87.500 (83.620)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [200/391]	Time 0.065 (0.069)	Data 0.002 (0.004)	Loss 0.474 (0.477)	Prec@1 84.375 (83.722)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [250/391]	Time 0.061 (0.069)	Data 0.002 (0.004)	Loss 0.410 (0.477)	Prec@1 85.156 (83.606)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [300/391]	Time 0.068 (0.069)	Data 0.003 (0.004)	Loss 0.574 (0.478)	Prec@1 81.250 (83.630)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [350/391]	Time 0.061 (0.068)	Data 0.002 (0.003)	Loss 0.420 (0.479)	Prec@1 84.375 (83.683)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Test: [0/79]	Time: 0.3784(0.3784)	Loss: 3.930(3.930)	Prec@1: 43.750(43.750)	
06-15-23 19:15:Test: [50/79]	Time: 0.0161(0.0235)	Loss: 4.369(4.595)	Prec@1: 43.750(39.706)	
06-15-23 19:15:Test: [78/79]	Time: 0.0150(0.0209)	Loss: 1.981(4.576)	Prec@1: 56.250(40.170)	
06-15-23 19:15:Step 106 * Prec@1 40.170
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [0/391]	Time 0.397 (0.397)	Data 0.361 (0.361)	Loss 0.459 (0.459)	Prec@1 87.500 (87.500)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [50/391]	Time 0.071 (0.074)	Data 0.003 (0.010)	Loss 0.577 (0.472)	Prec@1 82.031 (84.252)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [100/391]	Time 0.074 (0.073)	Data 0.003 (0.006)	Loss 0.453 (0.475)	Prec@1 84.375 (83.741)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [150/391]	Time 0.073 (0.072)	Data 0.003 (0.005)	Loss 0.614 (0.479)	Prec@1 79.688 (83.640)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [200/391]	Time 0.071 (0.071)	Data 0.003 (0.004)	Loss 0.355 (0.477)	Prec@1 87.500 (83.675)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [250/391]	Time 0.059 (0.070)	Data 0.002 (0.004)	Loss 0.603 (0.476)	Prec@1 78.906 (83.675)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [300/391]	Time 0.068 (0.069)	Data 0.002 (0.004)	Loss 0.548 (0.476)	Prec@1 83.594 (83.731)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [350/391]	Time 0.071 (0.069)	Data 0.003 (0.004)	Loss 0.388 (0.478)	Prec@1 88.281 (83.609)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Test: [0/79]	Time: 0.3688(0.3688)	Loss: 2.699(2.699)	Prec@1: 48.438(48.438)	
06-15-23 19:15:Test: [50/79]	Time: 0.0168(0.0235)	Loss: 3.422(2.749)	Prec@1: 35.156(47.932)	
06-15-23 19:15:Test: [78/79]	Time: 0.0151(0.0211)	Loss: 2.633(2.756)	Prec@1: 56.250(47.950)	
06-15-23 19:15:Step 107 * Prec@1 47.950
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [0/391]	Time 0.387 (0.387)	Data 0.350 (0.350)	Loss 0.434 (0.434)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [50/391]	Time 0.038 (0.042)	Data 0.002 (0.009)	Loss 0.578 (0.472)	Prec@1 78.125 (83.211)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [100/391]	Time 0.037 (0.039)	Data 0.002 (0.005)	Loss 0.600 (0.478)	Prec@1 82.031 (83.354)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [150/391]	Time 0.066 (0.046)	Data 0.002 (0.004)	Loss 0.512 (0.470)	Prec@1 83.594 (83.687)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:15:Num bit 8	Num grad bit 8	
06-15-23 19:15:Iter: [200/391]	Time 0.062 (0.052)	Data 0.002 (0.004)	Loss 0.465 (0.466)	Prec@1 83.594 (83.979)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [250/391]	Time 0.074 (0.056)	Data 0.003 (0.004)	Loss 0.412 (0.471)	Prec@1 85.156 (83.809)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [300/391]	Time 0.070 (0.058)	Data 0.002 (0.003)	Loss 0.457 (0.475)	Prec@1 84.375 (83.646)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [350/391]	Time 0.072 (0.059)	Data 0.003 (0.003)	Loss 0.484 (0.475)	Prec@1 80.469 (83.587)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Test: [0/79]	Time: 0.3680(0.3680)	Loss: 5.623(5.623)	Prec@1: 28.125(28.125)	
06-15-23 19:16:Test: [50/79]	Time: 0.0163(0.0235)	Loss: 5.464(6.187)	Prec@1: 25.000(21.829)	
06-15-23 19:16:Test: [78/79]	Time: 0.0151(0.0210)	Loss: 5.835(6.110)	Prec@1: 31.250(22.300)	
06-15-23 19:16:Step 108 * Prec@1 22.300
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [0/391]	Time 0.417 (0.417)	Data 0.378 (0.378)	Loss 0.317 (0.317)	Prec@1 89.062 (89.062)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [50/391]	Time 0.069 (0.071)	Data 0.003 (0.010)	Loss 0.523 (0.473)	Prec@1 84.375 (83.946)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [100/391]	Time 0.070 (0.070)	Data 0.003 (0.006)	Loss 0.469 (0.477)	Prec@1 84.375 (83.718)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [150/391]	Time 0.074 (0.070)	Data 0.003 (0.005)	Loss 0.450 (0.475)	Prec@1 82.812 (83.609)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [200/391]	Time 0.062 (0.070)	Data 0.002 (0.004)	Loss 0.452 (0.472)	Prec@1 83.594 (83.749)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [250/391]	Time 0.068 (0.070)	Data 0.002 (0.004)	Loss 0.516 (0.472)	Prec@1 83.594 (83.774)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [300/391]	Time 0.063 (0.070)	Data 0.002 (0.004)	Loss 0.417 (0.474)	Prec@1 85.156 (83.677)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [350/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.538 (0.477)	Prec@1 78.906 (83.514)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Test: [0/79]	Time: 0.3753(0.3753)	Loss: 2.641(2.641)	Prec@1: 50.000(50.000)	
06-15-23 19:16:Test: [50/79]	Time: 0.0166(0.0236)	Loss: 3.478(3.074)	Prec@1: 41.406(47.044)	
06-15-23 19:16:Test: [78/79]	Time: 0.0150(0.0211)	Loss: 1.765(3.032)	Prec@1: 62.500(47.250)	
06-15-23 19:16:Step 109 * Prec@1 47.250
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [0/391]	Time 0.414 (0.414)	Data 0.363 (0.363)	Loss 0.394 (0.394)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [50/391]	Time 0.042 (0.054)	Data 0.002 (0.009)	Loss 0.345 (0.469)	Prec@1 87.500 (84.161)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [100/391]	Time 0.055 (0.051)	Data 0.002 (0.005)	Loss 0.348 (0.465)	Prec@1 87.500 (84.220)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [150/391]	Time 0.044 (0.050)	Data 0.002 (0.004)	Loss 0.563 (0.466)	Prec@1 77.344 (84.137)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [200/391]	Time 0.055 (0.049)	Data 0.002 (0.004)	Loss 0.469 (0.467)	Prec@1 80.469 (84.087)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [250/391]	Time 0.042 (0.049)	Data 0.002 (0.003)	Loss 0.501 (0.471)	Prec@1 84.375 (83.886)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:16:Num bit 8	Num grad bit 8	
06-15-23 19:16:Iter: [300/391]	Time 0.055 (0.049)	Data 0.002 (0.003)	Loss 0.550 (0.472)	Prec@1 82.812 (83.895)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [350/391]	Time 0.043 (0.048)	Data 0.002 (0.003)	Loss 0.504 (0.473)	Prec@1 81.250 (83.854)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Test: [0/79]	Time: 0.4023(0.4023)	Loss: 3.343(3.343)	Prec@1: 48.438(48.438)	
06-15-23 19:17:Test: [50/79]	Time: 0.0162(0.0240)	Loss: 3.614(3.571)	Prec@1: 43.750(42.433)	
06-15-23 19:17:Test: [78/79]	Time: 0.0150(0.0213)	Loss: 3.975(3.507)	Prec@1: 50.000(42.950)	
06-15-23 19:17:Step 110 * Prec@1 42.950
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [0/391]	Time 0.397 (0.397)	Data 0.352 (0.352)	Loss 0.438 (0.438)	Prec@1 88.281 (88.281)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [50/391]	Time 0.046 (0.047)	Data 0.002 (0.009)	Loss 0.499 (0.470)	Prec@1 82.812 (83.900)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [100/391]	Time 0.043 (0.045)	Data 0.002 (0.005)	Loss 0.636 (0.467)	Prec@1 80.469 (83.942)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [150/391]	Time 0.052 (0.045)	Data 0.002 (0.004)	Loss 0.481 (0.463)	Prec@1 82.031 (84.147)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [200/391]	Time 0.041 (0.044)	Data 0.002 (0.004)	Loss 0.413 (0.467)	Prec@1 87.500 (84.025)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [250/391]	Time 0.046 (0.044)	Data 0.002 (0.003)	Loss 0.380 (0.471)	Prec@1 86.719 (83.952)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [300/391]	Time 0.045 (0.044)	Data 0.002 (0.003)	Loss 0.519 (0.474)	Prec@1 80.469 (83.718)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [350/391]	Time 0.046 (0.044)	Data 0.002 (0.003)	Loss 0.476 (0.474)	Prec@1 86.719 (83.770)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Test: [0/79]	Time: 0.3547(0.3547)	Loss: 2.957(2.957)	Prec@1: 40.625(40.625)	
06-15-23 19:17:Test: [50/79]	Time: 0.0166(0.0231)	Loss: 3.866(3.710)	Prec@1: 37.500(35.738)	
06-15-23 19:17:Test: [78/79]	Time: 0.0150(0.0208)	Loss: 4.105(3.683)	Prec@1: 31.250(36.020)	
06-15-23 19:17:Step 111 * Prec@1 36.020
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [0/391]	Time 0.428 (0.428)	Data 0.380 (0.380)	Loss 0.456 (0.456)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [50/391]	Time 0.062 (0.066)	Data 0.002 (0.010)	Loss 0.429 (0.461)	Prec@1 86.719 (83.609)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [100/391]	Time 0.064 (0.068)	Data 0.003 (0.007)	Loss 0.495 (0.464)	Prec@1 79.688 (83.779)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [150/391]	Time 0.074 (0.069)	Data 0.003 (0.005)	Loss 0.468 (0.466)	Prec@1 85.938 (83.842)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [200/391]	Time 0.063 (0.069)	Data 0.003 (0.005)	Loss 0.444 (0.469)	Prec@1 82.812 (83.761)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [250/391]	Time 0.072 (0.069)	Data 0.002 (0.004)	Loss 0.514 (0.470)	Prec@1 82.031 (83.709)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [300/391]	Time 0.071 (0.069)	Data 0.002 (0.004)	Loss 0.544 (0.472)	Prec@1 81.250 (83.653)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [350/391]	Time 0.066 (0.069)	Data 0.002 (0.004)	Loss 0.407 (0.475)	Prec@1 85.938 (83.545)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Test: [0/79]	Time: 0.4527(0.4527)	Loss: 3.118(3.118)	Prec@1: 42.969(42.969)	
06-15-23 19:17:Test: [50/79]	Time: 0.0163(0.0249)	Loss: 3.808(3.340)	Prec@1: 35.156(38.680)	
06-15-23 19:17:Test: [78/79]	Time: 0.0152(0.0220)	Loss: 4.200(3.327)	Prec@1: 37.500(38.770)	
06-15-23 19:17:Step 112 * Prec@1 38.770
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [0/391]	Time 0.412 (0.412)	Data 0.371 (0.371)	Loss 0.503 (0.503)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [50/391]	Time 0.076 (0.066)	Data 0.003 (0.010)	Loss 0.535 (0.472)	Prec@1 80.469 (83.563)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:17:Num bit 8	Num grad bit 8	
06-15-23 19:17:Iter: [100/391]	Time 0.073 (0.069)	Data 0.003 (0.006)	Loss 0.554 (0.475)	Prec@1 83.594 (83.354)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [150/391]	Time 0.069 (0.069)	Data 0.003 (0.005)	Loss 0.583 (0.473)	Prec@1 75.000 (83.532)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [200/391]	Time 0.065 (0.068)	Data 0.002 (0.004)	Loss 0.498 (0.472)	Prec@1 85.156 (83.706)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [250/391]	Time 0.073 (0.068)	Data 0.003 (0.004)	Loss 0.455 (0.474)	Prec@1 80.469 (83.662)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [300/391]	Time 0.059 (0.067)	Data 0.002 (0.004)	Loss 0.466 (0.474)	Prec@1 87.500 (83.711)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [350/391]	Time 0.058 (0.067)	Data 0.003 (0.004)	Loss 0.532 (0.474)	Prec@1 82.812 (83.663)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Test: [0/79]	Time: 0.3732(0.3732)	Loss: 0.817(0.817)	Prec@1: 68.750(68.750)	
06-15-23 19:18:Test: [50/79]	Time: 0.0180(0.0245)	Loss: 1.056(0.961)	Prec@1: 66.406(69.945)	
06-15-23 19:18:Test: [78/79]	Time: 0.0153(0.0220)	Loss: 1.191(0.960)	Prec@1: 68.750(70.090)	
06-15-23 19:18:Step 113 * Prec@1 70.090
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [0/391]	Time 0.468 (0.468)	Data 0.429 (0.429)	Loss 0.344 (0.344)	Prec@1 87.500 (87.500)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [50/391]	Time 0.065 (0.076)	Data 0.002 (0.011)	Loss 0.406 (0.474)	Prec@1 85.156 (83.747)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [100/391]	Time 0.074 (0.073)	Data 0.003 (0.007)	Loss 0.425 (0.482)	Prec@1 89.062 (83.369)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [150/391]	Time 0.067 (0.072)	Data 0.003 (0.006)	Loss 0.546 (0.482)	Prec@1 81.250 (83.439)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [200/391]	Time 0.069 (0.072)	Data 0.003 (0.005)	Loss 0.395 (0.481)	Prec@1 84.375 (83.427)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [250/391]	Time 0.073 (0.071)	Data 0.003 (0.005)	Loss 0.388 (0.480)	Prec@1 85.156 (83.432)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [300/391]	Time 0.053 (0.071)	Data 0.003 (0.004)	Loss 0.438 (0.476)	Prec@1 85.156 (83.617)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [350/391]	Time 0.080 (0.070)	Data 0.012 (0.004)	Loss 0.464 (0.476)	Prec@1 81.250 (83.616)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Test: [0/79]	Time: 0.4502(0.4502)	Loss: 6.538(6.538)	Prec@1: 21.094(21.094)	
06-15-23 19:18:Test: [50/79]	Time: 0.0166(0.0261)	Loss: 5.806(6.915)	Prec@1: 22.656(17.800)	
06-15-23 19:18:Test: [78/79]	Time: 0.0151(0.0228)	Loss: 6.791(6.869)	Prec@1: 25.000(17.930)	
06-15-23 19:18:Step 114 * Prec@1 17.930
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [0/391]	Time 0.394 (0.394)	Data 0.354 (0.354)	Loss 0.578 (0.578)	Prec@1 81.250 (81.250)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [50/391]	Time 0.076 (0.069)	Data 0.003 (0.009)	Loss 0.537 (0.460)	Prec@1 80.469 (83.824)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:18:Num bit 8	Num grad bit 8	
06-15-23 19:18:Iter: [100/391]	Time 0.078 (0.069)	Data 0.003 (0.006)	Loss 0.505 (0.471)	Prec@1 83.594 (83.625)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [150/391]	Time 0.068 (0.069)	Data 0.003 (0.005)	Loss 0.420 (0.468)	Prec@1 84.375 (83.677)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [200/391]	Time 0.065 (0.068)	Data 0.003 (0.005)	Loss 0.421 (0.479)	Prec@1 87.500 (83.423)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [250/391]	Time 0.080 (0.068)	Data 0.003 (0.004)	Loss 0.494 (0.474)	Prec@1 80.469 (83.584)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [300/391]	Time 0.069 (0.069)	Data 0.003 (0.004)	Loss 0.509 (0.474)	Prec@1 83.594 (83.705)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [350/391]	Time 0.061 (0.069)	Data 0.002 (0.004)	Loss 0.390 (0.473)	Prec@1 88.281 (83.812)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Test: [0/79]	Time: 0.3765(0.3765)	Loss: 0.776(0.776)	Prec@1: 72.656(72.656)	
06-15-23 19:19:Test: [50/79]	Time: 0.0168(0.0239)	Loss: 1.080(0.961)	Prec@1: 67.969(67.800)	
06-15-23 19:19:Test: [78/79]	Time: 0.0152(0.0214)	Loss: 0.817(0.957)	Prec@1: 68.750(68.050)	
06-15-23 19:19:Step 115 * Prec@1 68.050
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [0/391]	Time 0.477 (0.477)	Data 0.426 (0.426)	Loss 0.441 (0.441)	Prec@1 88.281 (88.281)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [50/391]	Time 0.071 (0.074)	Data 0.003 (0.011)	Loss 0.349 (0.444)	Prec@1 86.719 (84.819)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [100/391]	Time 0.068 (0.073)	Data 0.003 (0.007)	Loss 0.514 (0.454)	Prec@1 84.375 (84.182)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [150/391]	Time 0.072 (0.072)	Data 0.004 (0.006)	Loss 0.524 (0.457)	Prec@1 85.156 (84.303)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [200/391]	Time 0.066 (0.070)	Data 0.002 (0.005)	Loss 0.638 (0.460)	Prec@1 78.125 (84.410)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [250/391]	Time 0.070 (0.068)	Data 0.003 (0.004)	Loss 0.429 (0.462)	Prec@1 85.938 (84.335)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [300/391]	Time 0.073 (0.067)	Data 0.003 (0.004)	Loss 0.560 (0.468)	Prec@1 82.812 (84.128)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [350/391]	Time 0.069 (0.067)	Data 0.003 (0.004)	Loss 0.607 (0.465)	Prec@1 78.125 (84.201)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Test: [0/79]	Time: 0.4444(0.4444)	Loss: 2.453(2.453)	Prec@1: 47.656(47.656)	
06-15-23 19:19:Test: [50/79]	Time: 0.0169(0.0254)	Loss: 2.853(2.915)	Prec@1: 47.656(43.290)	
06-15-23 19:19:Test: [78/79]	Time: 0.0154(0.0224)	Loss: 2.597(2.852)	Prec@1: 50.000(43.760)	
06-15-23 19:19:Step 116 * Prec@1 43.760
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [0/391]	Time 0.534 (0.534)	Data 0.467 (0.467)	Loss 0.465 (0.465)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [50/391]	Time 0.074 (0.073)	Data 0.007 (0.012)	Loss 0.426 (0.466)	Prec@1 85.156 (83.992)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [100/391]	Time 0.056 (0.070)	Data 0.002 (0.007)	Loss 0.349 (0.473)	Prec@1 90.625 (83.718)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:19:Num bit 8	Num grad bit 8	
06-15-23 19:19:Iter: [150/391]	Time 0.070 (0.070)	Data 0.003 (0.006)	Loss 0.462 (0.474)	Prec@1 83.594 (83.723)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [200/391]	Time 0.071 (0.070)	Data 0.003 (0.005)	Loss 0.467 (0.469)	Prec@1 83.594 (83.901)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [250/391]	Time 0.058 (0.070)	Data 0.003 (0.005)	Loss 0.520 (0.470)	Prec@1 78.125 (83.821)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [300/391]	Time 0.078 (0.070)	Data 0.003 (0.004)	Loss 0.571 (0.471)	Prec@1 82.031 (83.788)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [350/391]	Time 0.073 (0.071)	Data 0.003 (0.004)	Loss 0.523 (0.473)	Prec@1 82.031 (83.689)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Test: [0/79]	Time: 0.4591(0.4591)	Loss: 1.304(1.304)	Prec@1: 65.625(65.625)	
06-15-23 19:20:Test: [50/79]	Time: 0.0169(0.0256)	Loss: 1.491(1.427)	Prec@1: 64.844(60.999)	
06-15-23 19:20:Test: [78/79]	Time: 0.0155(0.0225)	Loss: 0.800(1.438)	Prec@1: 75.000(60.340)	
06-15-23 19:20:Step 117 * Prec@1 60.340
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [0/391]	Time 0.498 (0.498)	Data 0.452 (0.452)	Loss 0.480 (0.480)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [50/391]	Time 0.072 (0.064)	Data 0.003 (0.011)	Loss 0.494 (0.478)	Prec@1 82.812 (83.747)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [100/391]	Time 0.068 (0.067)	Data 0.003 (0.007)	Loss 0.618 (0.472)	Prec@1 80.469 (84.027)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [150/391]	Time 0.072 (0.068)	Data 0.003 (0.006)	Loss 0.433 (0.472)	Prec@1 84.375 (83.832)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [200/391]	Time 0.064 (0.069)	Data 0.003 (0.005)	Loss 0.489 (0.471)	Prec@1 84.375 (83.815)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [250/391]	Time 0.072 (0.069)	Data 0.003 (0.005)	Loss 0.634 (0.469)	Prec@1 76.562 (83.896)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [300/391]	Time 0.075 (0.070)	Data 0.003 (0.004)	Loss 0.519 (0.472)	Prec@1 81.250 (83.783)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [350/391]	Time 0.073 (0.069)	Data 0.003 (0.004)	Loss 0.610 (0.475)	Prec@1 79.688 (83.649)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Test: [0/79]	Time: 0.4592(0.4592)	Loss: 1.709(1.709)	Prec@1: 48.438(48.438)	
06-15-23 19:20:Test: [50/79]	Time: 0.0169(0.0304)	Loss: 2.124(2.131)	Prec@1: 39.844(41.406)	
06-15-23 19:20:Test: [78/79]	Time: 0.0152(0.0263)	Loss: 1.850(2.113)	Prec@1: 43.750(41.830)	
06-15-23 19:20:Step 118 * Prec@1 41.830
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [0/391]	Time 0.487 (0.487)	Data 0.446 (0.446)	Loss 0.480 (0.480)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [50/391]	Time 0.067 (0.073)	Data 0.003 (0.011)	Loss 0.375 (0.450)	Prec@1 88.281 (84.559)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [100/391]	Time 0.075 (0.072)	Data 0.003 (0.007)	Loss 0.515 (0.466)	Prec@1 82.812 (84.120)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [150/391]	Time 0.071 (0.072)	Data 0.003 (0.006)	Loss 0.529 (0.470)	Prec@1 79.688 (83.915)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:20:Num bit 8	Num grad bit 8	
06-15-23 19:20:Iter: [200/391]	Time 0.065 (0.067)	Data 0.003 (0.005)	Loss 0.332 (0.472)	Prec@1 85.156 (83.792)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [250/391]	Time 0.075 (0.069)	Data 0.003 (0.004)	Loss 0.437 (0.474)	Prec@1 82.812 (83.690)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [300/391]	Time 0.073 (0.069)	Data 0.003 (0.004)	Loss 0.471 (0.471)	Prec@1 84.375 (83.734)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [350/391]	Time 0.075 (0.069)	Data 0.003 (0.004)	Loss 0.321 (0.472)	Prec@1 89.844 (83.794)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Test: [0/79]	Time: 0.4606(0.4606)	Loss: 2.382(2.382)	Prec@1: 45.312(45.312)	
06-15-23 19:21:Test: [50/79]	Time: 0.0185(0.0260)	Loss: 2.321(2.235)	Prec@1: 38.281(45.757)	
06-15-23 19:21:Test: [78/79]	Time: 0.0154(0.0231)	Loss: 1.969(2.241)	Prec@1: 62.500(45.670)	
06-15-23 19:21:Step 119 * Prec@1 45.670
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [0/391]	Time 0.509 (0.509)	Data 0.440 (0.440)	Loss 0.548 (0.548)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [50/391]	Time 0.042 (0.059)	Data 0.002 (0.011)	Loss 0.475 (0.473)	Prec@1 83.594 (84.145)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [100/391]	Time 0.073 (0.056)	Data 0.003 (0.006)	Loss 0.345 (0.466)	Prec@1 86.719 (84.166)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [150/391]	Time 0.065 (0.060)	Data 0.003 (0.005)	Loss 0.393 (0.464)	Prec@1 85.156 (84.209)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [200/391]	Time 0.076 (0.063)	Data 0.003 (0.005)	Loss 0.587 (0.468)	Prec@1 81.250 (84.017)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [250/391]	Time 0.072 (0.064)	Data 0.003 (0.004)	Loss 0.476 (0.469)	Prec@1 79.688 (83.952)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [300/391]	Time 0.056 (0.063)	Data 0.002 (0.004)	Loss 0.419 (0.472)	Prec@1 86.719 (83.843)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [350/391]	Time 0.056 (0.064)	Data 0.003 (0.004)	Loss 0.414 (0.471)	Prec@1 82.031 (83.741)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Test: [0/79]	Time: 0.4576(0.4576)	Loss: 7.309(7.309)	Prec@1: 23.438(23.438)	
06-15-23 19:21:Test: [50/79]	Time: 0.0167(0.0253)	Loss: 7.317(6.700)	Prec@1: 23.438(27.880)	
06-15-23 19:21:Test: [78/79]	Time: 0.0152(0.0223)	Loss: 7.140(6.713)	Prec@1: 25.000(28.010)	
06-15-23 19:21:Step 120 * Prec@1 28.010
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [0/391]	Time 0.478 (0.478)	Data 0.431 (0.431)	Loss 0.398 (0.398)	Prec@1 87.500 (87.500)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [50/391]	Time 0.067 (0.069)	Data 0.003 (0.011)	Loss 0.572 (0.468)	Prec@1 80.469 (84.498)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [100/391]	Time 0.071 (0.071)	Data 0.003 (0.007)	Loss 0.484 (0.468)	Prec@1 85.156 (84.282)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [150/391]	Time 0.077 (0.071)	Data 0.003 (0.006)	Loss 0.562 (0.464)	Prec@1 82.812 (84.313)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:21:Num bit 8	Num grad bit 8	
06-15-23 19:21:Iter: [200/391]	Time 0.068 (0.069)	Data 0.003 (0.005)	Loss 0.459 (0.466)	Prec@1 83.594 (84.095)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [250/391]	Time 0.058 (0.069)	Data 0.003 (0.004)	Loss 0.458 (0.473)	Prec@1 82.812 (83.787)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [300/391]	Time 0.075 (0.069)	Data 0.003 (0.004)	Loss 0.440 (0.473)	Prec@1 85.938 (83.775)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [350/391]	Time 0.078 (0.070)	Data 0.003 (0.004)	Loss 0.379 (0.474)	Prec@1 86.719 (83.718)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Test: [0/79]	Time: 0.4603(0.4603)	Loss: 1.679(1.679)	Prec@1: 42.188(42.188)	
06-15-23 19:22:Test: [50/79]	Time: 0.0173(0.0267)	Loss: 1.786(1.847)	Prec@1: 37.500(40.916)	
06-15-23 19:22:Test: [78/79]	Time: 0.0156(0.0239)	Loss: 1.939(1.855)	Prec@1: 37.500(40.950)	
06-15-23 19:22:Step 121 * Prec@1 40.950
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [0/391]	Time 0.518 (0.518)	Data 0.458 (0.458)	Loss 0.496 (0.496)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [50/391]	Time 0.050 (0.076)	Data 0.002 (0.012)	Loss 0.402 (0.477)	Prec@1 88.281 (83.441)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [100/391]	Time 0.065 (0.067)	Data 0.003 (0.007)	Loss 0.414 (0.482)	Prec@1 86.719 (83.424)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [150/391]	Time 0.059 (0.065)	Data 0.002 (0.006)	Loss 0.536 (0.471)	Prec@1 81.250 (83.775)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [200/391]	Time 0.059 (0.065)	Data 0.003 (0.005)	Loss 0.357 (0.468)	Prec@1 86.719 (83.975)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [250/391]	Time 0.075 (0.066)	Data 0.003 (0.004)	Loss 0.495 (0.469)	Prec@1 79.688 (83.889)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [300/391]	Time 0.063 (0.066)	Data 0.002 (0.004)	Loss 0.495 (0.472)	Prec@1 87.500 (83.744)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [350/391]	Time 0.072 (0.067)	Data 0.003 (0.004)	Loss 0.444 (0.474)	Prec@1 83.594 (83.647)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Test: [0/79]	Time: 0.4807(0.4807)	Loss: 7.722(7.722)	Prec@1: 28.906(28.906)	
06-15-23 19:22:Test: [50/79]	Time: 0.0168(0.0261)	Loss: 8.425(8.607)	Prec@1: 30.469(23.238)	
06-15-23 19:22:Test: [78/79]	Time: 0.0161(0.0228)	Loss: 6.208(8.502)	Prec@1: 37.500(23.860)	
06-15-23 19:22:Step 122 * Prec@1 23.860
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [0/391]	Time 0.513 (0.513)	Data 0.455 (0.455)	Loss 0.351 (0.351)	Prec@1 89.062 (89.062)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [50/391]	Time 0.071 (0.072)	Data 0.003 (0.011)	Loss 0.425 (0.452)	Prec@1 83.594 (84.206)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [100/391]	Time 0.071 (0.072)	Data 0.003 (0.007)	Loss 0.447 (0.472)	Prec@1 84.375 (83.656)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [150/391]	Time 0.065 (0.070)	Data 0.003 (0.006)	Loss 0.463 (0.475)	Prec@1 82.031 (83.495)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [200/391]	Time 0.069 (0.069)	Data 0.003 (0.005)	Loss 0.355 (0.473)	Prec@1 86.719 (83.633)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:22:Num bit 8	Num grad bit 8	
06-15-23 19:22:Iter: [250/391]	Time 0.072 (0.069)	Data 0.003 (0.004)	Loss 0.459 (0.475)	Prec@1 85.156 (83.588)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [300/391]	Time 0.073 (0.069)	Data 0.005 (0.004)	Loss 0.664 (0.477)	Prec@1 78.906 (83.526)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [350/391]	Time 0.067 (0.069)	Data 0.003 (0.004)	Loss 0.441 (0.477)	Prec@1 84.375 (83.543)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Test: [0/79]	Time: 0.4686(0.4686)	Loss: 2.661(2.661)	Prec@1: 37.500(37.500)	
06-15-23 19:23:Test: [50/79]	Time: 0.0169(0.0258)	Loss: 2.639(2.701)	Prec@1: 35.938(39.400)	
06-15-23 19:23:Test: [78/79]	Time: 0.0155(0.0227)	Loss: 2.391(2.684)	Prec@1: 37.500(39.250)	
06-15-23 19:23:Step 123 * Prec@1 39.250
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [0/391]	Time 0.494 (0.494)	Data 0.437 (0.437)	Loss 0.445 (0.445)	Prec@1 82.031 (82.031)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [50/391]	Time 0.060 (0.078)	Data 0.002 (0.011)	Loss 0.391 (0.458)	Prec@1 85.938 (84.360)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [100/391]	Time 0.057 (0.072)	Data 0.002 (0.007)	Loss 0.547 (0.470)	Prec@1 84.375 (83.911)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [150/391]	Time 0.049 (0.066)	Data 0.003 (0.005)	Loss 0.521 (0.476)	Prec@1 80.469 (83.733)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [200/391]	Time 0.070 (0.067)	Data 0.003 (0.005)	Loss 0.428 (0.476)	Prec@1 85.938 (83.574)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [250/391]	Time 0.070 (0.068)	Data 0.003 (0.004)	Loss 0.633 (0.480)	Prec@1 81.250 (83.441)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [300/391]	Time 0.069 (0.068)	Data 0.002 (0.004)	Loss 0.533 (0.478)	Prec@1 80.469 (83.500)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [350/391]	Time 0.075 (0.069)	Data 0.003 (0.004)	Loss 0.312 (0.477)	Prec@1 87.500 (83.596)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Test: [0/79]	Time: 0.4638(0.4638)	Loss: 14.876(14.876)	Prec@1: 15.625(15.625)	
06-15-23 19:23:Test: [50/79]	Time: 0.0167(0.0254)	Loss: 16.949(16.793)	Prec@1: 16.406(10.983)	
06-15-23 19:23:Test: [78/79]	Time: 0.0156(0.0224)	Loss: 10.882(16.608)	Prec@1: 25.000(11.320)	
06-15-23 19:23:Step 124 * Prec@1 11.320
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [0/391]	Time 0.476 (0.476)	Data 0.435 (0.435)	Loss 0.464 (0.464)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [50/391]	Time 0.074 (0.070)	Data 0.003 (0.011)	Loss 0.563 (0.470)	Prec@1 81.250 (83.778)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [100/391]	Time 0.073 (0.072)	Data 0.003 (0.007)	Loss 0.411 (0.467)	Prec@1 89.062 (83.965)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [150/391]	Time 0.075 (0.071)	Data 0.003 (0.006)	Loss 0.344 (0.473)	Prec@1 89.062 (83.682)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [200/391]	Time 0.069 (0.071)	Data 0.003 (0.005)	Loss 0.456 (0.476)	Prec@1 82.812 (83.706)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:23:Num bit 8	Num grad bit 8	
06-15-23 19:23:Iter: [250/391]	Time 0.067 (0.070)	Data 0.003 (0.004)	Loss 0.472 (0.475)	Prec@1 85.156 (83.693)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [300/391]	Time 0.070 (0.070)	Data 0.003 (0.004)	Loss 0.468 (0.475)	Prec@1 82.812 (83.791)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [350/391]	Time 0.072 (0.071)	Data 0.002 (0.004)	Loss 0.421 (0.475)	Prec@1 82.812 (83.747)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Test: [0/79]	Time: 0.4766(0.4766)	Loss: 8.009(8.009)	Prec@1: 25.000(25.000)	
06-15-23 19:24:Test: [50/79]	Time: 0.0168(0.0258)	Loss: 9.540(9.136)	Prec@1: 17.969(19.072)	
06-15-23 19:24:Test: [78/79]	Time: 0.0154(0.0226)	Loss: 4.959(9.086)	Prec@1: 31.250(19.010)	
06-15-23 19:24:Step 125 * Prec@1 19.010
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [0/391]	Time 0.493 (0.493)	Data 0.437 (0.437)	Loss 0.455 (0.455)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [50/391]	Time 0.063 (0.079)	Data 0.002 (0.011)	Loss 0.366 (0.480)	Prec@1 87.500 (83.272)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [100/391]	Time 0.071 (0.074)	Data 0.003 (0.007)	Loss 0.458 (0.475)	Prec@1 82.031 (83.571)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [150/391]	Time 0.070 (0.073)	Data 0.003 (0.006)	Loss 0.642 (0.472)	Prec@1 74.219 (83.599)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [200/391]	Time 0.074 (0.072)	Data 0.003 (0.005)	Loss 0.422 (0.474)	Prec@1 89.844 (83.547)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [250/391]	Time 0.073 (0.072)	Data 0.003 (0.004)	Loss 0.378 (0.473)	Prec@1 89.062 (83.631)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [300/391]	Time 0.075 (0.072)	Data 0.002 (0.004)	Loss 0.514 (0.473)	Prec@1 82.812 (83.705)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [350/391]	Time 0.068 (0.072)	Data 0.003 (0.004)	Loss 0.543 (0.473)	Prec@1 81.250 (83.767)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Test: [0/79]	Time: 0.4685(0.4685)	Loss: 1.780(1.780)	Prec@1: 43.750(43.750)	
06-15-23 19:24:Test: [50/79]	Time: 0.0171(0.0258)	Loss: 1.875(2.024)	Prec@1: 44.531(38.266)	
06-15-23 19:24:Test: [78/79]	Time: 0.0155(0.0226)	Loss: 2.125(2.032)	Prec@1: 37.500(37.820)	
06-15-23 19:24:Step 126 * Prec@1 37.820
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [0/391]	Time 0.509 (0.509)	Data 0.453 (0.453)	Loss 0.499 (0.499)	Prec@1 82.812 (82.812)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [50/391]	Time 0.067 (0.069)	Data 0.003 (0.011)	Loss 0.579 (0.483)	Prec@1 81.250 (82.904)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [100/391]	Time 0.067 (0.068)	Data 0.003 (0.007)	Loss 0.538 (0.480)	Prec@1 83.594 (83.122)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [150/391]	Time 0.073 (0.069)	Data 0.002 (0.006)	Loss 0.543 (0.478)	Prec@1 78.125 (83.206)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [200/391]	Time 0.064 (0.068)	Data 0.003 (0.005)	Loss 0.473 (0.477)	Prec@1 82.031 (83.256)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [250/391]	Time 0.066 (0.068)	Data 0.003 (0.004)	Loss 0.488 (0.477)	Prec@1 85.938 (83.410)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:24:Num bit 8	Num grad bit 8	
06-15-23 19:24:Iter: [300/391]	Time 0.071 (0.068)	Data 0.002 (0.004)	Loss 0.509 (0.478)	Prec@1 83.594 (83.373)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [350/391]	Time 0.063 (0.068)	Data 0.002 (0.004)	Loss 0.576 (0.479)	Prec@1 80.469 (83.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Test: [0/79]	Time: 0.4425(0.4425)	Loss: 2.778(2.778)	Prec@1: 40.625(40.625)	
06-15-23 19:25:Test: [50/79]	Time: 0.0167(0.0250)	Loss: 2.722(2.749)	Prec@1: 45.312(45.297)	
06-15-23 19:25:Test: [78/79]	Time: 0.0152(0.0222)	Loss: 2.248(2.753)	Prec@1: 37.500(45.050)	
06-15-23 19:25:Step 127 * Prec@1 45.050
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [0/391]	Time 0.509 (0.509)	Data 0.456 (0.456)	Loss 0.415 (0.415)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [50/391]	Time 0.075 (0.077)	Data 0.004 (0.011)	Loss 0.456 (0.484)	Prec@1 85.156 (82.904)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [100/391]	Time 0.065 (0.073)	Data 0.003 (0.007)	Loss 0.604 (0.481)	Prec@1 75.000 (83.122)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [150/391]	Time 0.046 (0.070)	Data 0.002 (0.006)	Loss 0.428 (0.479)	Prec@1 88.281 (83.340)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [200/391]	Time 0.048 (0.064)	Data 0.002 (0.005)	Loss 0.455 (0.486)	Prec@1 84.375 (83.104)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [250/391]	Time 0.068 (0.062)	Data 0.003 (0.004)	Loss 0.390 (0.482)	Prec@1 87.500 (83.276)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [300/391]	Time 0.069 (0.063)	Data 0.003 (0.004)	Loss 0.514 (0.482)	Prec@1 82.031 (83.368)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [350/391]	Time 0.066 (0.064)	Data 0.003 (0.004)	Loss 0.413 (0.479)	Prec@1 89.844 (83.534)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Test: [0/79]	Time: 0.4535(0.4535)	Loss: 1.704(1.704)	Prec@1: 54.688(54.688)	
06-15-23 19:25:Test: [50/79]	Time: 0.0169(0.0255)	Loss: 1.975(2.081)	Prec@1: 47.656(46.048)	
06-15-23 19:25:Test: [78/79]	Time: 0.0157(0.0225)	Loss: 1.494(2.048)	Prec@1: 50.000(46.550)	
06-15-23 19:25:Step 128 * Prec@1 46.550
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [0/391]	Time 0.476 (0.476)	Data 0.438 (0.438)	Loss 0.538 (0.538)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [50/391]	Time 0.071 (0.071)	Data 0.002 (0.011)	Loss 0.493 (0.464)	Prec@1 81.250 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [100/391]	Time 0.072 (0.070)	Data 0.003 (0.007)	Loss 0.454 (0.468)	Prec@1 85.156 (83.826)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [150/391]	Time 0.069 (0.068)	Data 0.003 (0.005)	Loss 0.564 (0.473)	Prec@1 77.344 (83.578)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [200/391]	Time 0.056 (0.067)	Data 0.002 (0.005)	Loss 0.522 (0.478)	Prec@1 82.812 (83.396)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [250/391]	Time 0.066 (0.067)	Data 0.003 (0.004)	Loss 0.474 (0.476)	Prec@1 84.375 (83.479)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [300/391]	Time 0.069 (0.067)	Data 0.003 (0.004)	Loss 0.493 (0.478)	Prec@1 82.812 (83.456)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:25:Num bit 8	Num grad bit 8	
06-15-23 19:25:Iter: [350/391]	Time 0.061 (0.066)	Data 0.002 (0.004)	Loss 0.526 (0.478)	Prec@1 80.469 (83.442)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Test: [0/79]	Time: 0.4644(0.4644)	Loss: 3.288(3.288)	Prec@1: 41.406(41.406)	
06-15-23 19:26:Test: [50/79]	Time: 0.0168(0.0255)	Loss: 4.044(3.724)	Prec@1: 23.438(36.412)	
06-15-23 19:26:Test: [78/79]	Time: 0.0154(0.0223)	Loss: 2.817(3.698)	Prec@1: 31.250(36.290)	
06-15-23 19:26:Step 129 * Prec@1 36.290
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [0/391]	Time 0.490 (0.490)	Data 0.438 (0.438)	Loss 0.381 (0.381)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [50/391]	Time 0.069 (0.072)	Data 0.003 (0.011)	Loss 0.415 (0.468)	Prec@1 83.594 (84.053)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [100/391]	Time 0.075 (0.072)	Data 0.003 (0.007)	Loss 0.424 (0.463)	Prec@1 83.594 (84.073)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [150/391]	Time 0.070 (0.072)	Data 0.002 (0.006)	Loss 0.418 (0.460)	Prec@1 86.719 (84.282)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [200/391]	Time 0.074 (0.072)	Data 0.003 (0.005)	Loss 0.406 (0.466)	Prec@1 84.375 (84.157)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [250/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.621 (0.468)	Prec@1 78.125 (84.039)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [300/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.412 (0.470)	Prec@1 86.719 (83.999)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [350/391]	Time 0.066 (0.071)	Data 0.002 (0.004)	Loss 0.371 (0.471)	Prec@1 86.719 (83.957)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Test: [0/79]	Time: 0.4569(0.4569)	Loss: 1.580(1.580)	Prec@1: 56.250(56.250)	
06-15-23 19:26:Test: [50/79]	Time: 0.0167(0.0254)	Loss: 1.739(1.939)	Prec@1: 53.125(51.854)	
06-15-23 19:26:Test: [78/79]	Time: 0.0155(0.0223)	Loss: 1.987(1.946)	Prec@1: 50.000(51.480)	
06-15-23 19:26:Step 130 * Prec@1 51.480
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [0/391]	Time 0.472 (0.472)	Data 0.434 (0.434)	Loss 0.480 (0.480)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [50/391]	Time 0.067 (0.073)	Data 0.003 (0.011)	Loss 0.377 (0.463)	Prec@1 85.938 (84.161)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [100/391]	Time 0.070 (0.072)	Data 0.003 (0.007)	Loss 0.544 (0.460)	Prec@1 79.688 (84.244)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [150/391]	Time 0.069 (0.072)	Data 0.002 (0.006)	Loss 0.387 (0.468)	Prec@1 88.281 (83.925)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [200/391]	Time 0.070 (0.072)	Data 0.003 (0.005)	Loss 0.365 (0.467)	Prec@1 86.719 (83.866)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [250/391]	Time 0.072 (0.072)	Data 0.002 (0.004)	Loss 0.720 (0.470)	Prec@1 78.125 (83.777)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [300/391]	Time 0.070 (0.072)	Data 0.003 (0.004)	Loss 0.641 (0.471)	Prec@1 77.344 (83.749)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:26:Num bit 8	Num grad bit 8	
06-15-23 19:26:Iter: [350/391]	Time 0.064 (0.072)	Data 0.002 (0.004)	Loss 0.446 (0.470)	Prec@1 85.938 (83.792)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Test: [0/79]	Time: 0.4716(0.4716)	Loss: 2.686(2.686)	Prec@1: 42.969(42.969)	
06-15-23 19:27:Test: [50/79]	Time: 0.0168(0.0273)	Loss: 3.033(3.124)	Prec@1: 37.500(33.640)	
06-15-23 19:27:Test: [78/79]	Time: 0.0154(0.0236)	Loss: 2.863(3.115)	Prec@1: 31.250(33.920)	
06-15-23 19:27:Step 131 * Prec@1 33.920
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [0/391]	Time 0.474 (0.474)	Data 0.437 (0.437)	Loss 0.605 (0.605)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [50/391]	Time 0.074 (0.072)	Data 0.003 (0.011)	Loss 0.506 (0.463)	Prec@1 83.594 (84.084)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [100/391]	Time 0.070 (0.071)	Data 0.003 (0.007)	Loss 0.506 (0.470)	Prec@1 78.906 (83.895)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [150/391]	Time 0.073 (0.071)	Data 0.003 (0.006)	Loss 0.444 (0.473)	Prec@1 84.375 (83.723)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [200/391]	Time 0.074 (0.071)	Data 0.003 (0.005)	Loss 0.511 (0.473)	Prec@1 82.031 (83.691)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [250/391]	Time 0.069 (0.071)	Data 0.003 (0.004)	Loss 0.391 (0.470)	Prec@1 86.719 (83.861)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [300/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.528 (0.470)	Prec@1 82.031 (83.871)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [350/391]	Time 0.069 (0.071)	Data 0.003 (0.004)	Loss 0.521 (0.471)	Prec@1 82.031 (83.770)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Test: [0/79]	Time: 0.4371(0.4371)	Loss: 3.663(3.663)	Prec@1: 35.938(35.938)	
06-15-23 19:27:Test: [50/79]	Time: 0.0165(0.0249)	Loss: 3.656(3.686)	Prec@1: 38.281(37.331)	
06-15-23 19:27:Test: [78/79]	Time: 0.0154(0.0220)	Loss: 3.056(3.734)	Prec@1: 31.250(37.130)	
06-15-23 19:27:Step 132 * Prec@1 37.130
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [0/391]	Time 0.473 (0.473)	Data 0.435 (0.435)	Loss 0.467 (0.467)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [50/391]	Time 0.070 (0.069)	Data 0.003 (0.011)	Loss 0.506 (0.470)	Prec@1 82.812 (83.900)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [100/391]	Time 0.073 (0.071)	Data 0.003 (0.007)	Loss 0.377 (0.472)	Prec@1 85.938 (83.880)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [150/391]	Time 0.066 (0.069)	Data 0.003 (0.005)	Loss 0.545 (0.474)	Prec@1 81.250 (83.925)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [200/391]	Time 0.074 (0.069)	Data 0.004 (0.005)	Loss 0.557 (0.477)	Prec@1 78.906 (83.800)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [250/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.571 (0.481)	Prec@1 78.125 (83.662)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [300/391]	Time 0.040 (0.068)	Data 0.002 (0.004)	Loss 0.446 (0.478)	Prec@1 82.031 (83.739)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Num bit 8	Num grad bit 8	
06-15-23 19:27:Iter: [350/391]	Time 0.071 (0.067)	Data 0.003 (0.004)	Loss 0.404 (0.476)	Prec@1 85.156 (83.787)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:27:Test: [0/79]	Time: 0.4616(0.4616)	Loss: 0.635(0.635)	Prec@1: 77.344(77.344)	
06-15-23 19:27:Test: [50/79]	Time: 0.0170(0.0256)	Loss: 0.949(1.004)	Prec@1: 70.312(70.037)	
06-15-23 19:28:Test: [78/79]	Time: 0.0154(0.0225)	Loss: 0.554(0.992)	Prec@1: 68.750(70.080)	
06-15-23 19:28:Step 133 * Prec@1 70.080
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [0/391]	Time 0.487 (0.487)	Data 0.450 (0.450)	Loss 0.493 (0.493)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [50/391]	Time 0.068 (0.072)	Data 0.002 (0.011)	Loss 0.543 (0.457)	Prec@1 82.031 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [100/391]	Time 0.073 (0.069)	Data 0.003 (0.007)	Loss 0.500 (0.456)	Prec@1 83.594 (84.491)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [150/391]	Time 0.073 (0.070)	Data 0.003 (0.006)	Loss 0.323 (0.459)	Prec@1 92.188 (84.520)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [200/391]	Time 0.049 (0.069)	Data 0.002 (0.005)	Loss 0.469 (0.465)	Prec@1 82.031 (84.115)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [250/391]	Time 0.077 (0.069)	Data 0.003 (0.004)	Loss 0.408 (0.468)	Prec@1 85.156 (83.983)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [300/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.405 (0.470)	Prec@1 86.719 (83.833)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [350/391]	Time 0.066 (0.070)	Data 0.003 (0.004)	Loss 0.403 (0.472)	Prec@1 87.500 (83.758)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Test: [0/79]	Time: 0.4710(0.4710)	Loss: 5.266(5.266)	Prec@1: 32.812(32.812)	
06-15-23 19:28:Test: [50/79]	Time: 0.0168(0.0261)	Loss: 6.291(5.177)	Prec@1: 20.312(28.998)	
06-15-23 19:28:Test: [78/79]	Time: 0.0155(0.0230)	Loss: 5.580(5.221)	Prec@1: 43.750(28.930)	
06-15-23 19:28:Step 134 * Prec@1 28.930
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [0/391]	Time 0.473 (0.473)	Data 0.436 (0.436)	Loss 0.400 (0.400)	Prec@1 81.250 (81.250)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [50/391]	Time 0.070 (0.075)	Data 0.003 (0.011)	Loss 0.408 (0.473)	Prec@1 85.938 (83.885)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [100/391]	Time 0.070 (0.072)	Data 0.002 (0.007)	Loss 0.426 (0.472)	Prec@1 85.938 (83.841)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [150/391]	Time 0.075 (0.072)	Data 0.003 (0.006)	Loss 0.634 (0.476)	Prec@1 78.906 (83.645)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [200/391]	Time 0.069 (0.072)	Data 0.003 (0.005)	Loss 0.604 (0.473)	Prec@1 78.125 (83.815)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [250/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.320 (0.477)	Prec@1 87.500 (83.703)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [300/391]	Time 0.073 (0.072)	Data 0.003 (0.004)	Loss 0.480 (0.477)	Prec@1 84.375 (83.703)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Num bit 8	Num grad bit 8	
06-15-23 19:28:Iter: [350/391]	Time 0.072 (0.072)	Data 0.003 (0.004)	Loss 0.429 (0.476)	Prec@1 86.719 (83.778)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:28:Test: [0/79]	Time: 0.4757(0.4757)	Loss: 2.797(2.797)	Prec@1: 51.562(51.562)	
06-15-23 19:28:Test: [50/79]	Time: 0.0168(0.0257)	Loss: 3.221(3.191)	Prec@1: 46.094(46.906)	
06-15-23 19:28:Test: [78/79]	Time: 0.0153(0.0225)	Loss: 1.425(3.198)	Prec@1: 75.000(47.090)	
06-15-23 19:29:Step 135 * Prec@1 47.090
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [0/391]	Time 0.487 (0.487)	Data 0.449 (0.449)	Loss 0.491 (0.491)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [50/391]	Time 0.077 (0.069)	Data 0.002 (0.011)	Loss 0.380 (0.441)	Prec@1 85.156 (85.417)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [100/391]	Time 0.070 (0.071)	Data 0.003 (0.007)	Loss 0.597 (0.463)	Prec@1 81.250 (84.476)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [150/391]	Time 0.075 (0.072)	Data 0.003 (0.006)	Loss 0.462 (0.465)	Prec@1 85.938 (84.261)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [200/391]	Time 0.073 (0.071)	Data 0.003 (0.005)	Loss 0.377 (0.464)	Prec@1 86.719 (84.153)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [250/391]	Time 0.063 (0.071)	Data 0.002 (0.005)	Loss 0.565 (0.467)	Prec@1 78.906 (84.008)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [300/391]	Time 0.068 (0.071)	Data 0.003 (0.004)	Loss 0.497 (0.470)	Prec@1 82.031 (83.965)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [350/391]	Time 0.068 (0.071)	Data 0.003 (0.004)	Loss 0.340 (0.472)	Prec@1 88.281 (83.850)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Test: [0/79]	Time: 0.4658(0.4658)	Loss: 2.024(2.024)	Prec@1: 49.219(49.219)	
06-15-23 19:29:Test: [50/79]	Time: 0.0170(0.0257)	Loss: 2.206(2.383)	Prec@1: 42.969(39.553)	
06-15-23 19:29:Test: [78/79]	Time: 0.0155(0.0226)	Loss: 1.906(2.394)	Prec@1: 50.000(39.370)	
06-15-23 19:29:Step 136 * Prec@1 39.370
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [0/391]	Time 0.448 (0.448)	Data 0.411 (0.411)	Loss 0.488 (0.488)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [50/391]	Time 0.071 (0.075)	Data 0.003 (0.011)	Loss 0.481 (0.474)	Prec@1 82.812 (83.931)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [100/391]	Time 0.058 (0.070)	Data 0.002 (0.007)	Loss 0.390 (0.471)	Prec@1 85.938 (84.081)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [150/391]	Time 0.059 (0.070)	Data 0.002 (0.005)	Loss 0.496 (0.470)	Prec@1 83.594 (83.940)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [200/391]	Time 0.073 (0.069)	Data 0.003 (0.005)	Loss 0.550 (0.472)	Prec@1 83.594 (83.800)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [250/391]	Time 0.066 (0.068)	Data 0.003 (0.004)	Loss 0.406 (0.470)	Prec@1 89.844 (83.936)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [300/391]	Time 0.062 (0.068)	Data 0.002 (0.004)	Loss 0.499 (0.469)	Prec@1 84.375 (83.986)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [350/391]	Time 0.059 (0.067)	Data 0.002 (0.004)	Loss 0.360 (0.470)	Prec@1 86.719 (83.943)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:29:Test: [0/79]	Time: 0.4646(0.4646)	Loss: 4.689(4.689)	Prec@1: 29.688(29.688)	
06-15-23 19:29:Test: [50/79]	Time: 0.0168(0.0290)	Loss: 4.679(4.760)	Prec@1: 34.375(31.526)	
06-15-23 19:29:Test: [78/79]	Time: 0.0153(0.0248)	Loss: 6.851(4.772)	Prec@1: 0.000(31.230)	
06-15-23 19:29:Step 137 * Prec@1 31.230
06-15-23 19:29:Num bit 8	Num grad bit 8	
06-15-23 19:29:Iter: [0/391]	Time 0.477 (0.477)	Data 0.440 (0.440)	Loss 0.407 (0.407)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [50/391]	Time 0.068 (0.070)	Data 0.003 (0.011)	Loss 0.446 (0.472)	Prec@1 85.156 (83.333)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [100/391]	Time 0.070 (0.070)	Data 0.003 (0.007)	Loss 0.442 (0.461)	Prec@1 87.500 (83.826)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [150/391]	Time 0.075 (0.069)	Data 0.003 (0.006)	Loss 0.324 (0.465)	Prec@1 89.844 (83.739)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [200/391]	Time 0.073 (0.069)	Data 0.003 (0.005)	Loss 0.354 (0.467)	Prec@1 89.062 (83.846)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [250/391]	Time 0.070 (0.069)	Data 0.003 (0.004)	Loss 0.451 (0.469)	Prec@1 85.156 (83.784)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [300/391]	Time 0.066 (0.069)	Data 0.003 (0.004)	Loss 0.421 (0.470)	Prec@1 87.500 (83.807)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [350/391]	Time 0.067 (0.069)	Data 0.003 (0.004)	Loss 0.602 (0.470)	Prec@1 82.031 (83.850)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Test: [0/79]	Time: 0.4475(0.4475)	Loss: 2.668(2.668)	Prec@1: 42.188(42.188)	
06-15-23 19:30:Test: [50/79]	Time: 0.0172(0.0254)	Loss: 3.212(3.140)	Prec@1: 37.500(35.539)	
06-15-23 19:30:Test: [78/79]	Time: 0.0156(0.0224)	Loss: 1.480(3.111)	Prec@1: 62.500(35.900)	
06-15-23 19:30:Step 138 * Prec@1 35.900
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [0/391]	Time 0.482 (0.482)	Data 0.444 (0.444)	Loss 0.498 (0.498)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [50/391]	Time 0.069 (0.072)	Data 0.002 (0.011)	Loss 0.470 (0.466)	Prec@1 82.812 (84.298)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [100/391]	Time 0.069 (0.072)	Data 0.002 (0.007)	Loss 0.487 (0.459)	Prec@1 84.375 (84.251)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [150/391]	Time 0.071 (0.072)	Data 0.002 (0.006)	Loss 0.540 (0.461)	Prec@1 82.031 (84.225)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [200/391]	Time 0.067 (0.072)	Data 0.002 (0.005)	Loss 0.489 (0.464)	Prec@1 82.031 (84.064)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [250/391]	Time 0.076 (0.072)	Data 0.003 (0.004)	Loss 0.391 (0.466)	Prec@1 88.281 (83.970)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [300/391]	Time 0.070 (0.072)	Data 0.003 (0.004)	Loss 0.357 (0.469)	Prec@1 87.500 (83.879)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [350/391]	Time 0.074 (0.072)	Data 0.003 (0.004)	Loss 0.475 (0.471)	Prec@1 82.812 (83.745)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:30:Test: [0/79]	Time: 0.4578(0.4578)	Loss: 3.934(3.934)	Prec@1: 36.719(36.719)	
06-15-23 19:30:Test: [50/79]	Time: 0.0169(0.0256)	Loss: 4.727(3.798)	Prec@1: 24.219(37.776)	
06-15-23 19:30:Test: [78/79]	Time: 0.0156(0.0225)	Loss: 3.559(3.839)	Prec@1: 50.000(37.440)	
06-15-23 19:30:Step 139 * Prec@1 37.440
06-15-23 19:30:Num bit 8	Num grad bit 8	
06-15-23 19:30:Iter: [0/391]	Time 0.492 (0.492)	Data 0.440 (0.440)	Loss 0.498 (0.498)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [50/391]	Time 0.058 (0.070)	Data 0.002 (0.011)	Loss 0.439 (0.450)	Prec@1 84.375 (84.835)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [100/391]	Time 0.071 (0.069)	Data 0.003 (0.007)	Loss 0.537 (0.461)	Prec@1 85.156 (84.360)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [150/391]	Time 0.075 (0.070)	Data 0.003 (0.006)	Loss 0.381 (0.463)	Prec@1 90.625 (84.272)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [200/391]	Time 0.070 (0.070)	Data 0.003 (0.005)	Loss 0.439 (0.461)	Prec@1 84.375 (84.348)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [250/391]	Time 0.067 (0.069)	Data 0.003 (0.004)	Loss 0.435 (0.463)	Prec@1 84.375 (84.282)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [300/391]	Time 0.071 (0.069)	Data 0.003 (0.004)	Loss 0.458 (0.460)	Prec@1 85.156 (84.349)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [350/391]	Time 0.070 (0.067)	Data 0.003 (0.004)	Loss 0.575 (0.463)	Prec@1 79.688 (84.197)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Test: [0/79]	Time: 0.4607(0.4607)	Loss: 4.727(4.727)	Prec@1: 11.719(11.719)	
06-15-23 19:31:Test: [50/79]	Time: 0.0167(0.0253)	Loss: 4.651(4.660)	Prec@1: 12.500(14.568)	
06-15-23 19:31:Test: [78/79]	Time: 0.0153(0.0222)	Loss: 4.848(4.657)	Prec@1: 6.250(14.860)	
06-15-23 19:31:Step 140 * Prec@1 14.860
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [0/391]	Time 0.514 (0.514)	Data 0.462 (0.462)	Loss 0.393 (0.393)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [50/391]	Time 0.068 (0.072)	Data 0.003 (0.011)	Loss 0.499 (0.449)	Prec@1 82.812 (84.666)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [100/391]	Time 0.071 (0.072)	Data 0.003 (0.007)	Loss 0.492 (0.459)	Prec@1 81.250 (84.127)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [150/391]	Time 0.062 (0.072)	Data 0.002 (0.006)	Loss 0.448 (0.467)	Prec@1 84.375 (83.971)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [200/391]	Time 0.070 (0.072)	Data 0.003 (0.005)	Loss 0.429 (0.462)	Prec@1 85.156 (84.161)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [250/391]	Time 0.069 (0.072)	Data 0.002 (0.005)	Loss 0.427 (0.463)	Prec@1 84.375 (84.132)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [300/391]	Time 0.060 (0.071)	Data 0.003 (0.004)	Loss 0.588 (0.467)	Prec@1 77.344 (83.916)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [350/391]	Time 0.032 (0.069)	Data 0.001 (0.004)	Loss 0.477 (0.470)	Prec@1 82.812 (83.823)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Test: [0/79]	Time: 0.4749(0.4749)	Loss: 8.048(8.048)	Prec@1: 20.312(20.312)	
06-15-23 19:31:Test: [50/79]	Time: 0.0166(0.0300)	Loss: 8.002(8.650)	Prec@1: 18.750(17.172)	
06-15-23 19:31:Test: [78/79]	Time: 0.0153(0.0252)	Loss: 9.282(8.569)	Prec@1: 18.750(17.370)	
06-15-23 19:31:Step 141 * Prec@1 17.370
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [0/391]	Time 0.475 (0.475)	Data 0.434 (0.434)	Loss 0.439 (0.439)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:31:Num bit 8	Num grad bit 8	
06-15-23 19:31:Iter: [50/391]	Time 0.070 (0.073)	Data 0.003 (0.011)	Loss 0.487 (0.467)	Prec@1 84.375 (84.191)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [100/391]	Time 0.067 (0.070)	Data 0.002 (0.007)	Loss 0.508 (0.472)	Prec@1 82.812 (83.748)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [150/391]	Time 0.077 (0.070)	Data 0.003 (0.006)	Loss 0.396 (0.470)	Prec@1 89.062 (83.868)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [200/391]	Time 0.072 (0.070)	Data 0.003 (0.005)	Loss 0.480 (0.467)	Prec@1 85.156 (83.990)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [250/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.408 (0.470)	Prec@1 85.938 (83.936)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [300/391]	Time 0.073 (0.069)	Data 0.003 (0.004)	Loss 0.371 (0.469)	Prec@1 86.719 (83.913)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [350/391]	Time 0.071 (0.069)	Data 0.002 (0.004)	Loss 0.528 (0.471)	Prec@1 81.250 (83.790)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Test: [0/79]	Time: 0.4664(0.4664)	Loss: 3.998(3.998)	Prec@1: 28.125(28.125)	
06-15-23 19:32:Test: [50/79]	Time: 0.0167(0.0257)	Loss: 4.310(4.057)	Prec@1: 27.344(28.968)	
06-15-23 19:32:Test: [78/79]	Time: 0.0155(0.0226)	Loss: 3.329(4.074)	Prec@1: 43.750(29.320)	
06-15-23 19:32:Step 142 * Prec@1 29.320
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [0/391]	Time 0.473 (0.473)	Data 0.435 (0.435)	Loss 0.548 (0.548)	Prec@1 82.031 (82.031)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [50/391]	Time 0.072 (0.060)	Data 0.003 (0.011)	Loss 0.540 (0.472)	Prec@1 80.469 (83.487)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [100/391]	Time 0.070 (0.065)	Data 0.003 (0.007)	Loss 0.396 (0.477)	Prec@1 86.719 (83.400)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [150/391]	Time 0.072 (0.067)	Data 0.002 (0.005)	Loss 0.442 (0.474)	Prec@1 85.156 (83.713)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [200/391]	Time 0.064 (0.068)	Data 0.002 (0.005)	Loss 0.384 (0.469)	Prec@1 87.500 (83.924)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [250/391]	Time 0.067 (0.068)	Data 0.003 (0.004)	Loss 0.489 (0.471)	Prec@1 83.594 (83.880)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [300/391]	Time 0.068 (0.068)	Data 0.003 (0.004)	Loss 0.438 (0.470)	Prec@1 86.719 (83.965)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [350/391]	Time 0.072 (0.068)	Data 0.002 (0.004)	Loss 0.388 (0.472)	Prec@1 89.844 (83.881)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Test: [0/79]	Time: 0.4764(0.4764)	Loss: 5.142(5.142)	Prec@1: 12.500(12.500)	
06-15-23 19:32:Test: [50/79]	Time: 0.0168(0.0263)	Loss: 5.392(5.539)	Prec@1: 11.719(12.745)	
06-15-23 19:32:Test: [78/79]	Time: 0.0156(0.0230)	Loss: 6.064(5.547)	Prec@1: 12.500(12.800)	
06-15-23 19:32:Step 143 * Prec@1 12.800
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [0/391]	Time 0.483 (0.483)	Data 0.437 (0.437)	Loss 0.550 (0.550)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [50/391]	Time 0.037 (0.053)	Data 0.001 (0.011)	Loss 0.443 (0.469)	Prec@1 86.719 (83.992)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:32:Num bit 8	Num grad bit 8	
06-15-23 19:32:Iter: [100/391]	Time 0.041 (0.047)	Data 0.002 (0.006)	Loss 0.442 (0.473)	Prec@1 84.375 (83.733)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [150/391]	Time 0.071 (0.052)	Data 0.002 (0.005)	Loss 0.515 (0.479)	Prec@1 86.719 (83.516)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [200/391]	Time 0.044 (0.055)	Data 0.002 (0.004)	Loss 0.405 (0.470)	Prec@1 87.500 (83.776)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [250/391]	Time 0.048 (0.053)	Data 0.002 (0.004)	Loss 0.409 (0.469)	Prec@1 85.156 (83.805)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [300/391]	Time 0.074 (0.055)	Data 0.003 (0.004)	Loss 0.583 (0.471)	Prec@1 84.375 (83.833)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [350/391]	Time 0.045 (0.054)	Data 0.002 (0.004)	Loss 0.406 (0.472)	Prec@1 84.375 (83.821)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Test: [0/79]	Time: 0.4802(0.4802)	Loss: 0.982(0.982)	Prec@1: 67.969(67.969)	
06-15-23 19:33:Test: [50/79]	Time: 0.0169(0.0259)	Loss: 1.268(1.135)	Prec@1: 60.156(63.266)	
06-15-23 19:33:Test: [78/79]	Time: 0.0154(0.0227)	Loss: 0.694(1.122)	Prec@1: 81.250(63.660)	
06-15-23 19:33:Step 144 * Prec@1 63.660
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [0/391]	Time 0.507 (0.507)	Data 0.470 (0.470)	Loss 0.290 (0.290)	Prec@1 90.625 (90.625)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [50/391]	Time 0.075 (0.072)	Data 0.003 (0.012)	Loss 0.555 (0.466)	Prec@1 82.031 (84.176)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [100/391]	Time 0.071 (0.072)	Data 0.003 (0.007)	Loss 0.528 (0.469)	Prec@1 82.812 (84.112)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [150/391]	Time 0.072 (0.071)	Data 0.003 (0.006)	Loss 0.563 (0.476)	Prec@1 82.812 (83.852)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [200/391]	Time 0.069 (0.070)	Data 0.003 (0.005)	Loss 0.410 (0.476)	Prec@1 81.250 (83.664)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [250/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.369 (0.476)	Prec@1 87.500 (83.675)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [300/391]	Time 0.056 (0.070)	Data 0.003 (0.004)	Loss 0.534 (0.474)	Prec@1 79.688 (83.749)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [350/391]	Time 0.069 (0.070)	Data 0.003 (0.004)	Loss 0.351 (0.472)	Prec@1 90.625 (83.814)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Test: [0/79]	Time: 0.4663(0.4663)	Loss: 4.864(4.864)	Prec@1: 27.344(27.344)	
06-15-23 19:33:Test: [50/79]	Time: 0.0169(0.0256)	Loss: 5.831(5.134)	Prec@1: 23.438(28.814)	
06-15-23 19:33:Test: [78/79]	Time: 0.0155(0.0225)	Loss: 6.016(5.106)	Prec@1: 31.250(28.750)	
06-15-23 19:33:Step 145 * Prec@1 28.750
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [0/391]	Time 0.485 (0.485)	Data 0.448 (0.448)	Loss 0.623 (0.623)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [50/391]	Time 0.066 (0.068)	Data 0.003 (0.011)	Loss 0.360 (0.475)	Prec@1 88.281 (84.038)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [100/391]	Time 0.067 (0.067)	Data 0.003 (0.007)	Loss 0.488 (0.464)	Prec@1 81.250 (84.011)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [150/391]	Time 0.064 (0.066)	Data 0.002 (0.005)	Loss 0.429 (0.469)	Prec@1 84.375 (83.759)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:33:Num bit 8	Num grad bit 8	
06-15-23 19:33:Iter: [200/391]	Time 0.067 (0.066)	Data 0.003 (0.005)	Loss 0.432 (0.472)	Prec@1 87.500 (83.660)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [250/391]	Time 0.076 (0.066)	Data 0.003 (0.004)	Loss 0.508 (0.471)	Prec@1 85.156 (83.743)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [300/391]	Time 0.069 (0.067)	Data 0.002 (0.004)	Loss 0.615 (0.474)	Prec@1 82.031 (83.646)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [350/391]	Time 0.069 (0.067)	Data 0.003 (0.004)	Loss 0.373 (0.473)	Prec@1 85.938 (83.643)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Test: [0/79]	Time: 0.4683(0.4683)	Loss: 2.945(2.945)	Prec@1: 32.812(32.812)	
06-15-23 19:34:Test: [50/79]	Time: 0.0169(0.0267)	Loss: 3.175(3.276)	Prec@1: 32.812(29.289)	
06-15-23 19:34:Test: [78/79]	Time: 0.0154(0.0231)	Loss: 3.112(3.261)	Prec@1: 37.500(29.770)	
06-15-23 19:34:Step 146 * Prec@1 29.770
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [0/391]	Time 0.501 (0.501)	Data 0.449 (0.449)	Loss 0.512 (0.512)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [50/391]	Time 0.050 (0.057)	Data 0.002 (0.011)	Loss 0.440 (0.492)	Prec@1 82.812 (83.226)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [100/391]	Time 0.048 (0.053)	Data 0.002 (0.006)	Loss 0.577 (0.488)	Prec@1 84.375 (83.308)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [150/391]	Time 0.074 (0.058)	Data 0.003 (0.005)	Loss 0.578 (0.478)	Prec@1 80.469 (83.656)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [200/391]	Time 0.069 (0.061)	Data 0.003 (0.004)	Loss 0.308 (0.479)	Prec@1 91.406 (83.582)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [250/391]	Time 0.048 (0.061)	Data 0.002 (0.004)	Loss 0.426 (0.478)	Prec@1 85.156 (83.612)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [300/391]	Time 0.074 (0.061)	Data 0.002 (0.004)	Loss 0.480 (0.478)	Prec@1 82.812 (83.573)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [350/391]	Time 0.060 (0.061)	Data 0.003 (0.004)	Loss 0.359 (0.476)	Prec@1 86.719 (83.667)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Test: [0/79]	Time: 0.4730(0.4730)	Loss: 2.482(2.482)	Prec@1: 42.188(42.188)	
06-15-23 19:34:Test: [50/79]	Time: 0.0169(0.0261)	Loss: 2.569(2.787)	Prec@1: 36.719(35.233)	
06-15-23 19:34:Test: [78/79]	Time: 0.0153(0.0228)	Loss: 1.441(2.792)	Prec@1: 50.000(35.220)	
06-15-23 19:34:Step 147 * Prec@1 35.220
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [0/391]	Time 0.468 (0.468)	Data 0.430 (0.430)	Loss 0.401 (0.401)	Prec@1 87.500 (87.500)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [50/391]	Time 0.074 (0.072)	Data 0.003 (0.011)	Loss 0.701 (0.476)	Prec@1 79.688 (83.839)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [100/391]	Time 0.069 (0.072)	Data 0.002 (0.007)	Loss 0.427 (0.460)	Prec@1 85.938 (84.305)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [150/391]	Time 0.047 (0.069)	Data 0.002 (0.005)	Loss 0.442 (0.472)	Prec@1 87.500 (83.837)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [200/391]	Time 0.057 (0.065)	Data 0.002 (0.005)	Loss 0.641 (0.478)	Prec@1 78.906 (83.633)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [250/391]	Time 0.056 (0.062)	Data 0.002 (0.004)	Loss 0.470 (0.473)	Prec@1 82.812 (83.793)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:34:Num bit 8	Num grad bit 8	
06-15-23 19:34:Iter: [300/391]	Time 0.068 (0.062)	Data 0.003 (0.004)	Loss 0.449 (0.472)	Prec@1 83.594 (83.765)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [350/391]	Time 0.069 (0.063)	Data 0.002 (0.004)	Loss 0.442 (0.471)	Prec@1 86.719 (83.859)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Test: [0/79]	Time: 0.4619(0.4619)	Loss: 4.708(4.708)	Prec@1: 39.062(39.062)	
06-15-23 19:35:Test: [50/79]	Time: 0.0166(0.0273)	Loss: 5.710(5.046)	Prec@1: 36.719(37.914)	
06-15-23 19:35:Test: [78/79]	Time: 0.0154(0.0235)	Loss: 4.371(4.953)	Prec@1: 43.750(38.370)	
06-15-23 19:35:Step 148 * Prec@1 38.370
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [0/391]	Time 0.507 (0.507)	Data 0.456 (0.456)	Loss 0.426 (0.426)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [50/391]	Time 0.074 (0.078)	Data 0.003 (0.011)	Loss 0.508 (0.475)	Prec@1 83.594 (83.808)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [100/391]	Time 0.070 (0.075)	Data 0.003 (0.007)	Loss 0.606 (0.477)	Prec@1 77.344 (83.586)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [150/391]	Time 0.069 (0.074)	Data 0.003 (0.006)	Loss 0.461 (0.475)	Prec@1 81.250 (83.583)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [200/391]	Time 0.058 (0.069)	Data 0.002 (0.005)	Loss 0.482 (0.471)	Prec@1 86.719 (83.718)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [250/391]	Time 0.057 (0.067)	Data 0.002 (0.004)	Loss 0.629 (0.470)	Prec@1 78.125 (83.759)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [300/391]	Time 0.060 (0.065)	Data 0.002 (0.004)	Loss 0.370 (0.470)	Prec@1 85.938 (83.835)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [350/391]	Time 0.062 (0.064)	Data 0.002 (0.004)	Loss 0.531 (0.473)	Prec@1 81.250 (83.747)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Test: [0/79]	Time: 0.4613(0.4613)	Loss: 3.383(3.383)	Prec@1: 46.875(46.875)	
06-15-23 19:35:Test: [50/79]	Time: 0.0167(0.0253)	Loss: 4.231(4.352)	Prec@1: 35.938(34.666)	
06-15-23 19:35:Test: [78/79]	Time: 0.0152(0.0222)	Loss: 3.157(4.296)	Prec@1: 43.750(35.210)	
06-15-23 19:35:Step 149 * Prec@1 35.210
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [0/391]	Time 0.513 (0.513)	Data 0.475 (0.475)	Loss 0.468 (0.468)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [50/391]	Time 0.057 (0.062)	Data 0.002 (0.011)	Loss 0.518 (0.451)	Prec@1 85.938 (84.620)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [100/391]	Time 0.043 (0.056)	Data 0.002 (0.007)	Loss 0.440 (0.459)	Prec@1 85.156 (84.274)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [150/391]	Time 0.078 (0.058)	Data 0.003 (0.005)	Loss 0.570 (0.456)	Prec@1 79.688 (84.416)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [200/391]	Time 0.070 (0.062)	Data 0.003 (0.005)	Loss 0.500 (0.461)	Prec@1 82.812 (84.231)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [250/391]	Time 0.070 (0.064)	Data 0.002 (0.004)	Loss 0.479 (0.458)	Prec@1 82.031 (84.331)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [300/391]	Time 0.072 (0.065)	Data 0.003 (0.004)	Loss 0.442 (0.458)	Prec@1 86.719 (84.243)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Num bit 8	Num grad bit 8	
06-15-23 19:35:Iter: [350/391]	Time 0.064 (0.065)	Data 0.003 (0.004)	Loss 0.453 (0.460)	Prec@1 82.812 (84.161)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:35:Test: [0/79]	Time: 0.4634(0.4634)	Loss: 2.651(2.651)	Prec@1: 50.000(50.000)	
06-15-23 19:35:Test: [50/79]	Time: 0.0165(0.0255)	Loss: 2.951(3.207)	Prec@1: 46.875(42.279)	
06-15-23 19:36:Test: [78/79]	Time: 0.0154(0.0224)	Loss: 2.174(3.172)	Prec@1: 50.000(42.630)	
06-15-23 19:36:Step 150 * Prec@1 42.630
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [0/391]	Time 0.475 (0.475)	Data 0.436 (0.436)	Loss 0.475 (0.475)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [50/391]	Time 0.071 (0.072)	Data 0.003 (0.011)	Loss 0.521 (0.457)	Prec@1 85.938 (84.406)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [100/391]	Time 0.070 (0.070)	Data 0.003 (0.007)	Loss 0.361 (0.452)	Prec@1 88.281 (84.390)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [150/391]	Time 0.071 (0.070)	Data 0.002 (0.005)	Loss 0.506 (0.454)	Prec@1 82.031 (84.318)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [200/391]	Time 0.070 (0.071)	Data 0.003 (0.005)	Loss 0.480 (0.457)	Prec@1 83.594 (84.239)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [250/391]	Time 0.072 (0.071)	Data 0.002 (0.004)	Loss 0.474 (0.459)	Prec@1 85.156 (84.120)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [300/391]	Time 0.076 (0.071)	Data 0.003 (0.004)	Loss 0.382 (0.460)	Prec@1 85.938 (84.224)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [350/391]	Time 0.066 (0.071)	Data 0.002 (0.004)	Loss 0.353 (0.461)	Prec@1 89.844 (84.244)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Test: [0/79]	Time: 0.4676(0.4676)	Loss: 16.451(16.451)	Prec@1: 15.625(15.625)	
06-15-23 19:36:Test: [50/79]	Time: 0.0167(0.0254)	Loss: 16.597(17.026)	Prec@1: 9.375(14.982)	
06-15-23 19:36:Test: [78/79]	Time: 0.0153(0.0223)	Loss: 9.843(16.982)	Prec@1: 25.000(15.080)	
06-15-23 19:36:Step 151 * Prec@1 15.080
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [0/391]	Time 0.487 (0.487)	Data 0.448 (0.448)	Loss 0.399 (0.399)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [50/391]	Time 0.066 (0.075)	Data 0.002 (0.011)	Loss 0.564 (0.455)	Prec@1 78.125 (84.099)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [100/391]	Time 0.057 (0.069)	Data 0.002 (0.007)	Loss 0.505 (0.449)	Prec@1 83.594 (84.290)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [150/391]	Time 0.069 (0.065)	Data 0.003 (0.005)	Loss 0.387 (0.449)	Prec@1 84.375 (84.396)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [200/391]	Time 0.068 (0.065)	Data 0.003 (0.005)	Loss 0.458 (0.455)	Prec@1 84.375 (84.247)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [250/391]	Time 0.068 (0.064)	Data 0.002 (0.004)	Loss 0.398 (0.453)	Prec@1 83.594 (84.366)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [300/391]	Time 0.066 (0.065)	Data 0.002 (0.004)	Loss 0.403 (0.456)	Prec@1 88.281 (84.346)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [350/391]	Time 0.074 (0.066)	Data 0.003 (0.004)	Loss 0.549 (0.457)	Prec@1 81.250 (84.328)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:36:Test: [0/79]	Time: 0.4423(0.4423)	Loss: 10.499(10.499)	Prec@1: 13.281(13.281)	
06-15-23 19:36:Test: [50/79]	Time: 0.0166(0.0250)	Loss: 10.584(11.391)	Prec@1: 15.625(10.018)	
06-15-23 19:36:Test: [78/79]	Time: 0.0155(0.0221)	Loss: 8.881(11.344)	Prec@1: 18.750(10.280)	
06-15-23 19:36:Step 152 * Prec@1 10.280
06-15-23 19:36:Num bit 8	Num grad bit 8	
06-15-23 19:36:Iter: [0/391]	Time 0.503 (0.503)	Data 0.464 (0.464)	Loss 0.401 (0.401)	Prec@1 87.500 (87.500)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [50/391]	Time 0.070 (0.071)	Data 0.003 (0.011)	Loss 0.439 (0.460)	Prec@1 84.375 (84.436)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [100/391]	Time 0.073 (0.069)	Data 0.003 (0.007)	Loss 0.393 (0.451)	Prec@1 89.062 (84.623)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [150/391]	Time 0.070 (0.070)	Data 0.002 (0.006)	Loss 0.489 (0.456)	Prec@1 80.469 (84.391)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [200/391]	Time 0.069 (0.070)	Data 0.003 (0.005)	Loss 0.519 (0.451)	Prec@1 84.375 (84.612)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [250/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.652 (0.454)	Prec@1 78.906 (84.518)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [300/391]	Time 0.044 (0.067)	Data 0.002 (0.004)	Loss 0.461 (0.454)	Prec@1 86.719 (84.492)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [350/391]	Time 0.050 (0.064)	Data 0.002 (0.004)	Loss 0.434 (0.460)	Prec@1 84.375 (84.328)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Test: [0/79]	Time: 0.4651(0.4651)	Loss: 4.470(4.470)	Prec@1: 32.031(32.031)	
06-15-23 19:37:Test: [50/79]	Time: 0.0167(0.0255)	Loss: 5.577(5.220)	Prec@1: 21.875(21.844)	
06-15-23 19:37:Test: [78/79]	Time: 0.0153(0.0225)	Loss: 4.977(5.177)	Prec@1: 12.500(21.870)	
06-15-23 19:37:Step 153 * Prec@1 21.870
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [0/391]	Time 0.520 (0.520)	Data 0.460 (0.460)	Loss 0.530 (0.530)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [50/391]	Time 0.067 (0.075)	Data 0.003 (0.012)	Loss 0.470 (0.458)	Prec@1 82.812 (84.130)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [100/391]	Time 0.068 (0.071)	Data 0.002 (0.007)	Loss 0.499 (0.454)	Prec@1 84.375 (84.514)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [150/391]	Time 0.078 (0.070)	Data 0.003 (0.006)	Loss 0.297 (0.459)	Prec@1 88.281 (84.385)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [200/391]	Time 0.075 (0.069)	Data 0.002 (0.005)	Loss 0.561 (0.462)	Prec@1 80.469 (84.188)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [250/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.581 (0.458)	Prec@1 80.469 (84.322)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [300/391]	Time 0.069 (0.070)	Data 0.003 (0.004)	Loss 0.438 (0.456)	Prec@1 88.281 (84.357)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [350/391]	Time 0.043 (0.069)	Data 0.002 (0.004)	Loss 0.382 (0.455)	Prec@1 89.062 (84.333)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Test: [0/79]	Time: 0.4789(0.4789)	Loss: 2.845(2.845)	Prec@1: 31.250(31.250)	
06-15-23 19:37:Test: [50/79]	Time: 0.0166(0.0256)	Loss: 2.856(3.060)	Prec@1: 35.156(33.915)	
06-15-23 19:37:Test: [78/79]	Time: 0.0153(0.0224)	Loss: 2.111(3.053)	Prec@1: 37.500(33.920)	
06-15-23 19:37:Step 154 * Prec@1 33.920
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [0/391]	Time 0.491 (0.491)	Data 0.439 (0.439)	Loss 0.553 (0.553)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [50/391]	Time 0.072 (0.069)	Data 0.003 (0.011)	Loss 0.471 (0.458)	Prec@1 84.375 (84.528)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:37:Num bit 8	Num grad bit 8	
06-15-23 19:37:Iter: [100/391]	Time 0.070 (0.070)	Data 0.003 (0.007)	Loss 0.478 (0.460)	Prec@1 83.594 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [150/391]	Time 0.069 (0.070)	Data 0.003 (0.005)	Loss 0.415 (0.455)	Prec@1 85.156 (84.597)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [200/391]	Time 0.066 (0.070)	Data 0.003 (0.005)	Loss 0.535 (0.455)	Prec@1 83.594 (84.562)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [250/391]	Time 0.073 (0.071)	Data 0.003 (0.004)	Loss 0.500 (0.459)	Prec@1 82.031 (84.313)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [300/391]	Time 0.073 (0.071)	Data 0.003 (0.004)	Loss 0.293 (0.460)	Prec@1 91.406 (84.289)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [350/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.527 (0.460)	Prec@1 81.250 (84.257)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Test: [0/79]	Time: 0.4595(0.4595)	Loss: 5.186(5.186)	Prec@1: 33.594(33.594)	
06-15-23 19:38:Test: [50/79]	Time: 0.0169(0.0255)	Loss: 4.896(5.428)	Prec@1: 33.594(28.339)	
06-15-23 19:38:Test: [78/79]	Time: 0.0154(0.0223)	Loss: 3.523(5.425)	Prec@1: 43.750(28.620)	
06-15-23 19:38:Step 155 * Prec@1 28.620
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [0/391]	Time 0.483 (0.483)	Data 0.436 (0.436)	Loss 0.407 (0.407)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [50/391]	Time 0.074 (0.070)	Data 0.003 (0.011)	Loss 0.330 (0.465)	Prec@1 90.625 (84.007)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [100/391]	Time 0.051 (0.062)	Data 0.002 (0.007)	Loss 0.368 (0.460)	Prec@1 90.625 (84.259)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [150/391]	Time 0.045 (0.056)	Data 0.002 (0.005)	Loss 0.399 (0.460)	Prec@1 86.719 (84.173)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [200/391]	Time 0.051 (0.053)	Data 0.002 (0.004)	Loss 0.345 (0.456)	Prec@1 89.844 (84.321)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [250/391]	Time 0.055 (0.055)	Data 0.003 (0.004)	Loss 0.355 (0.457)	Prec@1 86.719 (84.288)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [300/391]	Time 0.069 (0.057)	Data 0.003 (0.004)	Loss 0.453 (0.458)	Prec@1 84.375 (84.232)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [350/391]	Time 0.070 (0.059)	Data 0.002 (0.004)	Loss 0.447 (0.460)	Prec@1 85.156 (84.224)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Test: [0/79]	Time: 0.4579(0.4579)	Loss: 6.253(6.253)	Prec@1: 25.781(25.781)	
06-15-23 19:38:Test: [50/79]	Time: 0.0168(0.0254)	Loss: 5.885(5.423)	Prec@1: 35.156(36.183)	
06-15-23 19:38:Test: [78/79]	Time: 0.0153(0.0223)	Loss: 3.419(5.492)	Prec@1: 37.500(36.140)	
06-15-23 19:38:Step 156 * Prec@1 36.140
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [0/391]	Time 0.501 (0.501)	Data 0.463 (0.463)	Loss 0.493 (0.493)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [50/391]	Time 0.046 (0.052)	Data 0.002 (0.011)	Loss 0.439 (0.456)	Prec@1 84.375 (84.161)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [100/391]	Time 0.043 (0.050)	Data 0.002 (0.006)	Loss 0.373 (0.450)	Prec@1 88.281 (84.398)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [150/391]	Time 0.072 (0.051)	Data 0.003 (0.005)	Loss 0.544 (0.463)	Prec@1 82.031 (83.992)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:38:Num bit 8	Num grad bit 8	
06-15-23 19:38:Iter: [200/391]	Time 0.073 (0.057)	Data 0.003 (0.004)	Loss 0.259 (0.460)	Prec@1 90.625 (84.087)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [250/391]	Time 0.062 (0.058)	Data 0.003 (0.004)	Loss 0.462 (0.460)	Prec@1 84.375 (84.114)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [300/391]	Time 0.076 (0.060)	Data 0.003 (0.004)	Loss 0.401 (0.457)	Prec@1 86.719 (84.245)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [350/391]	Time 0.071 (0.062)	Data 0.003 (0.004)	Loss 0.436 (0.458)	Prec@1 86.719 (84.266)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Test: [0/79]	Time: 0.4559(0.4559)	Loss: 3.052(3.052)	Prec@1: 50.000(50.000)	
06-15-23 19:39:Test: [50/79]	Time: 0.0166(0.0255)	Loss: 3.031(3.287)	Prec@1: 52.344(43.735)	
06-15-23 19:39:Test: [78/79]	Time: 0.0155(0.0224)	Loss: 3.573(3.279)	Prec@1: 37.500(44.230)	
06-15-23 19:39:Step 157 * Prec@1 44.230
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [0/391]	Time 0.477 (0.477)	Data 0.440 (0.440)	Loss 0.381 (0.381)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [50/391]	Time 0.074 (0.073)	Data 0.003 (0.011)	Loss 0.510 (0.460)	Prec@1 77.344 (84.023)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [100/391]	Time 0.078 (0.073)	Data 0.003 (0.007)	Loss 0.319 (0.457)	Prec@1 86.719 (83.895)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [150/391]	Time 0.067 (0.072)	Data 0.003 (0.006)	Loss 0.373 (0.457)	Prec@1 86.719 (84.008)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [200/391]	Time 0.070 (0.072)	Data 0.002 (0.005)	Loss 0.451 (0.452)	Prec@1 82.812 (84.340)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [250/391]	Time 0.070 (0.072)	Data 0.003 (0.004)	Loss 0.354 (0.452)	Prec@1 85.938 (84.447)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [300/391]	Time 0.072 (0.072)	Data 0.003 (0.004)	Loss 0.605 (0.453)	Prec@1 85.156 (84.489)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [350/391]	Time 0.071 (0.071)	Data 0.003 (0.004)	Loss 0.512 (0.457)	Prec@1 82.031 (84.393)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Test: [0/79]	Time: 0.4571(0.4571)	Loss: 12.627(12.627)	Prec@1: 14.062(14.062)	
06-15-23 19:39:Test: [50/79]	Time: 0.0169(0.0254)	Loss: 13.706(13.678)	Prec@1: 17.188(13.817)	
06-15-23 19:39:Test: [78/79]	Time: 0.0152(0.0223)	Loss: 9.724(13.586)	Prec@1: 18.750(13.910)	
06-15-23 19:39:Step 158 * Prec@1 13.910
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [0/391]	Time 0.475 (0.475)	Data 0.437 (0.437)	Loss 0.615 (0.615)	Prec@1 77.344 (77.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [50/391]	Time 0.069 (0.073)	Data 0.003 (0.011)	Loss 0.490 (0.451)	Prec@1 84.375 (84.390)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [100/391]	Time 0.073 (0.073)	Data 0.003 (0.007)	Loss 0.453 (0.456)	Prec@1 85.156 (84.158)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [150/391]	Time 0.067 (0.071)	Data 0.002 (0.006)	Loss 0.448 (0.457)	Prec@1 82.031 (84.308)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:39:Num bit 8	Num grad bit 8	
06-15-23 19:39:Iter: [200/391]	Time 0.066 (0.071)	Data 0.003 (0.005)	Loss 0.316 (0.453)	Prec@1 87.500 (84.422)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [250/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.423 (0.454)	Prec@1 84.375 (84.310)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [300/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.508 (0.457)	Prec@1 79.688 (84.219)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [350/391]	Time 0.074 (0.071)	Data 0.003 (0.004)	Loss 0.366 (0.458)	Prec@1 85.938 (84.195)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Test: [0/79]	Time: 0.4724(0.4724)	Loss: 14.124(14.124)	Prec@1: 7.812(7.812)	
06-15-23 19:40:Test: [50/79]	Time: 0.0171(0.0256)	Loss: 14.801(14.694)	Prec@1: 10.938(10.018)	
06-15-23 19:40:Test: [78/79]	Time: 0.0154(0.0224)	Loss: 14.706(14.634)	Prec@1: 6.250(10.240)	
06-15-23 19:40:Step 159 * Prec@1 10.240
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [0/391]	Time 0.472 (0.472)	Data 0.435 (0.435)	Loss 0.365 (0.365)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [50/391]	Time 0.068 (0.067)	Data 0.002 (0.011)	Loss 0.417 (0.430)	Prec@1 88.281 (85.478)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [100/391]	Time 0.068 (0.068)	Data 0.003 (0.007)	Loss 0.522 (0.448)	Prec@1 79.688 (84.684)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [150/391]	Time 0.043 (0.065)	Data 0.002 (0.005)	Loss 0.446 (0.455)	Prec@1 81.250 (84.530)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [200/391]	Time 0.072 (0.063)	Data 0.003 (0.005)	Loss 0.420 (0.454)	Prec@1 85.938 (84.581)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [250/391]	Time 0.059 (0.063)	Data 0.002 (0.004)	Loss 0.495 (0.458)	Prec@1 86.719 (84.422)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [300/391]	Time 0.071 (0.064)	Data 0.003 (0.004)	Loss 0.420 (0.459)	Prec@1 88.281 (84.398)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [350/391]	Time 0.070 (0.065)	Data 0.003 (0.004)	Loss 0.470 (0.459)	Prec@1 81.250 (84.391)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Test: [0/79]	Time: 0.4631(0.4631)	Loss: 2.857(2.857)	Prec@1: 34.375(34.375)	
06-15-23 19:40:Test: [50/79]	Time: 0.0167(0.0283)	Loss: 2.724(3.005)	Prec@1: 42.188(36.305)	
06-15-23 19:40:Test: [78/79]	Time: 0.0152(0.0242)	Loss: 3.470(2.987)	Prec@1: 25.000(36.590)	
06-15-23 19:40:Step 160 * Prec@1 36.590
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [0/391]	Time 0.509 (0.509)	Data 0.439 (0.439)	Loss 0.527 (0.527)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [50/391]	Time 0.063 (0.076)	Data 0.003 (0.011)	Loss 0.531 (0.474)	Prec@1 80.469 (83.839)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [100/391]	Time 0.071 (0.073)	Data 0.002 (0.007)	Loss 0.604 (0.462)	Prec@1 82.031 (84.398)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [150/391]	Time 0.069 (0.072)	Data 0.003 (0.005)	Loss 0.596 (0.458)	Prec@1 80.469 (84.359)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [200/391]	Time 0.073 (0.071)	Data 0.002 (0.005)	Loss 0.316 (0.459)	Prec@1 89.062 (84.301)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:40:Num bit 8	Num grad bit 8	
06-15-23 19:40:Iter: [250/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.543 (0.459)	Prec@1 83.594 (84.363)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [300/391]	Time 0.073 (0.072)	Data 0.003 (0.004)	Loss 0.490 (0.457)	Prec@1 85.938 (84.437)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [350/391]	Time 0.074 (0.072)	Data 0.003 (0.004)	Loss 0.442 (0.459)	Prec@1 85.938 (84.373)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Test: [0/79]	Time: 0.4465(0.4465)	Loss: 1.942(1.942)	Prec@1: 51.562(51.562)	
06-15-23 19:41:Test: [50/79]	Time: 0.0167(0.0263)	Loss: 2.434(2.142)	Prec@1: 41.406(49.556)	
06-15-23 19:41:Test: [78/79]	Time: 0.0155(0.0230)	Loss: 2.525(2.156)	Prec@1: 56.250(49.350)	
06-15-23 19:41:Step 161 * Prec@1 49.350
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [0/391]	Time 0.494 (0.494)	Data 0.453 (0.453)	Loss 0.385 (0.385)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [50/391]	Time 0.074 (0.076)	Data 0.003 (0.011)	Loss 0.420 (0.461)	Prec@1 85.156 (84.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [100/391]	Time 0.070 (0.069)	Data 0.002 (0.007)	Loss 0.481 (0.456)	Prec@1 80.469 (84.460)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [150/391]	Time 0.072 (0.070)	Data 0.003 (0.006)	Loss 0.537 (0.456)	Prec@1 81.250 (84.215)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [200/391]	Time 0.066 (0.067)	Data 0.002 (0.005)	Loss 0.503 (0.457)	Prec@1 78.906 (84.165)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [250/391]	Time 0.064 (0.066)	Data 0.002 (0.004)	Loss 0.414 (0.458)	Prec@1 85.156 (84.160)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [300/391]	Time 0.072 (0.066)	Data 0.003 (0.004)	Loss 0.505 (0.458)	Prec@1 82.031 (84.240)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [350/391]	Time 0.066 (0.067)	Data 0.003 (0.004)	Loss 0.505 (0.458)	Prec@1 83.594 (84.268)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Test: [0/79]	Time: 0.4470(0.4470)	Loss: 6.455(6.455)	Prec@1: 18.750(18.750)	
06-15-23 19:41:Test: [50/79]	Time: 0.0170(0.0253)	Loss: 6.248(6.409)	Prec@1: 26.562(24.188)	
06-15-23 19:41:Test: [78/79]	Time: 0.0154(0.0223)	Loss: 7.592(6.439)	Prec@1: 12.500(24.200)	
06-15-23 19:41:Step 162 * Prec@1 24.200
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [0/391]	Time 0.478 (0.478)	Data 0.441 (0.441)	Loss 0.386 (0.386)	Prec@1 88.281 (88.281)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [50/391]	Time 0.071 (0.074)	Data 0.003 (0.011)	Loss 0.444 (0.453)	Prec@1 83.594 (84.712)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [100/391]	Time 0.073 (0.071)	Data 0.003 (0.007)	Loss 0.498 (0.462)	Prec@1 81.250 (84.537)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [150/391]	Time 0.070 (0.070)	Data 0.003 (0.005)	Loss 0.407 (0.456)	Prec@1 88.281 (84.515)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [200/391]	Time 0.064 (0.069)	Data 0.003 (0.005)	Loss 0.367 (0.458)	Prec@1 88.281 (84.328)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:41:Num bit 8	Num grad bit 8	
06-15-23 19:41:Iter: [250/391]	Time 0.070 (0.069)	Data 0.003 (0.004)	Loss 0.529 (0.462)	Prec@1 84.375 (84.170)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [300/391]	Time 0.073 (0.069)	Data 0.002 (0.004)	Loss 0.468 (0.461)	Prec@1 85.156 (84.149)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [350/391]	Time 0.081 (0.070)	Data 0.003 (0.004)	Loss 0.257 (0.461)	Prec@1 93.750 (84.172)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Test: [0/79]	Time: 0.4548(0.4548)	Loss: 2.918(2.918)	Prec@1: 44.531(44.531)	
06-15-23 19:42:Test: [50/79]	Time: 0.0166(0.0252)	Loss: 3.033(3.023)	Prec@1: 32.031(37.975)	
06-15-23 19:42:Test: [78/79]	Time: 0.0153(0.0222)	Loss: 4.034(3.008)	Prec@1: 18.750(37.970)	
06-15-23 19:42:Step 163 * Prec@1 37.970
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [0/391]	Time 0.469 (0.469)	Data 0.431 (0.431)	Loss 0.560 (0.560)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [50/391]	Time 0.070 (0.072)	Data 0.003 (0.011)	Loss 0.370 (0.461)	Prec@1 88.281 (84.482)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [100/391]	Time 0.070 (0.071)	Data 0.003 (0.007)	Loss 0.324 (0.454)	Prec@1 89.844 (84.329)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [150/391]	Time 0.075 (0.071)	Data 0.003 (0.005)	Loss 0.439 (0.452)	Prec@1 83.594 (84.339)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [200/391]	Time 0.074 (0.071)	Data 0.003 (0.005)	Loss 0.481 (0.455)	Prec@1 80.469 (84.258)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [250/391]	Time 0.062 (0.071)	Data 0.002 (0.004)	Loss 0.400 (0.456)	Prec@1 85.156 (84.303)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [300/391]	Time 0.072 (0.071)	Data 0.003 (0.004)	Loss 0.550 (0.456)	Prec@1 77.344 (84.398)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [350/391]	Time 0.078 (0.072)	Data 0.003 (0.004)	Loss 0.397 (0.457)	Prec@1 87.500 (84.455)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Test: [0/79]	Time: 0.4619(0.4619)	Loss: 15.870(15.870)	Prec@1: 13.281(13.281)	
06-15-23 19:42:Test: [50/79]	Time: 0.0167(0.0254)	Loss: 14.723(15.196)	Prec@1: 16.406(16.054)	
06-15-23 19:42:Test: [78/79]	Time: 0.0154(0.0223)	Loss: 14.286(15.354)	Prec@1: 0.000(15.520)	
06-15-23 19:42:Step 164 * Prec@1 15.520
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [0/391]	Time 0.485 (0.485)	Data 0.448 (0.448)	Loss 0.488 (0.488)	Prec@1 85.156 (85.156)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [50/391]	Time 0.073 (0.072)	Data 0.003 (0.011)	Loss 0.483 (0.458)	Prec@1 85.938 (84.191)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [100/391]	Time 0.067 (0.072)	Data 0.003 (0.007)	Loss 0.475 (0.461)	Prec@1 80.469 (84.282)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [150/391]	Time 0.072 (0.072)	Data 0.003 (0.006)	Loss 0.471 (0.459)	Prec@1 81.250 (84.292)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [200/391]	Time 0.075 (0.072)	Data 0.003 (0.005)	Loss 0.428 (0.462)	Prec@1 84.375 (84.153)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:42:Num bit 8	Num grad bit 8	
06-15-23 19:42:Iter: [250/391]	Time 0.074 (0.072)	Data 0.003 (0.005)	Loss 0.352 (0.461)	Prec@1 85.938 (84.148)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [300/391]	Time 0.070 (0.072)	Data 0.002 (0.004)	Loss 0.468 (0.459)	Prec@1 84.375 (84.240)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [350/391]	Time 0.070 (0.072)	Data 0.003 (0.004)	Loss 0.477 (0.463)	Prec@1 83.594 (84.097)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Test: [0/79]	Time: 0.4629(0.4629)	Loss: 7.615(7.615)	Prec@1: 19.531(19.531)	
06-15-23 19:43:Test: [50/79]	Time: 0.0629(0.0262)	Loss: 8.193(6.950)	Prec@1: 16.406(21.186)	
06-15-23 19:43:Test: [78/79]	Time: 0.0154(0.0229)	Loss: 7.163(6.942)	Prec@1: 25.000(21.210)	
06-15-23 19:43:Step 165 * Prec@1 21.210
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [0/391]	Time 0.479 (0.479)	Data 0.441 (0.441)	Loss 0.529 (0.529)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [50/391]	Time 0.071 (0.065)	Data 0.002 (0.011)	Loss 0.357 (0.456)	Prec@1 89.844 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [100/391]	Time 0.050 (0.065)	Data 0.002 (0.007)	Loss 0.445 (0.455)	Prec@1 85.156 (84.506)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [150/391]	Time 0.074 (0.062)	Data 0.003 (0.005)	Loss 0.532 (0.461)	Prec@1 79.688 (84.204)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [200/391]	Time 0.066 (0.064)	Data 0.003 (0.005)	Loss 0.462 (0.459)	Prec@1 85.156 (84.324)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [250/391]	Time 0.059 (0.064)	Data 0.002 (0.004)	Loss 0.429 (0.458)	Prec@1 85.156 (84.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [300/391]	Time 0.064 (0.064)	Data 0.002 (0.004)	Loss 0.583 (0.460)	Prec@1 79.688 (84.232)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [350/391]	Time 0.062 (0.064)	Data 0.002 (0.004)	Loss 0.398 (0.458)	Prec@1 85.938 (84.255)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Test: [0/79]	Time: 0.4520(0.4520)	Loss: 3.909(3.909)	Prec@1: 35.938(35.938)	
06-15-23 19:43:Test: [50/79]	Time: 0.0170(0.0252)	Loss: 4.436(4.547)	Prec@1: 36.719(33.027)	
06-15-23 19:43:Test: [78/79]	Time: 0.0154(0.0222)	Loss: 3.670(4.509)	Prec@1: 37.500(33.480)	
06-15-23 19:43:Step 166 * Prec@1 33.480
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [0/391]	Time 0.486 (0.486)	Data 0.445 (0.445)	Loss 0.508 (0.508)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [50/391]	Time 0.074 (0.078)	Data 0.003 (0.011)	Loss 0.512 (0.455)	Prec@1 81.250 (84.482)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [100/391]	Time 0.074 (0.075)	Data 0.002 (0.007)	Loss 0.437 (0.454)	Prec@1 87.500 (84.452)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [150/391]	Time 0.067 (0.073)	Data 0.003 (0.006)	Loss 0.458 (0.454)	Prec@1 85.156 (84.256)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [200/391]	Time 0.063 (0.072)	Data 0.002 (0.005)	Loss 0.556 (0.453)	Prec@1 78.125 (84.282)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [250/391]	Time 0.064 (0.072)	Data 0.002 (0.004)	Loss 0.500 (0.460)	Prec@1 84.375 (84.138)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:43:Num bit 8	Num grad bit 8	
06-15-23 19:43:Iter: [300/391]	Time 0.073 (0.072)	Data 0.003 (0.004)	Loss 0.345 (0.459)	Prec@1 88.281 (84.126)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [350/391]	Time 0.070 (0.072)	Data 0.003 (0.004)	Loss 0.440 (0.460)	Prec@1 88.281 (84.166)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Test: [0/79]	Time: 0.4670(0.4670)	Loss: 4.071(4.071)	Prec@1: 47.656(47.656)	
06-15-23 19:44:Test: [50/79]	Time: 0.0170(0.0261)	Loss: 4.234(4.106)	Prec@1: 43.750(46.247)	
06-15-23 19:44:Test: [78/79]	Time: 0.0154(0.0228)	Loss: 4.956(4.058)	Prec@1: 31.250(46.230)	
06-15-23 19:44:Step 167 * Prec@1 46.230
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [0/391]	Time 0.485 (0.485)	Data 0.440 (0.440)	Loss 0.469 (0.469)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [50/391]	Time 0.069 (0.059)	Data 0.003 (0.011)	Loss 0.400 (0.456)	Prec@1 83.594 (84.712)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [100/391]	Time 0.072 (0.066)	Data 0.003 (0.007)	Loss 0.453 (0.454)	Prec@1 85.156 (84.592)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [150/391]	Time 0.047 (0.067)	Data 0.002 (0.005)	Loss 0.365 (0.457)	Prec@1 87.500 (84.339)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [200/391]	Time 0.067 (0.067)	Data 0.003 (0.005)	Loss 0.406 (0.461)	Prec@1 85.156 (84.173)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [250/391]	Time 0.076 (0.068)	Data 0.003 (0.004)	Loss 0.357 (0.461)	Prec@1 85.156 (84.048)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [300/391]	Time 0.069 (0.068)	Data 0.003 (0.004)	Loss 0.486 (0.458)	Prec@1 83.594 (84.134)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [350/391]	Time 0.071 (0.068)	Data 0.003 (0.004)	Loss 0.399 (0.461)	Prec@1 85.938 (84.054)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Test: [0/79]	Time: 0.4441(0.4441)	Loss: 2.414(2.414)	Prec@1: 49.219(49.219)	
06-15-23 19:44:Test: [50/79]	Time: 0.0165(0.0284)	Loss: 2.394(2.732)	Prec@1: 46.094(40.977)	
06-15-23 19:44:Test: [78/79]	Time: 0.0155(0.0243)	Loss: 3.255(2.710)	Prec@1: 25.000(41.230)	
06-15-23 19:44:Step 168 * Prec@1 41.230
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [0/391]	Time 0.472 (0.472)	Data 0.434 (0.434)	Loss 0.425 (0.425)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [50/391]	Time 0.076 (0.067)	Data 0.003 (0.011)	Loss 0.404 (0.446)	Prec@1 87.500 (85.187)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [100/391]	Time 0.074 (0.069)	Data 0.003 (0.007)	Loss 0.457 (0.446)	Prec@1 85.156 (84.684)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [150/391]	Time 0.070 (0.069)	Data 0.003 (0.005)	Loss 0.444 (0.450)	Prec@1 84.375 (84.406)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [200/391]	Time 0.069 (0.069)	Data 0.003 (0.005)	Loss 0.391 (0.455)	Prec@1 85.156 (84.153)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [250/391]	Time 0.074 (0.069)	Data 0.003 (0.004)	Loss 0.513 (0.454)	Prec@1 81.250 (84.260)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:44:Num bit 8	Num grad bit 8	
06-15-23 19:44:Iter: [300/391]	Time 0.067 (0.070)	Data 0.003 (0.004)	Loss 0.475 (0.454)	Prec@1 88.281 (84.365)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [350/391]	Time 0.075 (0.070)	Data 0.003 (0.004)	Loss 0.430 (0.455)	Prec@1 82.812 (84.233)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Test: [0/79]	Time: 0.4491(0.4491)	Loss: 14.776(14.776)	Prec@1: 11.719(11.719)	
06-15-23 19:45:Test: [50/79]	Time: 0.0169(0.0252)	Loss: 13.154(12.632)	Prec@1: 22.656(17.249)	
06-15-23 19:45:Test: [78/79]	Time: 0.0154(0.0222)	Loss: 13.211(12.754)	Prec@1: 6.250(16.810)	
06-15-23 19:45:Step 169 * Prec@1 16.810
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [0/391]	Time 0.492 (0.492)	Data 0.442 (0.442)	Loss 0.478 (0.478)	Prec@1 82.031 (82.031)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [50/391]	Time 0.046 (0.061)	Data 0.002 (0.011)	Loss 0.487 (0.450)	Prec@1 82.031 (84.773)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [100/391]	Time 0.038 (0.052)	Data 0.002 (0.006)	Loss 0.452 (0.451)	Prec@1 84.375 (84.537)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [150/391]	Time 0.040 (0.050)	Data 0.002 (0.005)	Loss 0.566 (0.451)	Prec@1 79.688 (84.665)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [200/391]	Time 0.048 (0.048)	Data 0.003 (0.004)	Loss 0.505 (0.451)	Prec@1 83.594 (84.589)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [250/391]	Time 0.044 (0.047)	Data 0.003 (0.004)	Loss 0.519 (0.450)	Prec@1 82.812 (84.615)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [300/391]	Time 0.047 (0.047)	Data 0.002 (0.004)	Loss 0.470 (0.452)	Prec@1 81.250 (84.526)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [350/391]	Time 0.042 (0.046)	Data 0.002 (0.003)	Loss 0.250 (0.456)	Prec@1 95.312 (84.371)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Test: [0/79]	Time: 0.4586(0.4586)	Loss: 7.258(7.258)	Prec@1: 20.312(20.312)	
06-15-23 19:45:Test: [50/79]	Time: 0.0170(0.0256)	Loss: 7.461(7.272)	Prec@1: 16.406(18.137)	
06-15-23 19:45:Test: [78/79]	Time: 0.0156(0.0225)	Loss: 6.799(7.212)	Prec@1: 6.250(17.790)	
06-15-23 19:45:Step 170 * Prec@1 17.790
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [0/391]	Time 0.474 (0.474)	Data 0.437 (0.437)	Loss 0.455 (0.455)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [50/391]	Time 0.064 (0.064)	Data 0.002 (0.011)	Loss 0.353 (0.441)	Prec@1 83.594 (84.758)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [100/391]	Time 0.069 (0.067)	Data 0.002 (0.007)	Loss 0.517 (0.454)	Prec@1 80.469 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [150/391]	Time 0.064 (0.068)	Data 0.002 (0.005)	Loss 0.447 (0.456)	Prec@1 85.156 (84.334)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [200/391]	Time 0.065 (0.068)	Data 0.003 (0.005)	Loss 0.374 (0.458)	Prec@1 86.719 (84.251)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [250/391]	Time 0.078 (0.068)	Data 0.003 (0.004)	Loss 0.337 (0.460)	Prec@1 87.500 (84.176)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [300/391]	Time 0.074 (0.068)	Data 0.009 (0.004)	Loss 0.538 (0.459)	Prec@1 82.812 (84.191)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [350/391]	Time 0.068 (0.069)	Data 0.003 (0.004)	Loss 0.444 (0.459)	Prec@1 85.938 (84.132)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Test: [0/79]	Time: 0.4631(0.4631)	Loss: 27.209(27.209)	Prec@1: 7.812(7.812)	
06-15-23 19:45:Test: [50/79]	Time: 0.0167(0.0263)	Loss: 28.976(27.071)	Prec@1: 10.156(11.596)	
06-15-23 19:45:Test: [78/79]	Time: 0.0152(0.0228)	Loss: 25.686(27.006)	Prec@1: 18.750(11.710)	
06-15-23 19:45:Step 171 * Prec@1 11.710
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [0/391]	Time 0.498 (0.498)	Data 0.460 (0.460)	Loss 0.515 (0.515)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:45:Num bit 8	Num grad bit 8	
06-15-23 19:45:Iter: [50/391]	Time 0.076 (0.062)	Data 0.003 (0.011)	Loss 0.555 (0.467)	Prec@1 81.250 (84.176)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [100/391]	Time 0.070 (0.067)	Data 0.002 (0.007)	Loss 0.362 (0.461)	Prec@1 88.281 (84.244)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [150/391]	Time 0.066 (0.068)	Data 0.002 (0.006)	Loss 0.481 (0.458)	Prec@1 84.375 (84.406)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [200/391]	Time 0.070 (0.068)	Data 0.002 (0.005)	Loss 0.459 (0.459)	Prec@1 83.594 (84.402)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [250/391]	Time 0.068 (0.069)	Data 0.003 (0.004)	Loss 0.439 (0.460)	Prec@1 85.156 (84.310)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [300/391]	Time 0.067 (0.069)	Data 0.003 (0.004)	Loss 0.358 (0.458)	Prec@1 89.062 (84.367)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [350/391]	Time 0.072 (0.070)	Data 0.003 (0.004)	Loss 0.367 (0.459)	Prec@1 85.938 (84.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Test: [0/79]	Time: 0.4694(0.4694)	Loss: 3.837(3.837)	Prec@1: 37.500(37.500)	
06-15-23 19:46:Test: [50/79]	Time: 0.0166(0.0255)	Loss: 4.556(4.079)	Prec@1: 29.688(32.981)	
06-15-23 19:46:Test: [78/79]	Time: 0.0168(0.0224)	Loss: 2.751(4.074)	Prec@1: 37.500(32.780)	
06-15-23 19:46:Step 172 * Prec@1 32.780
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [0/391]	Time 0.512 (0.512)	Data 0.474 (0.474)	Loss 0.439 (0.439)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [50/391]	Time 0.068 (0.072)	Data 0.002 (0.012)	Loss 0.489 (0.453)	Prec@1 81.250 (84.360)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [100/391]	Time 0.045 (0.061)	Data 0.002 (0.007)	Loss 0.413 (0.455)	Prec@1 85.156 (84.390)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [150/391]	Time 0.046 (0.057)	Data 0.002 (0.005)	Loss 0.395 (0.454)	Prec@1 85.938 (84.468)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [200/391]	Time 0.057 (0.054)	Data 0.002 (0.004)	Loss 0.442 (0.457)	Prec@1 87.500 (84.227)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [250/391]	Time 0.070 (0.057)	Data 0.003 (0.004)	Loss 0.361 (0.455)	Prec@1 89.062 (84.300)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [300/391]	Time 0.070 (0.059)	Data 0.003 (0.004)	Loss 0.502 (0.453)	Prec@1 82.812 (84.398)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [350/391]	Time 0.063 (0.061)	Data 0.003 (0.004)	Loss 0.414 (0.455)	Prec@1 82.812 (84.386)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Test: [0/79]	Time: 0.4568(0.4568)	Loss: 6.216(6.216)	Prec@1: 28.125(28.125)	
06-15-23 19:46:Test: [50/79]	Time: 0.0168(0.0253)	Loss: 6.858(6.642)	Prec@1: 22.656(28.171)	
06-15-23 19:46:Test: [78/79]	Time: 0.0153(0.0223)	Loss: 5.552(6.670)	Prec@1: 31.250(27.980)	
06-15-23 19:46:Step 173 * Prec@1 27.980
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [0/391]	Time 0.476 (0.476)	Data 0.438 (0.438)	Loss 0.390 (0.390)	Prec@1 88.281 (88.281)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [50/391]	Time 0.077 (0.075)	Data 0.003 (0.011)	Loss 0.617 (0.446)	Prec@1 81.250 (84.957)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:46:Num bit 8	Num grad bit 8	
06-15-23 19:46:Iter: [100/391]	Time 0.070 (0.074)	Data 0.003 (0.007)	Loss 0.562 (0.449)	Prec@1 82.031 (84.669)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [150/391]	Time 0.071 (0.073)	Data 0.003 (0.006)	Loss 0.510 (0.450)	Prec@1 83.594 (84.660)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [200/391]	Time 0.071 (0.072)	Data 0.003 (0.005)	Loss 0.563 (0.453)	Prec@1 83.594 (84.422)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [250/391]	Time 0.072 (0.072)	Data 0.003 (0.004)	Loss 0.319 (0.453)	Prec@1 89.844 (84.475)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [300/391]	Time 0.077 (0.072)	Data 0.003 (0.004)	Loss 0.466 (0.455)	Prec@1 83.594 (84.323)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [350/391]	Time 0.070 (0.072)	Data 0.003 (0.004)	Loss 0.466 (0.454)	Prec@1 87.500 (84.388)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Test: [0/79]	Time: 0.4684(0.4684)	Loss: 6.758(6.758)	Prec@1: 21.094(21.094)	
06-15-23 19:47:Test: [50/79]	Time: 0.0169(0.0256)	Loss: 5.740(6.020)	Prec@1: 28.906(25.245)	
06-15-23 19:47:Test: [78/79]	Time: 0.0154(0.0224)	Loss: 5.747(6.108)	Prec@1: 18.750(24.920)	
06-15-23 19:47:Step 174 * Prec@1 24.920
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [0/391]	Time 0.470 (0.470)	Data 0.432 (0.432)	Loss 0.661 (0.661)	Prec@1 74.219 (74.219)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [50/391]	Time 0.072 (0.075)	Data 0.003 (0.011)	Loss 0.524 (0.468)	Prec@1 82.812 (84.222)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [100/391]	Time 0.074 (0.073)	Data 0.003 (0.007)	Loss 0.346 (0.463)	Prec@1 86.719 (84.197)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [150/391]	Time 0.068 (0.073)	Data 0.003 (0.006)	Loss 0.534 (0.467)	Prec@1 81.250 (84.008)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [200/391]	Time 0.071 (0.072)	Data 0.003 (0.005)	Loss 0.460 (0.465)	Prec@1 81.250 (84.095)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [250/391]	Time 0.071 (0.072)	Data 0.003 (0.004)	Loss 0.432 (0.462)	Prec@1 85.156 (84.235)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [300/391]	Time 0.072 (0.072)	Data 0.003 (0.004)	Loss 0.377 (0.462)	Prec@1 88.281 (84.201)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [350/391]	Time 0.069 (0.072)	Data 0.003 (0.004)	Loss 0.344 (0.459)	Prec@1 86.719 (84.213)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Test: [0/79]	Time: 0.4470(0.4470)	Loss: 3.848(3.848)	Prec@1: 40.625(40.625)	
06-15-23 19:47:Test: [50/79]	Time: 0.0168(0.0252)	Loss: 3.709(3.676)	Prec@1: 44.531(40.533)	
06-15-23 19:47:Test: [78/79]	Time: 0.0154(0.0222)	Loss: 2.154(3.721)	Prec@1: 43.750(40.170)	
06-15-23 19:47:Step 175 * Prec@1 40.170
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [0/391]	Time 0.483 (0.483)	Data 0.434 (0.434)	Loss 0.465 (0.465)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [50/391]	Time 0.076 (0.072)	Data 0.003 (0.011)	Loss 0.507 (0.453)	Prec@1 83.594 (84.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:47:Num bit 8	Num grad bit 8	
06-15-23 19:47:Iter: [100/391]	Time 0.072 (0.072)	Data 0.002 (0.007)	Loss 0.609 (0.457)	Prec@1 81.250 (84.127)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [150/391]	Time 0.068 (0.071)	Data 0.003 (0.005)	Loss 0.527 (0.458)	Prec@1 79.688 (84.178)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [200/391]	Time 0.069 (0.070)	Data 0.003 (0.005)	Loss 0.638 (0.459)	Prec@1 80.469 (84.142)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [250/391]	Time 0.075 (0.070)	Data 0.004 (0.004)	Loss 0.469 (0.455)	Prec@1 82.031 (84.269)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [300/391]	Time 0.071 (0.070)	Data 0.003 (0.004)	Loss 0.503 (0.457)	Prec@1 83.594 (84.199)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [350/391]	Time 0.060 (0.071)	Data 0.002 (0.004)	Loss 0.516 (0.456)	Prec@1 84.375 (84.241)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Test: [0/79]	Time: 0.4758(0.4758)	Loss: 9.000(9.000)	Prec@1: 17.188(17.188)	
06-15-23 19:48:Test: [50/79]	Time: 0.0166(0.0256)	Loss: 8.665(8.029)	Prec@1: 24.219(20.849)	
06-15-23 19:48:Test: [78/79]	Time: 0.0153(0.0224)	Loss: 10.019(8.139)	Prec@1: 18.750(20.630)	
06-15-23 19:48:Step 176 * Prec@1 20.630
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [0/391]	Time 0.478 (0.478)	Data 0.427 (0.427)	Loss 0.467 (0.467)	Prec@1 82.031 (82.031)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [50/391]	Time 0.070 (0.074)	Data 0.003 (0.011)	Loss 0.453 (0.446)	Prec@1 85.156 (85.126)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [100/391]	Time 0.069 (0.071)	Data 0.002 (0.007)	Loss 0.338 (0.441)	Prec@1 83.594 (85.040)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [150/391]	Time 0.071 (0.071)	Data 0.003 (0.005)	Loss 0.529 (0.449)	Prec@1 82.812 (84.784)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [200/391]	Time 0.074 (0.072)	Data 0.003 (0.005)	Loss 0.518 (0.450)	Prec@1 82.812 (84.717)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [250/391]	Time 0.070 (0.072)	Data 0.003 (0.004)	Loss 0.436 (0.449)	Prec@1 85.938 (84.655)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [300/391]	Time 0.074 (0.072)	Data 0.003 (0.004)	Loss 0.486 (0.451)	Prec@1 82.031 (84.531)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [350/391]	Time 0.068 (0.072)	Data 0.003 (0.004)	Loss 0.426 (0.451)	Prec@1 84.375 (84.549)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Test: [0/79]	Time: 0.4650(0.4650)	Loss: 3.299(3.299)	Prec@1: 37.500(37.500)	
06-15-23 19:48:Test: [50/79]	Time: 0.0287(0.0306)	Loss: 3.732(3.578)	Prec@1: 33.594(36.366)	
06-15-23 19:48:Test: [78/79]	Time: 0.0156(0.0262)	Loss: 2.799(3.562)	Prec@1: 43.750(36.490)	
06-15-23 19:48:Step 177 * Prec@1 36.490
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [0/391]	Time 0.505 (0.505)	Data 0.453 (0.453)	Loss 0.509 (0.509)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [50/391]	Time 0.050 (0.053)	Data 0.002 (0.011)	Loss 0.498 (0.459)	Prec@1 82.812 (83.869)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [100/391]	Time 0.056 (0.051)	Data 0.003 (0.007)	Loss 0.409 (0.449)	Prec@1 85.938 (84.282)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:48:Num bit 8	Num grad bit 8	
06-15-23 19:48:Iter: [150/391]	Time 0.068 (0.051)	Data 0.002 (0.005)	Loss 0.424 (0.449)	Prec@1 85.156 (84.478)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [200/391]	Time 0.074 (0.057)	Data 0.003 (0.005)	Loss 0.558 (0.449)	Prec@1 82.812 (84.620)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [250/391]	Time 0.074 (0.059)	Data 0.002 (0.004)	Loss 0.435 (0.451)	Prec@1 83.594 (84.437)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [300/391]	Time 0.071 (0.061)	Data 0.003 (0.004)	Loss 0.508 (0.454)	Prec@1 84.375 (84.370)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [350/391]	Time 0.050 (0.062)	Data 0.003 (0.004)	Loss 0.596 (0.452)	Prec@1 78.906 (84.379)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Test: [0/79]	Time: 0.4754(0.4754)	Loss: 7.697(7.697)	Prec@1: 15.625(15.625)	
06-15-23 19:49:Test: [50/79]	Time: 0.0166(0.0275)	Loss: 8.320(7.775)	Prec@1: 11.719(17.662)	
06-15-23 19:49:Test: [78/79]	Time: 0.0152(0.0238)	Loss: 4.937(7.824)	Prec@1: 25.000(17.640)	
06-15-23 19:49:Step 178 * Prec@1 17.640
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [0/391]	Time 0.477 (0.477)	Data 0.432 (0.432)	Loss 0.582 (0.582)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [50/391]	Time 0.073 (0.068)	Data 0.002 (0.011)	Loss 0.474 (0.463)	Prec@1 88.281 (83.686)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [100/391]	Time 0.066 (0.067)	Data 0.003 (0.007)	Loss 0.417 (0.445)	Prec@1 85.938 (84.398)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [150/391]	Time 0.067 (0.067)	Data 0.003 (0.005)	Loss 0.366 (0.448)	Prec@1 85.156 (84.422)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [200/391]	Time 0.069 (0.068)	Data 0.003 (0.005)	Loss 0.415 (0.452)	Prec@1 86.719 (84.352)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [250/391]	Time 0.071 (0.069)	Data 0.002 (0.004)	Loss 0.461 (0.454)	Prec@1 85.156 (84.319)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [300/391]	Time 0.067 (0.069)	Data 0.003 (0.004)	Loss 0.508 (0.458)	Prec@1 78.125 (84.113)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [350/391]	Time 0.072 (0.069)	Data 0.003 (0.004)	Loss 0.390 (0.461)	Prec@1 84.375 (84.012)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Test: [0/79]	Time: 0.4608(0.4608)	Loss: 19.152(19.152)	Prec@1: 8.594(8.594)	
06-15-23 19:49:Test: [50/79]	Time: 0.0167(0.0254)	Loss: 18.928(19.201)	Prec@1: 10.156(10.003)	
06-15-23 19:49:Test: [78/79]	Time: 0.0153(0.0223)	Loss: 19.242(19.226)	Prec@1: 6.250(10.000)	
06-15-23 19:49:Step 179 * Prec@1 10.000
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [0/391]	Time 0.476 (0.476)	Data 0.430 (0.430)	Loss 0.583 (0.583)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [50/391]	Time 0.070 (0.072)	Data 0.003 (0.011)	Loss 0.383 (0.462)	Prec@1 87.500 (84.605)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [100/391]	Time 0.073 (0.071)	Data 0.003 (0.007)	Loss 0.466 (0.460)	Prec@1 79.688 (84.584)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [150/391]	Time 0.075 (0.070)	Data 0.003 (0.005)	Loss 0.580 (0.462)	Prec@1 78.125 (84.225)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:49:Num bit 8	Num grad bit 8	
06-15-23 19:49:Iter: [200/391]	Time 0.072 (0.070)	Data 0.003 (0.005)	Loss 0.496 (0.464)	Prec@1 80.469 (84.099)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [250/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.558 (0.469)	Prec@1 81.250 (83.945)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [300/391]	Time 0.064 (0.070)	Data 0.002 (0.004)	Loss 0.494 (0.470)	Prec@1 83.594 (83.929)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [350/391]	Time 0.059 (0.070)	Data 0.002 (0.004)	Loss 0.584 (0.468)	Prec@1 78.906 (83.957)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Test: [0/79]	Time: 0.5165(0.5165)	Loss: 8.780(8.780)	Prec@1: 20.312(20.312)	
06-15-23 19:50:Test: [50/79]	Time: 0.0168(0.0268)	Loss: 9.095(8.282)	Prec@1: 16.406(22.442)	
06-15-23 19:50:Test: [78/79]	Time: 0.0154(0.0232)	Loss: 6.823(8.269)	Prec@1: 43.750(22.660)	
06-15-23 19:50:Step 180 * Prec@1 22.660
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [0/391]	Time 0.486 (0.486)	Data 0.440 (0.440)	Loss 0.393 (0.393)	Prec@1 88.281 (88.281)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [50/391]	Time 0.053 (0.054)	Data 0.002 (0.011)	Loss 0.382 (0.461)	Prec@1 88.281 (84.283)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [100/391]	Time 0.037 (0.052)	Data 0.001 (0.006)	Loss 0.748 (0.467)	Prec@1 71.094 (83.841)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [150/391]	Time 0.064 (0.049)	Data 0.002 (0.005)	Loss 0.615 (0.472)	Prec@1 76.562 (83.718)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [200/391]	Time 0.070 (0.054)	Data 0.003 (0.004)	Loss 0.714 (0.475)	Prec@1 77.344 (83.609)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [250/391]	Time 0.071 (0.057)	Data 0.002 (0.004)	Loss 0.542 (0.477)	Prec@1 81.250 (83.538)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [300/391]	Time 0.067 (0.059)	Data 0.002 (0.004)	Loss 0.482 (0.479)	Prec@1 82.031 (83.472)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [350/391]	Time 0.071 (0.061)	Data 0.003 (0.004)	Loss 0.531 (0.478)	Prec@1 80.469 (83.538)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Test: [0/79]	Time: 0.4443(0.4443)	Loss: 13.063(13.063)	Prec@1: 8.594(8.594)	
06-15-23 19:50:Test: [50/79]	Time: 0.0167(0.0257)	Loss: 12.924(13.247)	Prec@1: 10.156(10.003)	
06-15-23 19:50:Test: [78/79]	Time: 0.0155(0.0225)	Loss: 12.931(13.244)	Prec@1: 6.250(10.000)	
06-15-23 19:50:Step 181 * Prec@1 10.000
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [0/391]	Time 0.472 (0.472)	Data 0.434 (0.434)	Loss 0.621 (0.621)	Prec@1 78.906 (78.906)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [50/391]	Time 0.066 (0.067)	Data 0.002 (0.011)	Loss 0.423 (0.486)	Prec@1 84.375 (83.134)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [100/391]	Time 0.073 (0.069)	Data 0.003 (0.007)	Loss 0.509 (0.483)	Prec@1 82.812 (83.106)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [150/391]	Time 0.072 (0.070)	Data 0.002 (0.005)	Loss 0.596 (0.479)	Prec@1 80.469 (83.288)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [200/391]	Time 0.074 (0.070)	Data 0.003 (0.005)	Loss 0.340 (0.476)	Prec@1 85.938 (83.520)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:50:Num bit 8	Num grad bit 8	
06-15-23 19:50:Iter: [250/391]	Time 0.074 (0.070)	Data 0.003 (0.004)	Loss 0.510 (0.480)	Prec@1 79.688 (83.395)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [300/391]	Time 0.073 (0.070)	Data 0.003 (0.004)	Loss 0.471 (0.479)	Prec@1 82.812 (83.454)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [350/391]	Time 0.071 (0.071)	Data 0.002 (0.004)	Loss 0.519 (0.480)	Prec@1 80.469 (83.422)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Test: [0/79]	Time: 0.4681(0.4681)	Loss: 4.152(4.152)	Prec@1: 35.938(35.938)	
06-15-23 19:51:Test: [50/79]	Time: 0.0168(0.0257)	Loss: 4.461(4.229)	Prec@1: 29.688(35.034)	
06-15-23 19:51:Test: [78/79]	Time: 0.0153(0.0230)	Loss: 3.063(4.184)	Prec@1: 56.250(35.420)	
06-15-23 19:51:Step 182 * Prec@1 35.420
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [0/391]	Time 0.483 (0.483)	Data 0.445 (0.445)	Loss 0.568 (0.568)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [50/391]	Time 0.072 (0.074)	Data 0.003 (0.011)	Loss 0.404 (0.517)	Prec@1 85.156 (82.261)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [100/391]	Time 0.073 (0.073)	Data 0.003 (0.007)	Loss 0.615 (0.505)	Prec@1 78.906 (82.565)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [150/391]	Time 0.042 (0.072)	Data 0.002 (0.006)	Loss 0.545 (0.502)	Prec@1 84.375 (82.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [200/391]	Time 0.050 (0.065)	Data 0.002 (0.005)	Loss 0.575 (0.496)	Prec@1 82.031 (82.758)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [250/391]	Time 0.064 (0.062)	Data 0.002 (0.004)	Loss 0.668 (0.494)	Prec@1 78.125 (82.806)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [300/391]	Time 0.047 (0.059)	Data 0.002 (0.004)	Loss 0.515 (0.493)	Prec@1 82.812 (82.797)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [350/391]	Time 0.052 (0.057)	Data 0.002 (0.004)	Loss 0.345 (0.491)	Prec@1 87.500 (82.868)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Test: [0/79]	Time: 0.4595(0.4595)	Loss: 4.554(4.554)	Prec@1: 32.812(32.812)	
06-15-23 19:51:Test: [50/79]	Time: 0.0168(0.0255)	Loss: 5.277(5.301)	Prec@1: 28.125(25.031)	
06-15-23 19:51:Test: [78/79]	Time: 0.0155(0.0224)	Loss: 4.763(5.264)	Prec@1: 37.500(25.040)	
06-15-23 19:51:Step 183 * Prec@1 25.040
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [0/391]	Time 0.472 (0.472)	Data 0.435 (0.435)	Loss 0.557 (0.557)	Prec@1 79.688 (79.688)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [50/391]	Time 0.061 (0.073)	Data 0.003 (0.011)	Loss 0.493 (0.492)	Prec@1 84.375 (83.012)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [100/391]	Time 0.043 (0.064)	Data 0.002 (0.007)	Loss 0.369 (0.487)	Prec@1 85.938 (83.014)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [150/391]	Time 0.040 (0.060)	Data 0.002 (0.005)	Loss 0.542 (0.491)	Prec@1 82.031 (83.009)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [200/391]	Time 0.067 (0.061)	Data 0.002 (0.005)	Loss 0.726 (0.488)	Prec@1 76.562 (83.123)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [250/391]	Time 0.061 (0.060)	Data 0.003 (0.004)	Loss 0.474 (0.485)	Prec@1 84.375 (83.195)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [300/391]	Time 0.044 (0.060)	Data 0.002 (0.004)	Loss 0.478 (0.487)	Prec@1 82.031 (83.212)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Num bit 8	Num grad bit 8	
06-15-23 19:51:Iter: [350/391]	Time 0.068 (0.060)	Data 0.002 (0.004)	Loss 0.539 (0.494)	Prec@1 81.250 (82.899)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:51:Test: [0/79]	Time: 0.4751(0.4751)	Loss: 4.577(4.577)	Prec@1: 18.750(18.750)	
06-15-23 19:51:Test: [50/79]	Time: 0.0183(0.0272)	Loss: 4.399(4.463)	Prec@1: 19.531(18.061)	
06-15-23 19:51:Test: [78/79]	Time: 0.0165(0.0241)	Loss: 4.204(4.492)	Prec@1: 18.750(17.340)	
06-15-23 19:51:Step 184 * Prec@1 17.340
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [0/391]	Time 0.540 (0.540)	Data 0.496 (0.496)	Loss 0.442 (0.442)	Prec@1 83.594 (83.594)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [50/391]	Time 0.041 (0.053)	Data 0.002 (0.012)	Loss 0.438 (0.505)	Prec@1 82.812 (82.690)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [100/391]	Time 0.067 (0.049)	Data 0.003 (0.007)	Loss 0.492 (0.493)	Prec@1 85.156 (83.083)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [150/391]	Time 0.043 (0.049)	Data 0.002 (0.006)	Loss 0.721 (0.493)	Prec@1 77.344 (83.097)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [200/391]	Time 0.049 (0.050)	Data 0.002 (0.005)	Loss 0.363 (0.494)	Prec@1 85.938 (83.077)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [250/391]	Time 0.057 (0.050)	Data 0.002 (0.004)	Loss 0.569 (0.499)	Prec@1 81.250 (82.831)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [300/391]	Time 0.062 (0.050)	Data 0.002 (0.004)	Loss 0.440 (0.497)	Prec@1 84.375 (82.825)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [350/391]	Time 0.071 (0.052)	Data 0.003 (0.004)	Loss 0.482 (0.497)	Prec@1 82.031 (82.817)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Test: [0/79]	Time: 0.4563(0.4563)	Loss: 2.673(2.673)	Prec@1: 35.938(35.938)	
06-15-23 19:52:Test: [50/79]	Time: 0.0194(0.0269)	Loss: 2.523(2.695)	Prec@1: 35.938(31.296)	
06-15-23 19:52:Test: [78/79]	Time: 0.0176(0.0240)	Loss: 1.864(2.690)	Prec@1: 37.500(31.700)	
06-15-23 19:52:Step 185 * Prec@1 31.700
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [0/391]	Time 0.470 (0.470)	Data 0.416 (0.416)	Loss 0.711 (0.711)	Prec@1 75.781 (75.781)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [50/391]	Time 0.064 (0.065)	Data 0.003 (0.011)	Loss 0.680 (0.504)	Prec@1 78.125 (82.966)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [100/391]	Time 0.070 (0.064)	Data 0.003 (0.007)	Loss 0.632 (0.508)	Prec@1 80.469 (82.735)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [150/391]	Time 0.074 (0.065)	Data 0.003 (0.006)	Loss 0.735 (0.510)	Prec@1 78.906 (82.606)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [200/391]	Time 0.066 (0.066)	Data 0.003 (0.005)	Loss 0.607 (0.507)	Prec@1 77.344 (82.676)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [250/391]	Time 0.054 (0.065)	Data 0.002 (0.005)	Loss 0.530 (0.507)	Prec@1 82.812 (82.682)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [300/391]	Time 0.069 (0.064)	Data 0.003 (0.004)	Loss 0.508 (0.507)	Prec@1 83.594 (82.644)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [350/391]	Time 0.050 (0.065)	Data 0.002 (0.004)	Loss 0.455 (0.506)	Prec@1 85.156 (82.710)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Test: [0/79]	Time: 0.4680(0.4680)	Loss: 2.264(2.264)	Prec@1: 43.750(43.750)	
06-15-23 19:52:Test: [50/79]	Time: 0.0302(0.0293)	Loss: 2.607(2.659)	Prec@1: 40.625(36.933)	
06-15-23 19:52:Test: [78/79]	Time: 0.0166(0.0255)	Loss: 2.636(2.637)	Prec@1: 43.750(37.260)	
06-15-23 19:52:Step 186 * Prec@1 37.260
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [0/391]	Time 0.498 (0.498)	Data 0.449 (0.449)	Loss 0.483 (0.483)	Prec@1 85.938 (85.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [50/391]	Time 0.070 (0.064)	Data 0.003 (0.011)	Loss 0.494 (0.501)	Prec@1 82.812 (82.843)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [100/391]	Time 0.054 (0.061)	Data 0.003 (0.007)	Loss 0.427 (0.505)	Prec@1 85.938 (82.812)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:52:Num bit 8	Num grad bit 8	
06-15-23 19:52:Iter: [150/391]	Time 0.045 (0.056)	Data 0.002 (0.005)	Loss 0.535 (0.504)	Prec@1 83.594 (82.802)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [200/391]	Time 0.071 (0.054)	Data 0.004 (0.005)	Loss 0.503 (0.507)	Prec@1 83.594 (82.653)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [250/391]	Time 0.048 (0.054)	Data 0.002 (0.004)	Loss 0.446 (0.514)	Prec@1 89.062 (82.408)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [300/391]	Time 0.066 (0.055)	Data 0.003 (0.004)	Loss 0.653 (0.518)	Prec@1 76.562 (82.213)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [350/391]	Time 0.061 (0.056)	Data 0.003 (0.004)	Loss 0.497 (0.519)	Prec@1 82.812 (82.149)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Test: [0/79]	Time: 0.4367(0.4367)	Loss: 9.192(9.192)	Prec@1: 17.188(17.188)	
06-15-23 19:53:Test: [50/79]	Time: 0.0185(0.0269)	Loss: 9.791(9.059)	Prec@1: 22.656(21.676)	
06-15-23 19:53:Test: [78/79]	Time: 0.0167(0.0240)	Loss: 9.416(9.110)	Prec@1: 18.750(21.390)	
06-15-23 19:53:Step 187 * Prec@1 21.390
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [0/391]	Time 0.476 (0.476)	Data 0.433 (0.433)	Loss 0.666 (0.666)	Prec@1 77.344 (77.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [50/391]	Time 0.060 (0.062)	Data 0.003 (0.011)	Loss 0.472 (0.509)	Prec@1 81.250 (82.690)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [100/391]	Time 0.050 (0.058)	Data 0.002 (0.007)	Loss 0.423 (0.515)	Prec@1 83.594 (82.310)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [150/391]	Time 0.064 (0.060)	Data 0.003 (0.006)	Loss 0.486 (0.511)	Prec@1 85.156 (82.476)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [200/391]	Time 0.065 (0.060)	Data 0.003 (0.005)	Loss 0.564 (0.516)	Prec@1 80.469 (82.264)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [250/391]	Time 0.056 (0.061)	Data 0.002 (0.004)	Loss 0.442 (0.515)	Prec@1 84.375 (82.299)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [300/391]	Time 0.059 (0.062)	Data 0.003 (0.004)	Loss 0.537 (0.514)	Prec@1 81.250 (82.353)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [350/391]	Time 0.056 (0.061)	Data 0.002 (0.004)	Loss 0.587 (0.519)	Prec@1 81.250 (82.174)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Test: [0/79]	Time: 0.4731(0.4731)	Loss: 5.814(5.814)	Prec@1: 20.312(20.312)	
06-15-23 19:53:Test: [50/79]	Time: 0.0212(0.0274)	Loss: 6.197(5.637)	Prec@1: 20.312(21.124)	
06-15-23 19:53:Test: [78/79]	Time: 0.0165(0.0245)	Loss: 6.873(5.666)	Prec@1: 6.250(20.710)	
06-15-23 19:53:Step 188 * Prec@1 20.710
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [0/391]	Time 0.428 (0.428)	Data 0.386 (0.386)	Loss 0.550 (0.550)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [50/391]	Time 0.058 (0.070)	Data 0.003 (0.010)	Loss 0.660 (0.545)	Prec@1 75.781 (80.959)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [100/391]	Time 0.065 (0.066)	Data 0.003 (0.007)	Loss 0.685 (0.530)	Prec@1 76.562 (81.567)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [150/391]	Time 0.053 (0.064)	Data 0.003 (0.005)	Loss 0.505 (0.527)	Prec@1 83.594 (81.612)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [200/391]	Time 0.067 (0.062)	Data 0.003 (0.005)	Loss 0.482 (0.525)	Prec@1 83.594 (81.829)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [250/391]	Time 0.054 (0.062)	Data 0.003 (0.004)	Loss 0.569 (0.523)	Prec@1 81.250 (81.969)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:53:Num bit 8	Num grad bit 8	
06-15-23 19:53:Iter: [300/391]	Time 0.051 (0.060)	Data 0.002 (0.004)	Loss 0.463 (0.523)	Prec@1 80.469 (81.922)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [350/391]	Time 0.070 (0.060)	Data 0.003 (0.004)	Loss 0.649 (0.522)	Prec@1 78.125 (81.953)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Test: [0/79]	Time: 0.4979(0.4979)	Loss: 2.732(2.732)	Prec@1: 42.969(42.969)	
06-15-23 19:54:Test: [50/79]	Time: 0.0180(0.0274)	Loss: 3.234(3.187)	Prec@1: 45.312(37.577)	
06-15-23 19:54:Test: [78/79]	Time: 0.0164(0.0242)	Loss: 3.387(3.159)	Prec@1: 50.000(37.860)	
06-15-23 19:54:Step 189 * Prec@1 37.860
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [0/391]	Time 0.532 (0.532)	Data 0.488 (0.488)	Loss 0.511 (0.511)	Prec@1 78.125 (78.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [50/391]	Time 0.060 (0.061)	Data 0.003 (0.012)	Loss 0.445 (0.518)	Prec@1 86.719 (82.276)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [100/391]	Time 0.073 (0.067)	Data 0.003 (0.008)	Loss 0.603 (0.523)	Prec@1 75.000 (81.985)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [150/391]	Time 0.058 (0.067)	Data 0.002 (0.006)	Loss 0.398 (0.528)	Prec@1 88.281 (81.907)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [200/391]	Time 0.073 (0.068)	Data 0.003 (0.005)	Loss 0.446 (0.523)	Prec@1 83.594 (82.175)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [250/391]	Time 0.051 (0.067)	Data 0.003 (0.005)	Loss 0.491 (0.521)	Prec@1 82.812 (82.174)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [300/391]	Time 0.073 (0.067)	Data 0.003 (0.004)	Loss 0.533 (0.517)	Prec@1 78.906 (82.262)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [350/391]	Time 0.040 (0.065)	Data 0.002 (0.004)	Loss 0.554 (0.519)	Prec@1 82.812 (82.171)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Test: [0/79]	Time: 0.4327(0.4327)	Loss: 13.078(13.078)	Prec@1: 8.594(8.594)	
06-15-23 19:54:Test: [50/79]	Time: 0.0180(0.0263)	Loss: 12.929(12.749)	Prec@1: 10.938(10.064)	
06-15-23 19:54:Test: [78/79]	Time: 0.0166(0.0234)	Loss: 12.700(12.829)	Prec@1: 6.250(10.040)	
06-15-23 19:54:Step 190 * Prec@1 10.040
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [0/391]	Time 0.487 (0.487)	Data 0.447 (0.447)	Loss 0.579 (0.579)	Prec@1 80.469 (80.469)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [50/391]	Time 0.062 (0.061)	Data 0.003 (0.012)	Loss 0.452 (0.516)	Prec@1 84.375 (81.771)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [100/391]	Time 0.063 (0.063)	Data 0.002 (0.007)	Loss 0.477 (0.511)	Prec@1 82.812 (82.039)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [150/391]	Time 0.079 (0.063)	Data 0.003 (0.006)	Loss 0.513 (0.520)	Prec@1 82.812 (81.866)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [200/391]	Time 0.073 (0.064)	Data 0.003 (0.005)	Loss 0.428 (0.520)	Prec@1 85.938 (82.125)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [250/391]	Time 0.067 (0.065)	Data 0.003 (0.005)	Loss 0.496 (0.525)	Prec@1 85.156 (81.879)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [300/391]	Time 0.053 (0.065)	Data 0.003 (0.004)	Loss 0.456 (0.524)	Prec@1 81.250 (81.894)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Num bit 8	Num grad bit 8	
06-15-23 19:54:Iter: [350/391]	Time 0.064 (0.063)	Data 0.002 (0.004)	Loss 0.519 (0.522)	Prec@1 83.594 (82.031)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:54:Test: [0/79]	Time: 0.4192(0.4192)	Loss: 4.703(4.703)	Prec@1: 25.781(25.781)	
06-15-23 19:54:Test: [50/79]	Time: 0.0182(0.0264)	Loss: 4.903(4.755)	Prec@1: 24.219(24.954)	
06-15-23 19:55:Test: [78/79]	Time: 0.0162(0.0234)	Loss: 4.217(4.737)	Prec@1: 25.000(24.980)	
06-15-23 19:55:Step 191 * Prec@1 24.980
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [0/391]	Time 0.470 (0.470)	Data 0.430 (0.430)	Loss 0.565 (0.565)	Prec@1 82.812 (82.812)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [50/391]	Time 0.056 (0.071)	Data 0.002 (0.011)	Loss 0.379 (0.515)	Prec@1 89.844 (82.521)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [100/391]	Time 0.049 (0.062)	Data 0.003 (0.007)	Loss 0.719 (0.525)	Prec@1 73.438 (82.101)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [150/391]	Time 0.071 (0.062)	Data 0.003 (0.006)	Loss 0.558 (0.524)	Prec@1 82.812 (82.047)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [200/391]	Time 0.061 (0.063)	Data 0.003 (0.005)	Loss 0.497 (0.526)	Prec@1 82.812 (82.047)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [250/391]	Time 0.061 (0.063)	Data 0.003 (0.005)	Loss 0.505 (0.530)	Prec@1 84.375 (81.972)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [300/391]	Time 0.069 (0.064)	Data 0.003 (0.004)	Loss 0.609 (0.528)	Prec@1 76.562 (82.104)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [350/391]	Time 0.067 (0.065)	Data 0.003 (0.004)	Loss 0.601 (0.528)	Prec@1 77.344 (82.029)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Test: [0/79]	Time: 0.4383(0.4383)	Loss: 4.220(4.220)	Prec@1: 35.156(35.156)	
06-15-23 19:55:Test: [50/79]	Time: 0.0168(0.0252)	Loss: 5.323(5.008)	Prec@1: 30.469(27.880)	
06-15-23 19:55:Test: [78/79]	Time: 0.0161(0.0225)	Loss: 4.268(4.928)	Prec@1: 43.750(28.430)	
06-15-23 19:55:Step 192 * Prec@1 28.430
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [0/391]	Time 0.395 (0.395)	Data 0.356 (0.356)	Loss 0.547 (0.547)	Prec@1 77.344 (77.344)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [50/391]	Time 0.058 (0.056)	Data 0.003 (0.009)	Loss 0.503 (0.521)	Prec@1 82.812 (82.154)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [100/391]	Time 0.072 (0.061)	Data 0.003 (0.006)	Loss 0.493 (0.526)	Prec@1 79.688 (81.892)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [150/391]	Time 0.065 (0.062)	Data 0.003 (0.005)	Loss 0.448 (0.528)	Prec@1 85.938 (81.938)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [200/391]	Time 0.066 (0.063)	Data 0.003 (0.005)	Loss 0.594 (0.528)	Prec@1 80.469 (81.988)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [250/391]	Time 0.060 (0.063)	Data 0.003 (0.004)	Loss 0.643 (0.533)	Prec@1 75.781 (81.757)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [300/391]	Time 0.060 (0.063)	Data 0.003 (0.004)	Loss 0.598 (0.534)	Prec@1 78.906 (81.652)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [350/391]	Time 0.068 (0.064)	Data 0.003 (0.004)	Loss 0.656 (0.532)	Prec@1 78.906 (81.760)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Test: [0/79]	Time: 0.4630(0.4630)	Loss: 5.272(5.272)	Prec@1: 9.375(9.375)	
06-15-23 19:55:Test: [50/79]	Time: 0.0174(0.0260)	Loss: 5.285(5.578)	Prec@1: 11.719(10.708)	
06-15-23 19:55:Test: [78/79]	Time: 0.0156(0.0229)	Loss: 4.664(5.606)	Prec@1: 12.500(10.880)	
06-15-23 19:55:Step 193 * Prec@1 10.880
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [0/391]	Time 0.502 (0.502)	Data 0.464 (0.464)	Loss 0.440 (0.440)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:55:Num bit 8	Num grad bit 8	
06-15-23 19:55:Iter: [50/391]	Time 0.056 (0.069)	Data 0.002 (0.012)	Loss 0.469 (0.528)	Prec@1 84.375 (82.016)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [100/391]	Time 0.048 (0.067)	Data 0.002 (0.007)	Loss 0.633 (0.531)	Prec@1 76.562 (81.977)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [150/391]	Time 0.067 (0.067)	Data 0.003 (0.006)	Loss 0.615 (0.525)	Prec@1 78.125 (82.083)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [200/391]	Time 0.078 (0.067)	Data 0.003 (0.005)	Loss 0.435 (0.522)	Prec@1 84.375 (82.222)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [250/391]	Time 0.068 (0.068)	Data 0.003 (0.005)	Loss 0.518 (0.521)	Prec@1 83.594 (82.202)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [300/391]	Time 0.070 (0.068)	Data 0.003 (0.004)	Loss 0.519 (0.527)	Prec@1 82.812 (81.914)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [350/391]	Time 0.079 (0.068)	Data 0.003 (0.004)	Loss 0.552 (0.531)	Prec@1 78.125 (81.822)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Test: [0/79]	Time: 0.4826(0.4826)	Loss: 2.336(2.336)	Prec@1: 37.500(37.500)	
06-15-23 19:56:Test: [50/79]	Time: 0.0170(0.0260)	Loss: 2.695(2.516)	Prec@1: 34.375(35.830)	
06-15-23 19:56:Test: [78/79]	Time: 0.0155(0.0228)	Loss: 2.857(2.501)	Prec@1: 37.500(36.200)	
06-15-23 19:56:Step 194 * Prec@1 36.200
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [0/391]	Time 0.487 (0.487)	Data 0.447 (0.447)	Loss 0.530 (0.530)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [50/391]	Time 0.049 (0.076)	Data 0.003 (0.012)	Loss 0.478 (0.542)	Prec@1 83.594 (81.036)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [100/391]	Time 0.070 (0.072)	Data 0.003 (0.007)	Loss 0.463 (0.529)	Prec@1 83.594 (81.853)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [150/391]	Time 0.074 (0.072)	Data 0.003 (0.006)	Loss 0.594 (0.528)	Prec@1 78.125 (81.866)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [200/391]	Time 0.074 (0.072)	Data 0.003 (0.005)	Loss 0.451 (0.528)	Prec@1 87.500 (81.969)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [250/391]	Time 0.072 (0.072)	Data 0.003 (0.005)	Loss 0.462 (0.530)	Prec@1 84.375 (81.770)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [300/391]	Time 0.072 (0.072)	Data 0.003 (0.004)	Loss 0.494 (0.532)	Prec@1 84.375 (81.709)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [350/391]	Time 0.065 (0.071)	Data 0.002 (0.004)	Loss 0.533 (0.533)	Prec@1 83.594 (81.642)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Test: [0/79]	Time: 0.4686(0.4686)	Loss: 3.274(3.274)	Prec@1: 42.969(42.969)	
06-15-23 19:56:Test: [50/79]	Time: 0.0168(0.0260)	Loss: 2.997(2.897)	Prec@1: 42.188(44.868)	
06-15-23 19:56:Test: [78/79]	Time: 0.0154(0.0227)	Loss: 4.032(2.885)	Prec@1: 37.500(44.900)	
06-15-23 19:56:Step 195 * Prec@1 44.900
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [0/391]	Time 0.478 (0.478)	Data 0.438 (0.438)	Loss 0.609 (0.609)	Prec@1 81.250 (81.250)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:56:Num bit 8	Num grad bit 8	
06-15-23 19:56:Iter: [50/391]	Time 0.074 (0.074)	Data 0.003 (0.011)	Loss 0.415 (0.520)	Prec@1 86.719 (82.215)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [100/391]	Time 0.071 (0.073)	Data 0.003 (0.007)	Loss 0.462 (0.530)	Prec@1 81.250 (81.869)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [150/391]	Time 0.070 (0.073)	Data 0.003 (0.006)	Loss 0.506 (0.538)	Prec@1 78.906 (81.648)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [200/391]	Time 0.042 (0.071)	Data 0.002 (0.005)	Loss 0.665 (0.539)	Prec@1 77.344 (81.608)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [250/391]	Time 0.068 (0.069)	Data 0.003 (0.004)	Loss 0.698 (0.544)	Prec@1 77.344 (81.483)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [300/391]	Time 0.068 (0.069)	Data 0.003 (0.004)	Loss 0.498 (0.539)	Prec@1 82.812 (81.655)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [350/391]	Time 0.070 (0.069)	Data 0.003 (0.004)	Loss 0.638 (0.539)	Prec@1 80.469 (81.717)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Test: [0/79]	Time: 0.4662(0.4662)	Loss: 5.462(5.462)	Prec@1: 21.875(21.875)	
06-15-23 19:57:Test: [50/79]	Time: 0.0167(0.0255)	Loss: 5.765(6.046)	Prec@1: 19.531(18.199)	
06-15-23 19:57:Test: [78/79]	Time: 0.0152(0.0224)	Loss: 6.121(6.005)	Prec@1: 18.750(18.060)	
06-15-23 19:57:Step 196 * Prec@1 18.060
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [0/391]	Time 0.483 (0.483)	Data 0.437 (0.437)	Loss 0.341 (0.341)	Prec@1 86.719 (86.719)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [50/391]	Time 0.074 (0.054)	Data 0.004 (0.011)	Loss 0.548 (0.539)	Prec@1 80.469 (81.235)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [100/391]	Time 0.069 (0.059)	Data 0.003 (0.007)	Loss 0.429 (0.541)	Prec@1 88.281 (81.119)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [150/391]	Time 0.069 (0.061)	Data 0.003 (0.005)	Loss 0.432 (0.541)	Prec@1 85.156 (81.229)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [200/391]	Time 0.063 (0.064)	Data 0.002 (0.005)	Loss 0.545 (0.540)	Prec@1 81.250 (81.320)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [250/391]	Time 0.063 (0.065)	Data 0.002 (0.004)	Loss 0.558 (0.541)	Prec@1 82.812 (81.368)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [300/391]	Time 0.071 (0.065)	Data 0.003 (0.004)	Loss 0.682 (0.541)	Prec@1 75.781 (81.359)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [350/391]	Time 0.078 (0.066)	Data 0.002 (0.004)	Loss 0.527 (0.538)	Prec@1 80.469 (81.386)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Test: [0/79]	Time: 0.4580(0.4580)	Loss: 6.010(6.010)	Prec@1: 27.344(27.344)	
06-15-23 19:57:Test: [50/79]	Time: 0.0170(0.0268)	Loss: 7.344(6.396)	Prec@1: 16.406(24.663)	
06-15-23 19:57:Test: [78/79]	Time: 0.0155(0.0233)	Loss: 3.805(6.435)	Prec@1: 50.000(24.810)	
06-15-23 19:57:Step 197 * Prec@1 24.810
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [0/391]	Time 0.495 (0.495)	Data 0.444 (0.444)	Loss 0.582 (0.582)	Prec@1 78.906 (78.906)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [50/391]	Time 0.068 (0.061)	Data 0.003 (0.011)	Loss 0.517 (0.543)	Prec@1 84.375 (80.959)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:57:Num bit 8	Num grad bit 8	
06-15-23 19:57:Iter: [100/391]	Time 0.075 (0.063)	Data 0.003 (0.007)	Loss 0.624 (0.544)	Prec@1 76.562 (80.995)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [150/391]	Time 0.071 (0.065)	Data 0.003 (0.005)	Loss 0.608 (0.544)	Prec@1 82.031 (81.229)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [200/391]	Time 0.051 (0.066)	Data 0.002 (0.005)	Loss 0.559 (0.545)	Prec@1 79.688 (81.048)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [250/391]	Time 0.046 (0.064)	Data 0.002 (0.004)	Loss 0.479 (0.542)	Prec@1 83.594 (81.172)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [300/391]	Time 0.063 (0.062)	Data 0.003 (0.004)	Loss 0.499 (0.540)	Prec@1 84.375 (81.279)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [350/391]	Time 0.067 (0.063)	Data 0.003 (0.004)	Loss 0.656 (0.538)	Prec@1 78.125 (81.421)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Test: [0/79]	Time: 0.4508(0.4508)	Loss: 6.366(6.366)	Prec@1: 25.781(25.781)	
06-15-23 19:58:Test: [50/79]	Time: 0.0169(0.0253)	Loss: 6.043(5.979)	Prec@1: 22.656(23.346)	
06-15-23 19:58:Test: [78/79]	Time: 0.0153(0.0223)	Loss: 6.847(5.991)	Prec@1: 12.500(23.410)	
06-15-23 19:58:Step 198 * Prec@1 23.410
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [0/391]	Time 0.495 (0.495)	Data 0.457 (0.457)	Loss 0.527 (0.527)	Prec@1 84.375 (84.375)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [50/391]	Time 0.045 (0.065)	Data 0.002 (0.011)	Loss 0.406 (0.536)	Prec@1 86.719 (81.097)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [100/391]	Time 0.071 (0.062)	Data 0.003 (0.007)	Loss 0.496 (0.533)	Prec@1 82.812 (81.289)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [150/391]	Time 0.049 (0.062)	Data 0.002 (0.005)	Loss 0.425 (0.531)	Prec@1 87.500 (81.452)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [200/391]	Time 0.062 (0.059)	Data 0.003 (0.004)	Loss 0.418 (0.533)	Prec@1 82.812 (81.460)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [250/391]	Time 0.059 (0.060)	Data 0.002 (0.004)	Loss 0.542 (0.535)	Prec@1 82.031 (81.455)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [300/391]	Time 0.055 (0.062)	Data 0.002 (0.004)	Loss 0.545 (0.537)	Prec@1 82.031 (81.369)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Num bit 8	Num grad bit 8	
06-15-23 19:58:Iter: [350/391]	Time 0.071 (0.060)	Data 0.003 (0.004)	Loss 0.787 (0.538)	Prec@1 75.781 (81.401)	Training FLOPS ratio: 0.062500 (0.062500)	
06-15-23 19:58:Test: [0/79]	Time: 0.4641(0.4641)	Loss: 5.382(5.382)	Prec@1: 19.531(19.531)	
06-15-23 19:58:Test: [50/79]	Time: 0.0166(0.0254)	Loss: 5.044(5.748)	Prec@1: 24.219(22.335)	
06-15-23 19:58:Test: [78/79]	Time: 0.0154(0.0229)	Loss: 4.824(5.793)	Prec@1: 31.250(22.160)	
06-15-23 19:58:Step 199 * Prec@1 22.160
06-15-23 19:58:resnet20 training is over!
*********************************************************
*         Best Prec@: 73.89, Best Epoch: 012.           *
*********************************************************
